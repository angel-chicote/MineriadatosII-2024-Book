<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Módulo 8. Minería de Datos II - 1&nbsp; Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./capitulo2.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar floating">% Para el pseudocodigo
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./capitulo1.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Deep Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Módulo 8. Minería de Datos II</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introducción</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./capitulo1.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./capitulo2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Modelos Gráficos Probabilísticos y Análisis Causal</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introducción" id="toc-introducción" class="nav-link active" data-scroll-target="#introducción"><span class="header-section-number">1.1</span> Introducción</a></li>
  <li><a href="#principales-arquitecturas-y-software-de-deep-learning" id="toc-principales-arquitecturas-y-software-de-deep-learning" class="nav-link" data-scroll-target="#principales-arquitecturas-y-software-de-deep-learning"><span class="header-section-number">1.2</span> Principales arquitecturas y software de Deep Learning</a>
  <ul class="collapse">
  <li><a href="#principales-arquitecturas" id="toc-principales-arquitecturas" class="nav-link" data-scroll-target="#principales-arquitecturas"><span class="header-section-number">1.2.1</span> Principales arquitecturas</a></li>
  <li><a href="#software" id="toc-software" class="nav-link" data-scroll-target="#software"><span class="header-section-number">1.2.2</span> Software</a></li>
  </ul></li>
  <li><a href="#conceptos-básicos-de-las-redes-neuronales" id="toc-conceptos-básicos-de-las-redes-neuronales" class="nav-link" data-scroll-target="#conceptos-básicos-de-las-redes-neuronales"><span class="header-section-number">1.3</span> Conceptos básicos de las Redes Neuronales</a>
  <ul class="collapse">
  <li><a href="#datos" id="toc-datos" class="nav-link" data-scroll-target="#datos"><span class="header-section-number">1.3.1</span> Datos</a></li>
  <li><a href="#arquitectura-de-red" id="toc-arquitectura-de-red" class="nav-link" data-scroll-target="#arquitectura-de-red"><span class="header-section-number">1.3.2</span> Arquitectura de red</a></li>
  <li><a href="#función-de-coste-y-pérdida" id="toc-función-de-coste-y-pérdida" class="nav-link" data-scroll-target="#función-de-coste-y-pérdida"><span class="header-section-number">1.3.3</span> Función de coste y pérdida</a></li>
  <li><a href="#optimizador" id="toc-optimizador" class="nav-link" data-scroll-target="#optimizador"><span class="header-section-number">1.3.4</span> Optimizador</a></li>
  <li><a href="#función-de-activación" id="toc-función-de-activación" class="nav-link" data-scroll-target="#función-de-activación"><span class="header-section-number">1.3.5</span> Función de activación</a></li>
  <li><a href="#regularización" id="toc-regularización" class="nav-link" data-scroll-target="#regularización"><span class="header-section-number">1.3.6</span> Regularización</a></li>
  <li><a href="#dropout" id="toc-dropout" class="nav-link" data-scroll-target="#dropout"><span class="header-section-number">1.3.7</span> Dropout</a></li>
  <li><a href="#dropconnect" id="toc-dropconnect" class="nav-link" data-scroll-target="#dropconnect"><span class="header-section-number">1.3.8</span> Dropconnect</a></li>
  <li><a href="#inicialización-de-pesos" id="toc-inicialización-de-pesos" class="nav-link" data-scroll-target="#inicialización-de-pesos"><span class="header-section-number">1.3.9</span> Inicialización de pesos</a></li>
  <li><a href="#batch-normalization" id="toc-batch-normalization" class="nav-link" data-scroll-target="#batch-normalization"><span class="header-section-number">1.3.10</span> Batch normalization</a></li>
  <li><a href="#ejemplo-de-red-neuronal-con-keras" id="toc-ejemplo-de-red-neuronal-con-keras" class="nav-link" data-scroll-target="#ejemplo-de-red-neuronal-con-keras"><span class="header-section-number">1.3.11</span> Ejemplo de Red Neuronal con Keras</a></li>
  </ul></li>
  <li><a href="#redes-neuronales-convolucionales" id="toc-redes-neuronales-convolucionales" class="nav-link" data-scroll-target="#redes-neuronales-convolucionales"><span class="header-section-number">1.4</span> Redes Neuronales Convolucionales</a>
  <ul class="collapse">
  <li><a href="#introducción-1" id="toc-introducción-1" class="nav-link" data-scroll-target="#introducción-1"><span class="header-section-number">1.4.1</span> Introducción</a></li>
  <li><a href="#clasificación-de-imágenes" id="toc-clasificación-de-imágenes" class="nav-link" data-scroll-target="#clasificación-de-imágenes"><span class="header-section-number">1.4.2</span> Clasificación de imágenes</a></li>
  <li><a href="#clasificación-de-textos" id="toc-clasificación-de-textos" class="nav-link" data-scroll-target="#clasificación-de-textos"><span class="header-section-number">1.4.3</span> Clasificación de textos</a></li>
  </ul></li>
  <li><a href="#redes-recurrentes" id="toc-redes-recurrentes" class="nav-link" data-scroll-target="#redes-recurrentes"><span class="header-section-number">1.5</span> Redes Recurrentes</a></li>
  <li><a href="#redes-recurrentes-elman-jordan-lstm-y-gru" id="toc-redes-recurrentes-elman-jordan-lstm-y-gru" class="nav-link" data-scroll-target="#redes-recurrentes-elman-jordan-lstm-y-gru"><span class="header-section-number">1.6</span> Redes recurrentes: Elman, Jordan, LSTM y GRU</a>
  <ul class="collapse">
  <li><a href="#forecasting-en-series-de-tiempo" id="toc-forecasting-en-series-de-tiempo" class="nav-link" data-scroll-target="#forecasting-en-series-de-tiempo"><span class="header-section-number">1.6.1</span> Forecasting en series de tiempo</a></li>
  <li><a href="#clasificación-y-generación-de-textos" id="toc-clasificación-y-generación-de-textos" class="nav-link" data-scroll-target="#clasificación-y-generación-de-textos"><span class="header-section-number">1.6.2</span> Clasificación y generación de textos</a></li>
  </ul></li>
  <li><a href="#autoencoders" id="toc-autoencoders" class="nav-link" data-scroll-target="#autoencoders"><span class="header-section-number">1.7</span> Autoencoders</a>
  <ul class="collapse">
  <li><a href="#bases-del-autoencoder" id="toc-bases-del-autoencoder" class="nav-link" data-scroll-target="#bases-del-autoencoder"><span class="header-section-number">1.7.1</span> Bases del Autoencoder</a></li>
  <li><a href="#idea-intuitiva-del-uso-de-autoencoders" id="toc-idea-intuitiva-del-uso-de-autoencoders" class="nav-link" data-scroll-target="#idea-intuitiva-del-uso-de-autoencoders"><span class="header-section-number">1.7.2</span> Idea intuitiva del uso de Autoencoders</a></li>
  <li><a href="#casos-de-uso" id="toc-casos-de-uso" class="nav-link" data-scroll-target="#casos-de-uso"><span class="header-section-number">1.7.3</span> Casos de uso</a></li>
  <li><a href="#diseño-del-modelo-de-ae" id="toc-diseño-del-modelo-de-ae" class="nav-link" data-scroll-target="#diseño-del-modelo-de-ae"><span class="header-section-number">1.7.4</span> Diseño del modelo de AE</a></li>
  <li><a href="#ejemplos-de-autoencoders" id="toc-ejemplos-de-autoencoders" class="nav-link" data-scroll-target="#ejemplos-de-autoencoders"><span class="header-section-number">1.7.5</span> Ejemplos de Autoencoders</a></li>
  </ul></li>
  <li><a href="#arquitecturas-preentrenadas" id="toc-arquitecturas-preentrenadas" class="nav-link" data-scroll-target="#arquitecturas-preentrenadas"><span class="header-section-number">1.8</span> Arquitecturas preentrenadas</a>
  <ul class="collapse">
  <li><a href="#paquetes-específicos-en-python" id="toc-paquetes-específicos-en-python" class="nav-link" data-scroll-target="#paquetes-específicos-en-python"><span class="header-section-number">1.8.1</span> Paquetes específicos en Python</a></li>
  <li><a href="#tensorflow-hub" id="toc-tensorflow-hub" class="nav-link" data-scroll-target="#tensorflow-hub"><span class="header-section-number">1.8.2</span> Tensorflow-Hub</a></li>
  <li><a href="#arquitecturas-zero-shot" id="toc-arquitecturas-zero-shot" class="nav-link" data-scroll-target="#arquitecturas-zero-shot"><span class="header-section-number">1.8.3</span> Arquitecturas Zero-shot</a></li>
  <li><a href="#detección-de-objetos" id="toc-detección-de-objetos" class="nav-link" data-scroll-target="#detección-de-objetos"><span class="header-section-number">1.8.4</span> Detección de objetos</a></li>
  <li><a href="#conversión-de-voz-a-texto" id="toc-conversión-de-voz-a-texto" class="nav-link" data-scroll-target="#conversión-de-voz-a-texto"><span class="header-section-number">1.8.5</span> Conversión de voz a texto</a></li>
  </ul></li>
  <li><a href="#aprendizaje-por-refuerzo" id="toc-aprendizaje-por-refuerzo" class="nav-link" data-scroll-target="#aprendizaje-por-refuerzo"><span class="header-section-number">1.9</span> Aprendizaje por Refuerzo</a>
  <ul class="collapse">
  <li><a href="#introducción-2" id="toc-introducción-2" class="nav-link" data-scroll-target="#introducción-2"><span class="header-section-number">1.9.1</span> Introducción</a></li>
  <li><a href="#formalismo-matemático" id="toc-formalismo-matemático" class="nav-link" data-scroll-target="#formalismo-matemático"><span class="header-section-number">1.9.2</span> Formalismo Matemático</a></li>
  <li><a href="#taxonomía-de-algoritmos" id="toc-taxonomía-de-algoritmos" class="nav-link" data-scroll-target="#taxonomía-de-algoritmos"><span class="header-section-number">1.9.3</span> Taxonomía de Algoritmos</a></li>
  <li><a href="#q-learning-value" id="toc-q-learning-value" class="nav-link" data-scroll-target="#q-learning-value"><span class="header-section-number">1.9.4</span> Q-Learning (value)</a></li>
  <li><a href="#dqn-deep-q-learning" id="toc-dqn-deep-q-learning" class="nav-link" data-scroll-target="#dqn-deep-q-learning"><span class="header-section-number">1.9.5</span> DQN (Deep Q-Learning)</a></li>
  <li><a href="#listado-algoritmos" id="toc-listado-algoritmos" class="nav-link" data-scroll-target="#listado-algoritmos"><span class="header-section-number">1.9.6</span> Listado Algoritmos</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Deep Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introducción" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="introducción"><span class="header-section-number">1.1</span> Introducción</h2>
<p>El <strong>deep learning</strong>, también conocido como aprendizaje <strong>profundo</strong>, es una disciplina que busca emular el funcionamiento del cerebro mediante el uso de hardware y software, generando inteligencia artificial. Este enfoque se materializa en redes neuronales artificiales (RNA), que emplean una abstracción jerárquica para representar datos en múltiples niveles. El proceso implica la utilización de arquitecturas de varias capas, donde cada una aprende patrones más complejos, favoreciendo el aprendizaje útil. Generalmente, se emplea aprendizaje no supervisado para guiar el entrenamiento de las capas intermedias. Aunque derivado del machine learning, el deep learning se distingue por su arquitectura en capas, inCluyendo redes convolucionales y recurrentes, en contraste con métodos más simples como el Perceptrón Multicapa de una sola capa. Su avance se vio inicialmente obstaculizado por problemas de estancamiento en mínimos locales, resueltos mediante preentrenamiento no supervisado de las capas. Este enfoque ha impulsado un rápido crecimiento en el desarrollo de arquitecturas y algoritmos de RNA en los últimos años, manteniendo la esencia del aprendizaje jerárquico y profundo.</p>
<p>Las redes neuronales tienen una amplia gama de aplicaciones en diversos campos, desde la clasificación y regresión de datos hasta la identificación de imágenes, texto y audio.</p>
<p>En la <em>identificación de imágenes</em>, por ejemplo, pueden reconocer animales, señales de tráfico, frutas, caras humanas e incluso tumores malignos en radiografías. A medida que se combinan estas capacidades, se pueden abordar problemas más complejos como la detección de objetos y personas en imágenes o el etiquetado de escenas. Con el <em>análisis de videos</em>, las redes neuronales pueden contar personas, reconocer objetos y señales de tráfico, o detectar comportamientos como llevar un arma.</p>
<p>Cuando se trata de <em>datos de texto</em>, las redes neuronales se utilizan en sistemas de traducción, chatbots y conversión de texto a audio. En el caso de <em>datos de audio</em>, se emplean en sistemas de traducción, altavoces inteligentes y conversión de audio a texto.</p>
<p>Para <strong>trabajar con redes neuronales</strong>, es crucial representar los datos de entrada numéricamente, incluso convirtiendo variables categóricas en valores numéricos y normalizando los datos entre 0 y 1. Esto facilita la convergencia hacia soluciones óptimas. Es importante que los datos seán números en coma flotante, sobre todo si se van a trabajar con <em>GPUs, Graphics Process Units</em>, ya que permitirán hacer un mejor uso de los multiples cores que les permiten operar en coma flotante de forma paralela. Actualmente, hay toda una serie de mejoras en las GPUs que permite aumentar el rendimiento de las redes neuronales como son el uso de operaciones en <em>FP16</em> (Floating Point de 16 bits en lugar de 32) de forma que pueden hacer dos operaciones de forma simultánea (el formato estándar es FP32) y además con la reducción de memoria (punto muy importante) al meter en los 32 bits 2 datos en lugar de sólo uno. También se han añadido técnicas de <em>Mixed Precision</em> (Narang et al.&nbsp;2018), los <em>Tensor Cores</em> (para las gráficas de NVIDIA) son otra de las mejoras que se han ido incorporando a la GPUs y que permiten acelerar los procesos tanto de entrenamiento como de predicción con las redes neuronales.</p>
</section>
<section id="principales-arquitecturas-y-software-de-deep-learning" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="principales-arquitecturas-y-software-de-deep-learning"><span class="header-section-number">1.2</span> Principales arquitecturas y software de Deep Learning</h2>
<section id="principales-arquitecturas" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="principales-arquitecturas"><span class="header-section-number">1.2.1</span> Principales arquitecturas</h3>
<p>Actualmente existen muchos tipos de estructuras de redes neuronales artificiales dado que logran resultados extraordinarios en muchos campos del conocimiento. Los primeros éxitos en el aprendizaje profundo se lograron a través de las investigaciones y trabajos de Geoffre Hinton (2006) que introduce las Redes de Creencia Profunda en cada capa de la red de una Máquina de Boltzmann Restringida (RBM) para la asignación inicial de los pesos sinápticos. Hace tiempo que se está trabajando con arquitecturas como los Autoencoders, Hinton y Zemel (1994), las RBMs de Hinton y Sejnowski (1986) y las DBNs (Deep Belief Networks), Hinton et al.&nbsp;(2006) y otras como las redes recurrentes y convolucionales. Estas técnicas constituyen en sí mismas arquitecturas de redes neuronales, aunque también algunas de ellas, como se ha afirmado en la introducción, se están empleando para inicializar los pesos de arquitecturas profundas de redes neuronales supervisadas con conexiones hacia adelante.</p>
<p><strong>Redes Convolucionales</strong></p>
<p>Las redes neuronales convolucionales (CNNs) han transformado el panorama del Deep Learning, destacándose por su habilidad para extraer características de alto nivel a través de la operación de convolución. Diseñadas específicamente para el procesamiento de imágenes, las CNNs son altamente eficientes en tareas de clasificación y segmentación en el ámbito de la visión artificial.</p>
<p>Inspiradas en el funcionamiento de la corteza visual del cerebro humano, estas redes representan una evolución del perceptrón multicapa. Aunque su uso se popularizó en la década de 1990 con el desarrollo de sistemas de lectura de cheques por parte de AT&amp;T, las CNNs han experimentado una evolución significativa desde entonces.</p>
<p>Su arquitectura se compone de capas de convolución, responsables de transformar los datos de entrada, y capas de pooling, encargadas de resumir la información relevante. Posteriormente, se aplican capas densamente conectadas para obtener el resultado final.</p>
<p>El auge de las CNNs se vio impulsado por iniciativas como la competencia ILSVRC, que propiciaron avances considerables en este campo. Entre los modelos más destacados se encuentran LeNet-5, AlexNet, VGG, GoogLeNet y ResNet, muchos de los cuales están disponibles como modelos preentrenados para su integración en diversas aplicaciones. Estos modelos, con estructuras de capas más complejas, representan el estado del arte en reconocimiento visual y están al alcance de cualquier investigador interesado en el Deep Learning.</p>
<p>Más allá de las arquitecturas conocidas, han surgido modelos más avanzados como DenseNet y EfficientNet, que optimizan el rendimiento y la eficiencia computacional. La transferencia de aprendizaje se ha convertido en una herramienta fundamental, permitiendo adaptar modelos preentrenados a tareas específicas con conjuntos de datos más pequeños, agilizando el entrenamiento y mejorando la generalización.</p>
<p>Las CNNs encuentran un amplio uso en tareas de segmentación semántica y detección de objetos, impulsadas por técnicas como U-Net y Mask R-CNN. Adicionalmente, métodos de aprendizaje débilmente supervisado y autoetiquetado están permitiendo entrenar modelos con datos etiquetados de manera menos precisa o incluso sin etiquetar.</p>
<p>Para mejorar la interpretabilidad de las CNNs, se han propuesto técnicas de visualización de atención visual, que permiten identificar las partes de una imagen que son más relevantes para la predicción del modelo.</p>
<p>Estos avances impulsan el continuo desarrollo de las CNNs, expandiendo su aplicación a diversos campos como el diagnóstico médico, la conducción autónoma y la robótica. La investigación activa en este campo sigue explorando nuevas formas de mejorar la eficiencia, la precisión y la interpretabilidad de las CNNs para abordar desafíos cada vez más complejos en el procesamiento de imágenes y otros tipos de datos.</p>
<p><strong>Autoencoders</strong></p>
<p>Los Autoencoders (AE) son una clase de redes neuronales dentro del ámbito del Deep Learning, caracterizadas por su enfoque en el aprendizaje no supervisado. Aunque se mencionaron por primera vez en la década de 1980, ha sido en los últimos años donde han experimentado un notable interés y desarrollo. La arquitectura de un AE consiste en dos partes principales: el encoder y el decoder. El encoder se encarga de codificar o comprimir los datos de entrada, mientras que el decoder se encarga de regenerar los datos originales en la salida, lo que resulta en una estructura simétrica.</p>
<p>Durante el entrenamiento, el AE aprende a reconstruir los datos de entrada en la capa de salida de la red, generalmente implementando restricciones como la reducción de elementos en las capas ocultas del encoder. Esto evita simplemente copiar la entrada en la salida y obliga al modelo a aprender representaciones más significativas de los datos. Entre las aplicaciones principales de los AE se encuentran la reducción de dimensiones y compresión de datos, la búsqueda de imágenes, la detección de anomalías y la eliminación de ruido.</p>
<p>Además de los autoencoders estándar, existen varias variaciones que han surgido para abordar diferentes desafíos y aplicaciones específicas, como los Variational Autoencoders (VAE), los Sparse Autoencoders, los Denoising Autoencoders y los Contractive Autoencoders. Estas variaciones amplían el alcance y la versatilidad de los autoencoders en una variedad de contextos de aprendizaje automático, desde la compresión de datos hasta la generación de nuevas muestras y la detección de anomalías en conjuntos de datos complejos.</p>
<p><strong>Redes Recurrentes</strong></p>
<p>Las redes neuronales recurrentes (RNNs) revolucionaron el panorama del machine learning, posicionándose como una herramienta fundamental para procesar y analizar datos secuenciales. A diferencia de las redes neuronales tradicionales con una estructura de capas fija, las RNNs poseen una arquitectura flexible que les permite incorporar información del pasado, presente y futuro, lo que las convirtió en una gran apuesta ante tareas omo el procesamiento del lenguaje natural, el reconocimiento de voz y la predicción de series temporales.</p>
<p>Gracias a su capacidad de memoria interna, las RNNs pueden capturar dependencias temporales en los datos secuenciales, una característica crucial para modelar el comportamiento de fenómenos que evolucionan con el tiempo. Esta característica las diferencia de las redes neuronales clásicas, que no tienen en cuenta el contexto temporal de la información.</p>
<p>La familia de las RNNs abarca diversas arquitecturas, cada una con sus propias fortalezas y aplicaciones. Entre las más populares encontramos las redes de Elman, las redes de Jordan, las redes Long Short-Term Memory (LSTM) y las redes Gated Recurrent Unit (GRU) que, introducidas en 2015 son una alternativa más ligera y eficiente a las LSTM.</p>
<p>El campo de las RNNs ha experimentado un rápido crecimiento en los últimos años, impulsado por avances en investigación y la disponibilidad de conjuntos de datos masivos. Entre las mejoras más notables encontramos las redes neuronales convolucionales recurrentes (CRNNs), las redes neuronales con atención y la integración del aprendizaje por refuerzo. Estas mejoras han ampliado aún más las capacidades de las RNNs, permitiéndolas abordar tareas cada vez más complejas y desafiantes.</p>
<p><strong>Redes Generativas Adversarias</strong></p>
<p>Las Generative Adversarial Networks (GAN) representan una innovadora aplicación del deep learning en la generación de contenido sintético, incluyendo imágenes, videos, música y caras extremadamente realistas. La arquitectura de una GAN consiste en dos componentes principales: un generador y un discriminador. El generador se encarga de crear nuevos datos sintéticos, como imágenes, a partir de un vector aleatorio en el espacio latente. Por otro lado, el discriminador tiene la tarea de distinguir entre datos reales y sintéticos, es decir, determinar si una imagen proviene del conjunto de datos original o si fue creada por el generador.</p>
<p>El generador se implementa típicamente utilizando una red neuronal convolucional profunda, con capas especializadas que aprenden a generar características de imágenes en lugar de extraerlas de una imagen de entrada. Algunas de las capas más comunes utilizadas en el modelo del generador son la capa de muestreo (UpSampling2D) que duplica las dimensiones de la entrada, y la capa convolucional de transposición (Conv2DTranspose) que realiza una operación de convolución inversa para generar datos sintéticos.</p>
<p>La idea clave detrás de las GAN es el entrenamiento adversarial, donde el generador y el discriminador compiten entre sí en un juego de suma cero. Mientras el generador trata de engañar al discriminador generando datos cada vez más realistas, el discriminador mejora su capacidad para distinguir entre datos reales y sintéticos. Este proceso de competencia continua lleva a la generación de datos sintéticos de alta calidad que son indistinguibles de los datos reales para el discriminador.</p>
<p>En los últimos años, las GAN han experimentado avances significativos en términos de nuevas arquitecturas y técnicas de entrenamiento. Por ejemplo, se han desarrollado variantes como las Conditional GAN (cGAN), que permiten controlar las características de los datos generados, y las Progressive GAN (ProgGAN), que generan imágenes de mayor resolución de forma progresiva. Además, se han propuesto técnicas de regularización, como la penalización del gradiente o la normalización espectral, para mejorar la estabilidad y la calidad de las GAN generadas.</p>
<p>Las GANs han abierto un abanico de posibilidades en diversos campos como el ámbito de la generación de texto así como aplicaciones en la realidad aumentada donde permiten integrar elementos sintéticos en el mundo real de forma realista, como la creación de avatares virtuales o la superposición de información sobre objetos físicos. Asimismo, de los videojuegos, las GANs se utilizan para desarrollar personajes, escenarios y objetos virtuales de alta calidad para experiencias de juego más inmersivas.</p>
<p><strong>Boltzmann Machine y Restricted Boltzmann Machine</strong></p>
<p>El aprendizaje de la denominada máquina de Boltzmann (BM) se realiza a través de un algoritmo estocástico que proviene de ideas basadas en la mecánica estadística. Este prototipo de red neuronal tiene una característica distintiva y es que el uso de conexiones sinápticas entre las neuronas es simétrico.</p>
<p>Las neuronas son de dos tipos: visibles y ocultas. Las neuronas visibles son las que interactúan y proveen una interface entre la red y el ambiente en el que operan, mientras que las neuronas actúan libremente sin interacciones con el entorno. Esta máquina dispone de dos modos de operación. El primero es la condición de anclaje donde las neuronas están fijas por los estímulos específicos que impone el ambiente. El otro modo es la condición de libertad, donde tanto las neuronas ocultas como las visibles actúan libremente sin condiciones impuestas por el medio ambiente. Las maquinas restringidas de Boltzmann (RBM) solamente toman en cuenta aquellos modelos en los que no existen conexiones del tipo visible-visible y oculta-oculta. Estas redes también asumen que los datos de entrenamiento son independientes y están idénticamente distribuidos.</p>
<p>Una forma de estimar los parámetros de un modelo estocástico es calculando la máxima verosimilitud. Para ello, se hace uso de los Markov Random Fiels (MRF), ya que al encontrar los parámetros que maximizan los datos de entrenamiento bajo una distribución MRF, equivale a encontrar los parámetros <span class="math inline">\(\theta\)</span> que maximizan la verosimilitud de los datos de entrenamiento, Fischer e Igel (2012). Maximizar dicha verosimilitud es el objetivo que persigue el algoritmo de entrenamiento de una RBM. A pesar de utilizar la distribución MRF, computacionalmente hablando se llega a ecuaciones inviables de implementar. Para evitar el problema anterior, las esperanzas que se obtienen de MRF pueden ser aproximadas por muestras extraídas de distribuciones basadas en las técnicas de Markov Chain Monte Carlo Techniques (MCMC). Las técnicas de MCMC utilizan un algoritmo denominado muestreo de Gibbs con el que obtenemos una secuencia de observaciones o muestras que se aproximan a partir de una distribución de verosimilitud de múltiples variables aleatorias. La idea básica del muestreo de Gibss es actualizar cada variable posteriormente en base a su distribución condicional dado el estado de las otras variables.</p>
<p><strong>Deep Belief Network</strong></p>
<p>Una red Deep Belief Network tal como demostró Hinton se puede considerar como un “apilamiento de redes restringidas de Boltzmann”. Tiene una estructura jerárquica que, como es sabido, es una de las características del deep learning. Como en el anterior modelo, esta red también es un modelo en grafo estocástico, que aprende a extraer una representación jerárquica profunda de los datos de entrenamiento. Cada capa de la RBM extrae un nivel de abstracción de características de los datos de entrenamiento, cada vez más significativo; pero para ello, la capa siguiente necesita la información de la capa anterior lo que implica el uso de las variables latentes.</p>
<p>Estos modelos caracterizan la distribución conjunta <span class="math inline">\(h_k\)</span> entre el vector de observaciones <em>x</em> y las capas ocultas, donde <span class="math inline">\(x=h_0\)</span>, es una distribución condicional para las unidades visibles limitadas sobre las unidades ocultas que pertenecen a la RBM en el nivel <em>k</em>, y es la distribución conjunta oculta visible en la red RBM del nivel superior o de salida.</p>
<p>El entrenamiento de esta red puede ser híbrido, empezando por un entrenamiento no supervisado para después aplicar un entrenamiento supervisado para un mejor y más óptimo ajuste, aunque pueden aplicarse diferentes tipos de entrenamiento, Bengio et al.&nbsp;(2007) y Salakhutdinov (2014) Para realizar un entrenamiento no supervisado se aplica a las redes de creencia profunda con Redes restringidas de Boltzmann el método de bloque constructor que fue presentado por Hinton (2006) y por Bengio (2007).</p>
</section>
<section id="software" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="software"><span class="header-section-number">1.2.2</span> Software</h3>
<p>Como se verá en los siguientes epígrafes, la opción preferida para este módulo de Deep Learning es el software llamado Keras que está programado en Python. En términos de eficiencia y de aprendizaje Keras presenta unas ventajas importantes que se especifican más adelante.</p>
<p>Aunque nuestra preferencia a nivel formativo es el <strong>uso de Keras y Tensorflow</strong>, a continuación serán descritos los principales softwares con los que poder realizar implementaciones de arquitecturas de aprendizaje profundo: <em>TensorFlow</em>, <em>Keras</em>, <em>Pytorch</em>, <em>MXNET</em>, <em>Caffe</em> y <em>JAX</em>.</p>
<p>Por su parte, también presentaremos <strong>Colaboratory Environment (Colab)</strong>, una herramienta de Google que dispone en la web y que no requiere ninguna instalación en nuestros ordenadores. Esta propuesta de Google resulta muy interesante dado que no requiere coste alguno, se puede ejecutar desde cualquier lugar aumentando nuestros recursos a la hora de trabajar con Deep Learning y admitiendo a su vez la implementación tanto de código Python como de R.</p>
<p><strong>TensorFlow</strong></p>
<p>TensorFlow es una biblioteca de código abierto para el cálculo numérico desarrollada por Google. Es una de las herramientas de Deep Learning más populares y ampliamente utilizadas, conocida por su flexibilidad, escalabilidad y comunidad activa. TensorFlow ofrece una amplia gama de funciones para construir, entrenar y desplegar modelos de Deep Learning, incluyendo:</p>
<ul>
<li>Soporte para una variedad de arquitecturas de redes neuronales: permite construir una amplia gama de arquitecturas de redes neuronales, desde redes convolucionales y recurrentes hasta modelos de atención y redes generativas adversarias (GANs)</li>
<li>Escalabilidad a grandes conjuntos de datos: está diseñado para manejar grandes conjuntos de datos y puede distribuirse en múltiples GPUs o TPU para acelerar el entrenamiento de modelos</li>
<li>Amplia gama de herramientas de visualización y depuración: proporciona una variedad de herramientas para visualizar y depurar modelos de Deep Learning, lo que facilita la identificación y resolución de problemas</li>
<li>Gran comunidad y recursos: cuenta con una gran y activa comunidad de desarrolladores y usuarios que proporcionan soporte y comparten recursos</li>
</ul>
<p><strong>Pytorch</strong></p>
<p>PyTorch es una biblioteca de código abierto para el aprendizaje automático desarrollada por Facebook. Es conocida por su sintaxis intuitiva y facilidad de uso, lo que la convierte en una opción popular para investigadores y desarrolladores principiantes. PyTorch ofrece características similares a TensorFlow, incluyendo:</p>
<ul>
<li>Soporte para una variedad de arquitecturas de redes neuronales: permite construir una amplia gama de arquitecturas de redes neuronales, desde redes convolucionales y recurrentes hasta modelos de atención y GANs</li>
<li>Ejecución dinámica de gráficos: utiliza un motor de ejecución de gráficos dinámico, lo que permite modificar los modelos durante el entrenamiento, lo que facilita la experimentación y el ajuste fino</li>
<li>Amplia gama de bibliotecas y herramientas de terceros: se beneficia de un ecosistema rico de bibliotecas y herramientas de terceros que amplían sus capacidades</li>
<li>Facilidad de uso: tiene una sintaxis similar a Python, lo que la hace fácil de aprender y usar para desarrolladores con experiencia en Python</li>
</ul>
<p><strong>Keras</strong></p>
<p>Keras es una biblioteca de código abierto para el aprendizaje automático de alto nivel que se ejecuta sobre TensorFlow o PyTorch. Es conocida por su simplicidad y facilidad de uso, lo que la convierte en una opción popular para principiantes y para desarrollar prototipos de modelos rápidamente. Keras ofrece una interfaz de alto nivel que abstrae las complejidades de las bibliotecas subyacentes, como TensorFlow o PyTorch, lo que permite a los usuarios centrarse en la construcción y el entrenamiento de modelos sin necesidad de profundizar en los detalles de implementación. Entre las principales características de Keras destaca:</p>
<ul>
<li>Simplicidad: tiene una sintaxis intuitiva y fácil de aprender, lo que la hace ideal para principiantes y para desarrollar prototipos de modelos rápidamente</li>
<li>Facilidad de uso: ofrece una API de alto nivel que abstrae las complejidades de las bibliotecas subyacentes, como TensorFlow o PyTorch, lo que permite a los usuarios centrarse en la construcción y el entrenamiento de modelos sin necesidad de profundizar en los detalles de implementación</li>
<li>Flexibilidad: permite construir una amplia gama de modelos de Deep Learning, desde redes neuronales convolucionales y recurrentes hasta modelos de atención y redes generativas adversarias (GANs)</li>
<li>Modularidad: al ser una biblioteca modular que permite a los usuarios combinar diferentes componentes para construir sus modelos personalizados</li>
<li>Soporte para múltiples plataformas: se puede utilizar en una variedad de plataformas, incluyendo Windows, macOS y Linux.</li>
</ul>
<p><strong>JAX</strong></p>
<p>JAX, desarrollada por Google Research, se posiciona como una biblioteca de Python para el aprendizaje automático y el cálculo numérico, diseñada para ofrecer un rendimiento y una flexibilidad excepcionales, especialmente en el entrenamiento de modelos de deep learning en aceleradores como GPUs y TPUs.</p>
<p>Su enfoque se basa en la composición de funciones puras y transformaciones automáticas de gradiente, lo que la convierte en una herramienta ideal para implementar algoritmos de aprendizaje automático diferenciables y de alto rendimiento. Entre sus características destacadas encontramos:</p>
<ul>
<li>Autodiferenciación: calcula automáticamente gradientes (autodiferenciación), simplificando el desarrollo de modelos de deep learning</li>
<li>Composición eficiente de transformaciones: combina operaciones elementales en funciones compuestas para un procesamiento eficiente</li>
<li>Integración con frameworks: se integra con frameworks de deep learning como TensorFlow y PyTorch, aprovechando las ventajas de cada uno</li>
<li>Paralelización y distribución: permite ejecutar operaciones en paralelo y de manera distribuida, ideal para grandes conjuntos de datos</li>
<li>Altas prestaciones para el entrenamiento de modelos: sobresale por su capacidad de computación de alto rendimiento, haciéndola ideal para entrenar modelos de deep learning complejos de manera eficiente. Así, se ha convertido en una opción atractiva para aquellos que manejan grandes conjuntos de datos y buscan optimizar el tiempo de entrenamiento</li>
<li>Flexibilidad para la investigación y experimentación: facilita la implementación de nuevas arquitecturas y algoritmos, permitiendo explorar diferentes enfoques y optimizar el rendimiento de los modelos</li>
<li>Personalización de flujos de trabajo: permite definir funciones y transformaciones personalizadas, proporcionando un control preciso sobre el pipeline de trabajo. Esto resulta útil para adaptar el proceso de entrenamiento a necesidades específicas y optimizar el rendimiento para tareas concretas</li>
</ul>
<p><strong>Mxnet</strong></p>
<p>MXNet es una biblioteca de código abierto para el aprendizaje automático desarrollada por Apache Software Foundation. Es conocida por su escalabilidad, flexibilidad y soporte para múltiples lenguajes de programación, incluyendo Python, R y C++. MXNet ofrece características similares a TensorFlow y PyTorch, incluyendo:</p>
<ul>
<li>Soporte para una variedad de arquitecturas de redes neuronales: permite construir una amplia gama de arquitecturas de redes neuronales, desde redes convolucionales y recurrentes hasta modelos de atención y GANs</li>
<li>Escalabilidad a grandes conjuntos de datos: está diseñado para manejar grandes conjuntos de datos y puede distribuirse en múltiples GPUs o TPU para acelerar el entrenamiento de modelos</li>
<li>Soporte para múltiples lenguajes de programación: se puede utilizar con Python, R y C++, lo que lo hace accesible a una amplia gama de desarrolladores</li>
<li>Flexibilidad: permite a los usuarios personalizar y extender la biblioteca para satisfacer sus necesidades específicas</li>
</ul>
<p><strong>Caffe</strong></p>
<p>Caffe es un marco de código abierto para el aprendizaje profundo desarrollado por la Universidad de California, Berkeley. Es conocido por su simplicidad, velocidad y eficiencia, lo que lo convierte en una opción popular para aplicaciones de Deep Learning en tiempo real. Caffe ofrece características similares a TensorFlow y PyTorch, incluyendo:</p>
<ul>
<li>Soporte para una variedad de arquitecturas de redes neuronales: permite construir una amplia gama de arquitecturas de redes neuronales, desde redes convolucionales y recurrentes hasta modelos de atención y GANs</li>
<li>Entrenamiento rápido y eficiente: está optimizado para el rendimiento y la eficiencia, lo que lo hace ideal para aplicaciones de aprendizaje profundo</li>
</ul>
<section id="google-colab" class="level4" data-number="1.2.2.1">
<h4 data-number="1.2.2.1" class="anchored" data-anchor-id="google-colab"><span class="header-section-number">1.2.2.1</span> Google Colab</h4>
<p>El entorno Colab (Google Colaboratory) es una potente herramienta de google para ejecutar código incluido el deep Dearning y que está disponible en la web (<a href="https://colab.research.google.com/" class="uri">https://colab.research.google.com/</a>). Se ha desarrollado para Python, pero actualmente también se puede ejecutar código de R. Esta funcionalidad puede importar un conjunto de datos de imágenes, entrenar un clasificador con este conjunto de datos y evaluar el modelo con tan solo usar unas pocas líneas de código. Los cuadernos de Colab ejecutan código en los servidores en la nube de Google, lo que nos permite aprovechar la potencia del hardware de Google, incluidas las GPU y TPU, independientemente de la potencia de tu equipo. Lo único que se necesita es un navegador.</p>
<p>Con Colab se puede aprovechar toda la potencia de las bibliotecas más populares de Python para analizar y visualizar datos. La celda de código de abajo utiliza NumPy para generar datos aleatorios y Matplotlib para visualizarlos. Para editar el código, solo se tiene que hacer clic en la celda.</p>
<p><img src="imagenes/capitulo1/colab_1.png" class="img-fluid" alt="Google Colab">{#fig-colab_1]</p>
<p>Este es el menú principal de colab desde donde podemos gestionar nuestros proyectos:</p>
<p><img src="imagenes/capitulo1/colab_nuevo_fichero.png" class="img-fluid" alt="Google Colab Nuevo">{#fig-colab_nuevo_fichero]</p>
<p>Desde el <code>menú Archivo</code>, como en la mayor parte de los programas, podemos llevar a cabo las operaciones habituales de abrir y guardar los ficheros en diferentes formatos. En este caso se pueden abrir ficheros de Jupyter/Python desde cualquier dispositivo externo, desde el repositorio Drive o de Github:</p>
<p><img src="imagenes/capitulo1/colab_importa.png" class="img-fluid" alt="Importación a Google Colab">{#fig-colab_importa]</p>
<p>Si queremos subir un fichero que tenemos en nuestro ordenador vamos a <code>Archivo/Subir</code> cuaderno y podemos elegir nuestro archivo cuando se despliegue la siguiente pantalla:</p>
<p><img src="imagenes/capitulo1/colab_importa_archivo.png" class="img-fluid" alt="Abrir Google Colab">{#fig-colab_importa_archivo]</p>
<p>Como se ha comentado también se pueden importar archivos desde GitHub introduciendo la url de GitHub:</p>
<p><img src="imagenes/capitulo1/colab_importa_github.png" class="img-fluid" alt="Seleccionar Fichero">{#fig-colab_importa_github]</p>
<p>Por último, <em>colab</em> nos permite tener acceso tanto a GPUs de forma como a CPUs más potentes que nuestro ordenador de escritorio de forma gratuita.</p>
<p><img src="imagenes/capitulo1/colab_entorno_ejecucion_inicio.png" class="img-fluid" alt="Ejecutar Notebook Google Colab">{#fig-colab_entorno_ejecucion_inicio]</p>
</section>
</section>
</section>
<section id="conceptos-básicos-de-las-redes-neuronales" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="conceptos-básicos-de-las-redes-neuronales"><span class="header-section-number">1.3</span> Conceptos básicos de las Redes Neuronales</h2>
<p>Vamos a hacer una revisión de las redes neuronales para posteriormente poder abordar los diferentes tipos de redes neuronales que se utilizan en Deep Learning. Algunos de los avances más recientes en varios de los diferentes componentes que forman parte de las redes neuronales están recopilados en (Gu et al.&nbsp;2017)</p>
<p>Las redes neuronales artificiales tienen sus orígenes en el Perceptrón, que fue el modelo creado por Frank Rosenblatt en 1957 y basado en los trabajos que previamente habían realizado Warren McCullon (neurofisiólogo) y Walter Pitts (matemático).</p>
<p>El Perceptrón está construido por una neurona artificial cuyas entradas y salida pueden ser datos numéricos, no como pasaba con la neurona de McCulloch y Pitts (eran sólo datos lógicos). Las neuronas pueden tener pesos y además se le aplica una función de activación Sigmoid (a diferencia de la usada anteriormente al Paso binario).</p>
<p>En esta neurona nos encontramos que se realizan los siguientes cálculos: <span class="math display">\[ z = \sum_{i=1}^{n}w_ix_i+b_i\]</span> <span class="math display">\[\hat{y} = \delta (z)\]</span> donde representan los datos numéricos de entrada, son los pesos, es el sesgo (bias), es la función de activación y finalmente es el dato de salida.</p>
<p>El modelo de perceptrón es el más simple, en el que hay una sola capa oculta con una única neurona.</p>
<p>El siguiente paso nos lleva al Perceptrón Multicapa donde ya pasamos a tener más de una capa oculta, y además podemos tener múltiples neuronas en cada capa oculta.</p>
<p>Cuando todas las neuronas de una capa están interconectadas con todas las de la siguiente capa estamos ante una red neuronal densamente conectada. A lo largo de las siguientes secciones nos encontraremos con redes en las que no todas las neuronas de una capa se conectan con todas de la siguiente.</p>
<p>Veamos como describiríamos ahora los resultados de las capas <span class="math display">\[
z_j^{(l)}=\sum_{i=1}^{n_j} w_{i j}^{(l)} a_i^{(l-1)}+b_i^{(l)} \\
a_j^{(l)}=\delta^{(l)}\left(z_j^{(l)}\right)
\]</span> donde <span class="math inline">\(a_i^{(l-1)}\)</span> representan los datos de la neurona <span class="math inline">\(i\)</span> en la capa <span class="math inline">\(l-1\)</span> ( siendo <span class="math inline">\(a_i^0=x_i\)</span> los valores de entrada), <span class="math inline">\(w_{i j}^{(l)}\)</span> son los pesos en la capa <span class="math inline">\(l\)</span>, <span class="math inline">\(b_i^{(l)}\)</span> es el sesgo (bias) en la capa <span class="math inline">\(l\)</span>, <span class="math inline">\(\delta^{(l)}\)</span> es la función de activación en la capa <span class="math inline">\(l\)</span> (puede que cada capa tenga una función de activación diferente), <span class="math inline">\(n_j\)</span> es el número de neurona de la capa anterior que conectan con la <span class="math inline">\(j\)</span> y finalmente <span class="math inline">\(a_j^{(l)}\)</span> es el dato de salida de la capa <span class="math inline">\(l\)</span>. Es decir, en cada capa para calcular el nuevo valor necesitamos usar los valores de la capa anterior.</p>
<p><strong>Aplicaciones de las Redes Neuronales</strong></p>
<p>Cada día las redes neuronales están más presentes en diferentes campos y ayudan a resolver una gran variedad de problemas. Podríamos pensar que de forma más básica una red neuronal nos puede ayudar a resolver problemas de regresión y clasificación, es decir, podríamos considerarlo como otro modelo más de los existentes que a partir de unos datos de entrada somos capaces de obtener o un dato numérico (o varios) para hacer una regresión (calcular en precio de una vivienda en función de diferentes valores de la misma) o que somos capaces de conseguir que en función de los datos de entrada nos deje clasificada una muestra (decidir si conceder o no una hipoteca en función de diferentes datos del cliente).</p>
<p>Si los datos de entrada son imágenes podríamos estar usando las redes neuronales como una forma de identificar esa imagen:</p>
<ul>
<li><p>Identificando que tipo de animal es</p></li>
<li><p>Identificando que señal de tráfico es</p></li>
<li><p>Identificando que tipo de fruta es</p></li>
<li><p>Identificando que una imagen es de exterior o interior de una casa</p></li>
<li><p>Identificando que es una cara de una persona</p></li>
<li><p>Identificando que una imagen radiográfica represente un tumor maligno</p></li>
<li><p>Identificando que haya texto en una imagen</p></li>
</ul>
<p>Luego podríamos pasar a revolver problemas más complejos combinando las capacidades anteriores:</p>
<ul>
<li><p>Detectar los diferentes objetos y personas que se encuentran en una imagen</p></li>
<li><p>Etiquedado de escenas (aula con alumnos, partido de futbol, etc…)</p></li>
</ul>
<p>Después podríamos dar el paso al video que lo podríamos considerar como una secuencia de imágenes:</p>
<ul>
<li><p>Contar el número de personas que entran y salen de una habitación</p></li>
<li><p>Reconocer que es una carretera</p></li>
<li><p>Identificar las señales de tráfico</p></li>
<li><p>Detectar si alguien lleva un arma</p></li>
<li><p>Seguimiento de objetos</p></li>
<li><p>Detección de estado/actitud de una persona</p></li>
<li><p>Reconocimiento de acciones (interpretar lenguaje de signos, interpretar lenguaje de banderas)</p></li>
<li><p>Vehículos inteligentes</p></li>
</ul>
<p>Si los datos de entrada son secuencias de texto</p>
<ul>
<li><p>Sistemas de traducción - Chatbots (resolución de preguntas a usuarios)</p></li>
<li><p>Conversión de texto a audio</p></li>
</ul>
<p>Si los datos de entrada son audios</p>
<ul>
<li><p>Sistemas de traducción</p></li>
<li><p>Altavoces inteligentes</p></li>
<li><p>Conversión de audio a texto</p></li>
</ul>
<p>A continuación, pasamos a revisar diferentes elementos de las redes neuronales que suelen ser comunes a todos los tipos de redes neuronales.</p>
<section id="datos" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="datos"><span class="header-section-number">1.3.1</span> Datos</h3>
<p>Cuando se trabaja con redes neuronales necesitamos representar los valores de las variables de entrada en forma numérica. En una red neuronal todos los datos son siempre numéricos. Esto significa que todas aquellas variables que sean categóricas necesitamos convertirlas en numéricas.</p>
<p>Además, es muy conveniente normalizar los datos para poder trabajar con valores entre 0 y 1, que van a ayudar a que sea más fácil que se pueda converger a la solución. Es importante que los datos seán números en coma flotante, sobre todo si se van a trabajar con GPUs (Graphics Process Units), ya que permitirán hacer un mejor uso de los multiples cores que les permiten operar en coma flotante de forma paralela. Actualmente, hay toda una serie de mejoras en las GPUs que permite aumentar el rendimiento de las redes neuronales como son el uso de operaciones en FP16 (Floating Point de 16 bits en lugar de 32) de forma que pueden hacer dos operaciones de forma simultánea (el formato estándar es FP32) y además con la reducción de memoria (punto muy importante) al meter en los 32 bits 2 datos en lugar de sólo uno. También se han añadido técnicas de Mixed Precision (Narang et al.&nbsp;2018), los Tensor Cores (para las gráficas de NVIDIA) son otra de las mejoras que se han ido incorporando a la GPUs y que permiten acelerar los procesos tanto de entrenamiento como de predicción con las redes neuronales.</p>
<p>El primer objetivo será convertir las variables categóricas en variables numéricas, de forma la red neuronal pueda trabajar con ellas. Para realizar la conversión de categórica a numérica básicamente tenemos dos métodos para realizarlo:</p>
<ul>
<li><p>Codificación one-hot.</p></li>
<li><p>Codificación entera.</p></li>
</ul>
<p>La <strong>codificación one-hot</strong> consiste en crear tantas variables como categorías tenga la variable, de forma que se asigna el valor 1 si tiene esa categoría y el 0 si no la tiene.</p>
<p>La <strong>codificación entera</strong> lo que hace es codificar con un número cada categoría. Realmente esta asignación no tiene ninguna interpretación numérica ya que en general las categorías no tienen porque representar un orden al que asociarlas.</p>
<p>Normalmente se trabaja con codificación one-hot para representar los datos categóricos de forma que será necesario preprocesar los datos de partida para realizar esta conversión, creando tantas variables como categorías haya por cada variable.</p>
<p>Si nosotros tenemos nuestra muestra de datos que tiene <span class="math inline">\(n\)</span> variables <span class="math inline">\(x=\{x_1,x_2,...,x_n\}\)</span> de forma que <span class="math inline">\(x_{n-2},x_{n-1},x_n\)</span> son variables categóricas que tienen <span class="math inline">\(k,l,m\)</span> número de categorías respectivamente, tendremos finalmente las siguientes variables sólo numéricas: <span class="math display">\[ x=\{x_1,x_2,...,x_{(n-2)_1},...,x_{(n-2)_k},x_{(n-1)_1},...,x_{(n-1)_l},x_{n_1},...,x_{n_m}\} \]</span></p>
<p>De esta forma, se aumentarán el número de variables con las que vamos a trabajar en función de las categorías que tengan las variables categóricas. Normalmente nos encontramos que en una red neuronal las variables de salida son:</p>
<ul>
<li>un número (regresión)</li>
<li>una serie de números (regresión múltiple)</li>
<li>un dato binario (clasificación binaria)</li>
<li>una serie de datos binarios que representa una categoría de varias (clasifiación múltiple)</li>
</ul>
</section>
<section id="arquitectura-de-red" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="arquitectura-de-red"><span class="header-section-number">1.3.2</span> Arquitectura de red</h3>
<p>Para la construcción de una red neuronal necesitamos definir la arquitectura de esa red. Esta arquitectura, si estamos pensando en una red neuronal densamente conectada, estará definida por la cantidad de capas ocultas y el número de neuronas que tenemos en cada capa. Más adelante veremos que dependiendo del tipo de red neuronal podrá haber otro tipo de elementos en estas capas.</p>
</section>
<section id="función-de-coste-y-pérdida" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="función-de-coste-y-pérdida"><span class="header-section-number">1.3.3</span> Función de coste y pérdida</h3>
<p>Otro de los elementos clave que tenemos que tener en cuenta a la hora de usar nuestra red neuronal son las <strong>funciones de pérdida y funciones de coste (objetivo)</strong>.</p>
<p><strong>La función de pérdida</strong> va a ser la función que nos dice cómo de diferente es el resultado del dato que nosotros queríamos conseguir respecto al dato original. Normalmente se suelen usar diferentes tipos de funciones de pérdida en función del tipo de resultado con el que se vaya a trabajar.</p>
<p><strong>La función de coste</strong> es la función que vamos a tener que <strong>optimizar</strong> para conseguir el mínimo valor posible, y que recoge el valor de la función de pérdida para toda la muestra.</p>
<p>Tanto las funciones de pérdida como las funciones de coste, son funciones que devuelven valores de de <span class="math inline">\(\mathbb{R}\)</span>..</p>
<p>Si tenemos un problema de <strong>regresión</strong> en el que tenemos que predecir un valor o varios valores numéricos, algunas de las funciones a usar son:</p>
<ul>
<li><strong>Error medio cuadrático</strong> <span class="math inline">\(\left(\mathrm{L}_2^2\right)\)</span></li>
</ul>
<p><span class="math display">\[
\mathcal{L}_{\text {MSE }}(\mathrm{y}, \hat{\mathrm{y}})=\|\hat{\mathrm{y}}-\mathrm{y}\|^2=\sum_{\mathrm{i}=1}^{\mathrm{n}}\left(\hat{\mathrm{y}}_{\mathrm{i}}-\mathrm{y}_{\mathrm{i}}\right)^2
\]</span> donde <span class="math inline">\(\hat{y}\)</span> y y son vectores de tamaño <span class="math inline">\(n, y\)</span> es el valor real e <span class="math inline">\(\hat{y}\)</span> es el valor predicho</p>
<ul>
<li><strong>Error medio absoluto (</strong> <span class="math inline">\(\mathrm{L}_1\)</span> )</li>
</ul>
<p><span class="math display">\[
\mathcal{L}_{\text {MAE }}(\mathrm{y}, \hat{y})=|\hat{y}-y|=\sum_{i=0}^n\left|\hat{y}_i-y_i\right|
\]</span> donde <span class="math inline">\(\hat{y}\)</span> y y son vectores de tamaño <span class="math inline">\(n, y\)</span> es el valor real e <span class="math inline">\(\hat{y}\)</span> es el valor predicho</p>
<p>Para los problemas de <strong>clasifiación</strong>:</p>
<ul>
<li><strong>Binary Crossentropy (Sólo hay dos clases)</strong> <span class="math display">\[
\mathcal{L}_{\text {CRE }}(\mathrm{y}, \hat{y})=-(\mathrm{y} \log (\hat{y})+(1-\mathrm{y}) \log (1-\hat{y}))
\]</span></li>
</ul>
<p><span class="math inline">\(\mathrm{y}\)</span> es el valor real e <span class="math inline">\(\hat{y}\)</span> es el valor predicho</p>
<ul>
<li><strong>Categorical Crosentropy (Múltiples clases representadas como one-hot)</strong></li>
</ul>
<p><span class="math display">\[
\mathcal{L}_{\text {CAE }}\left(\mathrm{y}_{\mathrm{c}}, \hat{\mathrm{y}}_{\mathrm{c}}\right)=-\sum_{\mathrm{c}=1}^{\mathrm{k}} \mathrm{y}_{\mathrm{c}} \log \left(\hat{y}_c\right)
\]</span></p>
<p><span class="math inline">\(y_c\)</span> es el valor real para la clase <span class="math inline">\(c\)</span> e <span class="math inline">\(\hat{y}_c\)</span> es el valor predicho para la clase <span class="math inline">\(c\)</span></p>
<ul>
<li><strong>Sparse Categorical Crossentropy (Múltiples clases representadas comp un entero)</strong></li>
</ul>
<p><span class="math display">\[
\mathcal{L}_{\text {SCAE }}\left(\mathrm{y}_{\mathrm{c}}, \hat{\mathrm{y}}_{\mathrm{c}}\right)=-\sum_{\mathrm{c}=1}^{\mathrm{k}} \mathrm{y}_{\mathrm{c}} \log \left(\hat{y}_{\mathrm{c}}\right)
\]</span></p>
<p><span class="math inline">\(\mathrm{y}_c\)</span> es el valor real para la clase <span class="math inline">\(c\)</span> e <span class="math inline">\(\hat{y}_c\)</span> es el valor predicho para la clase <span class="math inline">\(c\)</span></p>
<ul>
<li><strong>Kullback-Leibler Divergence</strong></li>
</ul>
<p>Esta función se usa para calcular la diferencia entre dos distribuciones de probabilidad se usa por ejemplo en algunas redes como <strong>Variational Autoencoders</strong> (Doersch 2016 Modelos GAN (Generative Adversarial Networks)</p>
<p><span class="math display">\[
\mathcal{D}_{\mathrm{KL}}(\mathrm{p} \| \mathrm{q})=-\mathrm{H}(\mathrm{p}(\mathrm{x}))-\mathrm{E}_{\mathrm{p}}[\log \mathrm{q}(\mathrm{x})]
\]</span></p>
<p><span class="math display">\[
=\sum_x p(x) \operatorname{logp}(x)-\sum_x p(x) \log q(x)=\sum_x p(x) \log \frac{p(x)}{q(x)}
\]</span></p>
<p><span class="math display">\[
\mathcal{L}_{\text {vae }}(y, \hat{y})=E_{z \sim q_\phi(z \mid x)}\left[\operatorname{logp}_\theta(x \mid z)\right]-\mathcal{D}_{\text {KL }}\left(q_\phi(z \mid x) \| p(z)\right)
\]</span></p>
<ul>
<li><strong>Hinge Loss</strong></li>
</ul>
<p><span class="math display">\[
\mathcal{L}_{\text {hinge }}(\mathrm{y}, \hat{y})=\max (0,1-\mathrm{y} * \hat{\mathrm{y}})
\]</span></p>
<p>Las correspondientes <strong>funciones de coste</strong> que se usarían, estarían asociadas a todas las muestras que se estén entrenando o sus correpondientes batch, así como posibles términos asociados a la regularización para evitar el sobreajuste del entrenamiento. Es decir, la función de pérdida se calcula para cada muestra, y la función de coste es la media de todas las muestras.</p>
<p>Por ejemplo, para el <strong>Error medio cuadrático</strong> <span class="math inline">\(\left(L_2\right)\)</span> tendríamos el siguiente valor: <span class="math display">\[
\mathcal{J}_{\text {MSB }}(y, \hat{y})=\frac{1}{m} \sum_{i=1}^m \mathcal{L}_{\text {MSE }}(y, \hat{y})=\frac{1}{m} \sum_{i=1}^m|| \hat{y}_i-y_i \|^2=\frac{1}{n} \sum_{i=1}^m \sum_{i=1}^n\left(\hat{y}_{j i}-y_{j i}\right)^2
\]</span></p>
</section>
<section id="optimizador" class="level3" data-number="1.3.4">
<h3 data-number="1.3.4" class="anchored" data-anchor-id="optimizador"><span class="header-section-number">1.3.4</span> Optimizador</h3>
<p>El <strong>Descenso del gradiente</strong> es la versión más básica de los algoritmos que permiten el aprendizaje en la red neuronal haciendo el proceso de <strong>backpropagation</strong> (propagación hacia atrás). A continuación veremos una breve explicación del algoritmo así como algunas variantes del mismo recogidas en (Ruder 2017).</p>
<p>Recordamos que el descenso del gradiente nos permitirá actualizar los parámetros de la red neuronal cada vez que demos una pasada hacia delante con todos los datos de entrada, volviendo con una pasada hacia atrás.</p>
<p><span class="math display">\[\mathrm{w}_{\mathrm{t}}=\mathrm{w}_{\mathrm{t}-1}-\alpha \nabla_{\mathrm{w}} \mathcal{J}(\mathrm{w})\]</span></p>
<p>donde <span class="math inline">\(\mathcal{J}\)</span> es la <strong>función de coste</strong>, <span class="math inline">\(\alpha\)</span> es el parámetro de <strong>ratio de aprendizaje</strong> que permite definir como de grandes se quiere que sean los pasos en el aprendizaje.</p>
<p>Cuando lo que hacemos es actualizar los parámetros para cada pasada hacia delante de una sola muestra, estaremos ante lo que llamamos <strong>Stochastic Gradient</strong> Descent (SGD). En este proceso convergerá en menos iteraciones, aunque puede tener alta varianza en los parámetros.</p>
<p><span class="math display">\[\mathrm{W}_{\mathrm{t}}=\mathrm{w}_{\mathrm{t}-1}-\alpha \nabla_{\mathrm{w}} \mathcal{J}(\mathrm{w}, x(i),y(i))\]</span></p>
<p>donde <span class="math inline">\(x(i)\)</span> e <span class="math inline">\(y(i)\)</span> son los valores en la pasada de la muestra <span class="math inline">\(i\)</span>.</p>
<p>Podemos buscar un punto intermedio que sería cuando trabajamos por lotes y cogemos un bloque de datos de la muestra, les aplicamos la pasada hacia delante y aprendemos los parámetros para ese bloque. En este caso lo llamaremos <strong>Mini-batch Gradient Descent</strong></p>
<p><span class="math display">\[\mathrm{W}_{\mathrm{t}}=\mathrm{w}_{\mathrm{t}-1}-\alpha \nabla_{\mathrm{w}} \mathcal{J}(\mathrm{w}, \mathrm{B}(\mathrm{i}))\]</span></p>
<p>donde <span class="math inline">\(\mathrm{B}(\mathrm{i})\)</span> son los valores de ese batch .</p>
<p>En general a estos métodos nos referiremos a ellos como <strong>SGD</strong>.</p>
<p>Sobre este algoritmo base se han hecho ciertas mejoras como:</p>
<p><strong>Learning rate decay</strong> Podemos definir un valor de decenso del ratio de aprendizaje, de forma que normalmente al inicio de las iteraciones de la red neuronal los pasos serán más grandes, pero conforme nos acercamos a la solución optima deberemos dar pasos más pequeños para ajustarnos mejor.</p>
<p><span class="math display">\[\mathrm{W}_{\mathrm{t}}=\mathrm{w}_{\mathrm{t}-1}-\alpha_{\mathrm{t}} \nabla_{\mathrm{w}} \mathcal{J}\left(\mathrm{w}_{\mathrm{t}-1}\right)\]</span></p>
<p>donde <span class="math inline">\(\alpha _t\)</span> ahora se irá reduciendo en función del valor del <strong>decay</strong>.</p>
<p><strong>Momentum</strong> El <strong>momentum</strong> se introdujo para suavizar la convergencia y reducir la alta varianza de SGD.</p>
<p><span class="math display">\[ V_ {t}  =  \gamma   v_ {t-1}  +  \alpha  V_ {w} J(  w_ {t-1}  ,x,y)\]</span> <span class="math display">\[ W_ {t} =  w_ {t-1}  -  v_ {1} \]</span></p>
<p>donde <span class="math inline">\(v_t\)</span> es lo que se llama el <strong>vector velocidad</strong> con la dirección correcta.</p>
<p><strong>NAG (Nesterov Accelerated Gradient)</strong> Ahora daremos un paso más con el NAG, calculando la función de coste junto con el vector velocidad.</p>
<p><span class="math display">\[ V_ {t}  =  \gamma   v_ {t-1}  +  \alpha   V_ {w}  J(  w_ {t-1}  -  \gamma   v_ {t-1}  ,x,y) \]</span> <span class="math display">\[ W_ {t}  =  w_ {t-1}  -  v_ {t}  \]</span></p>
<p>donde ahora vemos que la función de coste se calcula usando los parámetros de <span class="math inline">\(w_t\)</span> sumado a <span class="math inline">\(\gamma   v_ {t-1}\)</span></p>
<p>Veamos algunos algoritmos de optimización más que, aunque provienen del SGD, se consideran independientes a la hora de usarlos y no como parámetros extras del SGD.</p>
<p><strong>Adagrad (Adaptive Gradient)</strong> Esta variante del algoritmo lo que hace es adaptar el ratio de aprendizaje para cada uno de los pesos en lugar de que sea global para todos.</p>
<p><span class="math display">\[ W_ {t,i}  =  w_ {t-1,i}  -  \frac {\alpha }{\sqrt {G_ {t-1,i,j}+\epsilon }}   \nabla_ {w_{t-1}}  J(  w_ {t-1,i} ,x,y) \]</span></p>
<p>donde tenemos que <span class="math inline">\(G_t \in R^{dxd}\)</span>$es una matriz diagonal donde cada elemento es la suma de los cuadrados de los gradientes en el paso <span class="math inline">\(t-1\)</span> , y es un término de suavizado para evitar divisiones por 0.</p>
<p><strong>RMSEProp (Root Mean Square Propagation)</strong> En este caso tenemos una variación del Adagrad en el que intenta reducir su agresividad reduciendo monotonamente el ratio de aprendizaje. En lugar de usar el gradiente acumulado desde el principio de la ejecución, se restringe a una ventana de tamaño fijo para los últimos n gradientes calculando su media. Así calcularemos primero la media en ejecución de los cuadros de los gradientes como:</p>
<p><span class="math display">\[
\mathrm{E}\left[g^2\right]_{t-1}=\gamma E\left[g^2\right]_{t-2}+(1-\gamma) g_{t-1}^2
\]</span></p>
<p>y luego ya pasaremos a usar este valor en la actualización</p>
<p><span class="math display">\[
w_{t, i}=w_{t-1, i}-\frac{\alpha}{\sqrt{E\left[ g^2\right]_{t-1}+\epsilon}} \nabla_{w_{t-1}} \mathcal{J}\left(w_{t-1, i}, x, y\right)
\]</span></p>
<p><strong>AdaDelta</strong></p>
<p>Aunque se desarrollaron de forma simultánea el AdaDelta y el RMSProp son muy parecidos en su primer paso incial, llegando el de AdaDelta un poco más lejos en su desarrollo.</p>
<p><span class="math display">\[
w_{t, i}=w_{t-1, i}-\frac{\alpha}{\sqrt{E\left[ g^2\right]_{t-1}+\epsilon}} \nabla_{w_{t-1}} \mathcal{J}\left(w_{t-1, i}, x, y\right)
\]</span></p>
<p>y luego ya pasaremos a usar este valor en la actualización</p>
<p><span class="math display">\[
\begin{gathered}w_{t, i}=w_{t-1, i}-\frac{\alpha}{\sqrt{E\left[g^2\right]_{t-1}+\epsilon}} \nabla_{w_{t-1}} \mathcal{J}\left(w_{\mathrm{t}-1, \mathrm{i}}, \mathrm{X}, \mathrm{y}\right) \\ \Delta w_{\mathrm{t}}=-\frac{\alpha}{\sqrt{\mathrm{E}\left[g^2\right]_{\mathrm{t}}+\epsilon}} g_t\end{gathered}
\]</span></p>
<p><strong>Adam (Adaptive Moment Estimation)</strong></p>
<p><span class="math display">\[
\begin{gathered}G_t=\nabla_{w_t} \mathcal{J}\left(w_t\right) \\ M_{t-1}=\beta_1 m_{t-2}+\left(1-\beta_1\right) g_{t-1} \\ v_{t-1}=\beta_2 v_{t-2}+\left(1-\beta_2\right) g_{t-1}^2\end{gathered}
\]</span></p>
<p>donde <span class="math inline">\(m_{t-1}\)</span> y <span class="math inline">\(V_{t-1}\)</span> son estimaciones del primer y segundo momento de los gradientes respectivamente, y <span class="math inline">\(\beta_1\)</span> y <span class="math inline">\(\beta_2\)</span> parámetros a asignar.</p>
<p><span class="math display">\[\widehat{M}_{t-1}  =\frac{m_{t-1}}{1-\beta_1^{t-1}} \\  \widehat{V}_{t-1}  =\frac{v_{t-1}}{1-\beta_2^{t-1}} \\  W_t=w_{t-1}  -\frac{\alpha}{\sqrt{\hat{v}_{t-1}+\epsilon}} \widehat{m}_{t-1}\]</span></p>
<p><strong>Adamax</strong></p>
<p><span class="math display">\[
G_t=\nabla_{w_t} \mathcal{J}\left(w_t\right) \\
M_{t-1}=\beta_1 m_{t-2}+\left(1-\beta_1\right) g_{t-1} \\
\mathrm{~V}_{\mathrm{t}-1}=\beta_2 \mathrm{v}_{\mathrm{t}-2}+\left(1-\beta_2\right) \mathrm{g}_{\mathrm{t}-1}^2 \\
\mathrm{U}_{\mathrm{t}-1}=\max \left(\beta_2 \cdot \mathrm{v}_{\mathrm{t}-1},\left|\mathrm{~g}_{\mathrm{t}}\right|\right)
\]</span></p>
<p>donde <span class="math inline">\(m_{t-1}\)</span> y <span class="math inline">\(V_{t-1}\)</span> son estimaciones del primer y segundo momento de los gradientes respectivamente, y <span class="math inline">\(\beta_1\)</span> y <span class="math inline">\(\beta_2\)</span> parámetros a asignar.</p>
<p><span class="math display">\[
\widehat{M}_{t-1}=\frac{m_{t-1}}{1-\beta_1^{t-1}} \\
W_t=w_{t-1}-\frac{\alpha}{u_{t-1}} \widehat{m}_{t-1}
\]</span></p>
<p><strong>Nadam (Nesterov-accelerated Adaptive Moment Estimatio)</strong> Combina Adam y NAG.</p>
<p><span class="math display">\[
\begin{aligned} G_t &amp; =\nabla_{w_t} \mathcal{J}\left(w_t\right) \\ M_{t-1} &amp; =\gamma m_{t-2}+\alpha g_{t-1} \\ w_t &amp; =w_{t-1}-m_{t-1}\end{aligned}
\]</span></p>
</section>
<section id="función-de-activación" class="level3" data-number="1.3.5">
<h3 data-number="1.3.5" class="anchored" data-anchor-id="función-de-activación"><span class="header-section-number">1.3.5</span> Función de activación</h3>
<p>Las funciones de activación dentro de una red neuronal son uno de los elementos clave en el diseño de la misma. Cada tipo de función de activación podrá ayudar a la convergencia de forma más o menos rápida en función del tipo de problema que se plantee. En una red neuronal las funciones de activación en las capas ocultas van a conseguir establecer las restricciones <strong>no lineales</strong> al pasar de una capa a la siguiente, normalmente se evita usar la función de activación lineal en las capas intermedias ya que queremos conseguir transformaciones no lineales.</p>
<p>A continuación, exponemos las principales funciones de activación en las capas ocultas:</p>
<ul>
<li><strong>Paso binario</strong> (Usado por los primeros modelos de neuronas)</li>
</ul>
<p><span class="math inline">\(F(x)= \begin{cases}0 &amp; \text { for } x \leq 0 \\ x &amp; \text { for } x&gt;0\end{cases}\)</span></p>
<ul>
<li><strong>Identidad</strong></li>
</ul>
<p><span class="math inline">\(F(x)=x\)</span></p>
<ul>
<li><strong>Sigmoid (Logística)</strong></li>
</ul>
<p><span class="math inline">\(F(x)=\frac{1}{1+e^{-x}}\)</span></p>
<ul>
<li><strong>Tangente Hiperbólica (Tanh)</strong></li>
</ul>
<p><span class="math inline">\(F(x)=\tanh (x)=\frac{\left(e^x-e^{-x}\right)}{\left(e^x+e^{-x}\right)}\)</span></p>
<ul>
<li><strong>Softmax</strong></li>
</ul>
<p><span class="math inline">\(F\left(x_i\right)=\frac{e^{x_i}}{\sum_{j=0}^k e^{x_j}}\)</span></p>
<ul>
<li><p><strong>ReLu ( Rectified Linear Unit)</strong> <span class="math inline">\(\begin{aligned} &amp; F(x)=\max (0, x) \\ &amp; f(x)= \begin{cases}0 &amp; \text { for } x \leq 0 \\ x &amp; \text { for } x&gt;0\end{cases} \end{aligned}\)</span></p></li>
<li><p><strong>LReLU (Leaky Rectified Linear Unit)</strong> <span class="math inline">\(F(\alpha, x)= \begin{cases}\alpha x &amp; \text { for } x&lt;0 \\ x &amp; \text { for } x \geq 0\end{cases}\)</span></p></li>
<li><p><strong>PReLU (Parametric Rectified Linear Unit)</strong> <span class="math inline">\(F(\alpha, x)= \begin{cases}\alpha x &amp; \text { for } x&lt;0 \\ x &amp; \text { for } x \geq 0\end{cases}\)</span></p></li>
<li><p><strong>RReLU (Randomized Rectified Linear Unit)</strong> <span class="math inline">\(F(\alpha, x)= \begin{cases}\alpha x &amp; \text { for } x&lt;0 \\ x &amp; \text { for } x \geq 0\end{cases}\)</span></p></li>
</ul>
<p>*La diferencia entre LReLu, PReLu y RRLeLu es que en LReLu el parámetro es uno que se asigna fijo, en el caso de PReLu el parámetro también se aprende durante el entrenamiento y finalmente en RReLu es un parámetro con valores entre 0 y 1, que se obtiene de un muestreo en una distribución normal.</p>
<p>Se puede profundizar en este grupo de funciones de activación en (Xu et al.&nbsp;2015)</p>
<ul>
<li>ELU (Exponential Linear Unit) <span class="math inline">\(F(\alpha, x)= \begin{cases}\alpha\left(e^{x-1}\right) &amp; \text { for } x&lt;0 \\ x &amp; \text { for } x \geq 0\end{cases}\)</span></li>
</ul>
<div id="fig-funciones_activacion" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-funciones_activacion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/funciones_activacion.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-funciones_activacion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: Funciones ReLU
</figcaption>
</figure>
</div>
<p><strong>Función de activación en salida</strong></p>
<p>En la capa de salida tenemos que tener en cuenta cual es el tipo de datos final que queremos obtener, y en función de eso elegiremos cual es la función de activación de salida que usaremos. Normalmente las funciones de activación que se usarán en la última capa seran:</p>
<ul>
<li><p><strong>Lineal</strong> con una unidad, para regresión de un solo dato numérico <span class="math inline">\(F(x)=x\)</span> donde es un valor escalar.</p></li>
<li><p><strong>Lineal</strong> con multiples unidades, para regresión de varios datos numéricos <span class="math inline">\(F(x)=x\)</span> donde <span class="math inline">\(x\)</span> es un vector.</p></li>
<li><p><strong>Sigmoid</strong> para clasifiación binaria <span class="math inline">\(F(x)=\frac{1}{1+e^{-x}}\)</span></p></li>
<li><p><strong>Softmax</strong> para calsifiación múltiple <span class="math inline">\(F\left(x_i\right)=\frac{e^{x_i}}{\sum_{j=0}^k e^{x_j}}\)</span></p></li>
</ul>
</section>
<section id="regularización" class="level3" data-number="1.3.6">
<h3 data-number="1.3.6" class="anchored" data-anchor-id="regularización"><span class="header-section-number">1.3.6</span> Regularización</h3>
<p>Las técnicas de regularización nos permiten conseguir mejorar los problemas que tengamos por sobreajuste en el entrenamiento de nuestra red neuronal.</p>
<p>A continuación, vemos algunas de las técnicas de regularización existentes en la actualidad:</p>
<ul>
<li><strong>Norma LP</strong> Básicamente estos métodos tratan de hacer que los pesos de las neuronas tengan valores muy pequeños consiguiendo una distribución de pesos más regular. Esto lo consiguen al añadir a la función de pérdida un coste asociado a tener pesos grandes en las neuronas. Este peso se puede construir o bien con la <strong>norma L1</strong> (proporcional al valor absoluto) o con la <strong>norma L2</strong> (proporcional al cuadrado de los coeficientes de los pesos). En general se define la norma LP) <span class="math display">\[
\begin{gathered}
E(w, \mathbf{y}, \hat{\mathbf{y}})=\mathcal{L}(w, \mathbf{y}, \hat{\mathbf{y}})+\lambda R(w) \\
R(w)=\sum_j\left\|w_j\right\|_p^p
\end{gathered}
\]</span></li>
</ul>
<p>Para los casos más habituales tendríamos la norma <span class="math inline">\(\mathbf{L 1}\)</span> y <span class="math inline">\(\mathbf{L 2}\)</span>. <span class="math display">\[
\begin{aligned}
&amp; R(w)=\sum_j\left\|w_j\right\|^2 \\
&amp; R(w)=\sum_j\left|w_j\right|
\end{aligned}
\]</span></p>
</section>
<section id="dropout" class="level3" data-number="1.3.7">
<h3 data-number="1.3.7" class="anchored" data-anchor-id="dropout"><span class="header-section-number">1.3.7</span> Dropout</h3>
<p>Una de las técnicas de regularización que más se están usando actualmente es la llamada <strong>Dropout</strong>, su proceso es muy sencillo y consiste en que en cada iteración de forma aleatoria se dejan de usar un porcentaje de las neuronas de esa capa, de esta forma es más difícil conseguir un sobreajuste porque las neuronas no son capaces de memorizar parte de los datos de entrada.</p>
</section>
<section id="dropconnect" class="level3" data-number="1.3.8">
<h3 data-number="1.3.8" class="anchored" data-anchor-id="dropconnect"><span class="header-section-number">1.3.8</span> Dropconnect</h3>
<p>El Dropconnect es otra técnica que va un poco más allá del concepto de Dropout y en lugar de usar en cada capa de forma aleatoria una serie de neuronas, lo que se hace es que de forma aleatoria se ponen los pesos de la capa a cero. Es decir, lo que hacemos es que hay ciertos enlaces de alguna neurona de entrada con alguna de salida que no se activan.</p>
</section>
<section id="inicialización-de-pesos" class="level3" data-number="1.3.9">
<h3 data-number="1.3.9" class="anchored" data-anchor-id="inicialización-de-pesos"><span class="header-section-number">1.3.9</span> Inicialización de pesos</h3>
<p>Cuando empieza el entrenamiento de una red neuronal y tiene que realizar la primera pasada hacia delante de los datos, necesitamos que la red neuronal ya tenga asignados algún valor a los pesos.</p>
<p>Se pueden hacer inicializaciones del tipo:</p>
<ul>
<li><p><strong>Ceros</strong> Todos los pesos se inicializan a 0.</p></li>
<li><p><strong>Unos</strong> Todos los pesos se inicializan a 1.</p></li>
<li><p><strong>Distribución normal</strong>. Los pesos se inicializan con una distribución normal, normalmente con media 0 y una desviación alrededor de 0,05. Es decir, valores bastante cercanos al cero.</p></li>
<li><p><strong>Distribución normal truncada</strong>. Los pesos se inicializan con una distribución normal, normalmente con media 0 y una desviación alrededor de 0,05 y además se truncan con un máximo del doble de la desviación. Los valores aun són más cercanos a cero.</p></li>
<li><p><strong>Distribución uniforme</strong>. Los pesos se inicializan con una distribución uniforme, normalmente entre el 0 y el 1.</p></li>
<li><p><strong>Glorot Normal</strong> (También llamada Xavier normal) Los pesos se inicializan partiendo de una distribución normal truncada en la que la desivación es donde es el número de unidades de entrada y fanout es el número de unidades de salida. Ver (Glorot and Bengio 2010)</p></li>
<li><p><strong>Glorot Uniforme</strong> (También llamada Xavier uniforme) Los pesos se inicializan partiendo de una distribución uniforme donde los límites son <span class="math inline">\([-\)</span> limit,+ limit <span class="math inline">\(]\)</span> done limit <span class="math inline">\(=\sqrt{\frac{6}{\text { fanin }+ \text { fanout }}}\)</span> done <span class="math inline">\(fanin\)</span> y es el número de unidades de entrada y <span class="math inline">\(fanout\)</span> es el número de unidades de salida. Ver (Glorot and Bengio 2010)</p></li>
</ul>
</section>
<section id="batch-normalization" class="level3" data-number="1.3.10">
<h3 data-number="1.3.10" class="anchored" data-anchor-id="batch-normalization"><span class="header-section-number">1.3.10</span> Batch normalization</h3>
<p>Hemos comentado que cuando entrenamos una red neuronal los datos de entrada deben ser todos de tipo numérico y además los normalizamos para tener valores “cercanos a cero”, teniendo una media de 0 y varianza de 1, consiguiendo uniformizar todas las variables y conseguir que la red pueda converger más fácilmente.</p>
<p>Cuando los datos entran a la red neuronal y se comienza a operar con ellos, se convierten en nuevos valores que han perdido esa propiedad de normalización. Lo que hacemos con la normalización por lotes (batch normalization) (Ioffe and Szegedy 2015) es que añadimos un paso extra para normalizar las salidas de las funciones de activación. Lo normal es que se aplicara la normalización con la media y la varianza de todo el bloque de entrenamiento en ese paso, pero normalmente estaremos trabajando por lotes y se calculará la media y varianza con ese lote de datos.</p>
</section>
<section id="ejemplo-de-red-neuronal-con-keras" class="level3" data-number="1.3.11">
<h3 data-number="1.3.11" class="anchored" data-anchor-id="ejemplo-de-red-neuronal-con-keras"><span class="header-section-number">1.3.11</span> Ejemplo de Red Neuronal con Keras</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Importamos las librerías de keras/tensorflow</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Importamos la librería de los datasets de keras y cogemos el de boston_housing</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.datasets <span class="im">import</span> boston_housing</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtenemos los datos de entrenamiento y test</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># separados en las variables explicativas y la objetivo</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>(train_data, train_targets), (test_data, test_targets) <span class="op">=</span> boston_housing.load_data()</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>train_data.shape</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>test_data.shape</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Realizamos la "Normalización" restando la media y dividiendo por la desviación típica</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Ahora tendremos valores (-x,x) alredor de 0, pero en general pequeños</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> train_data.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">-=</span> mean</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> train_data.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">/=</span> std</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">-=</span> mean</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">/=</span> std</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Creamos el modelo</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Inicializamos el API Secuencial de capas</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential([</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Añadimos capa de entrada con las 13 variables explicativas</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        keras.Input(shape<span class="op">=</span>(<span class="dv">13</span>,)),</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Añadimos capa densamente conectada con 64 neuronas y activación relu</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Añadimos capa densamente conectada con 64 neuronas y activación relu</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Añadimos capa de salida densamente conectada con 1 neurona y activación lineal (para regresión)</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        layers.Dense(<span class="dv">1</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostramos el Modelo creado</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Compilamos el modelo definiendo el optimizador, función de pérdida y métrica</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="co"># RMSProp, mse, mae</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"rmsprop"</span>, loss<span class="op">=</span><span class="st">"mse"</span>, metrics<span class="op">=</span>[<span class="st">"mae"</span>])</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Realizamos el entrenamiento</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="co"># 130 épocos (iteraciones), con tamaño de batch de 16</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(train_data, train_targets,</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>          epochs<span class="op">=</span><span class="dv">130</span>, batch_size<span class="op">=</span><span class="dv">16</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Importamos la librería de pyplot para pintar gráficas</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="co"># list all data in history</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(history.history.keys())</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize history for accuracy</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'mae'</span>])</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.plot(history.history['val_mae'])</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'model mae'</span>)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'mae'</span>)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'epoch'</span>)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'train'</span>, <span class="st">'test'</span>], loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize history for loss</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'loss'</span>])</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.plot(history.history['val_loss'])</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'model loss'</span>)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'loss'</span>)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'epoch'</span>)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'train'</span>, <span class="st">'test'</span>], loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluamos el modelo con los datos de test</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model.predict(test_data)</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>predictions[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="redes-neuronales-convolucionales" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="redes-neuronales-convolucionales"><span class="header-section-number">1.4</span> Redes Neuronales Convolucionales</h2>
<section id="introducción-1" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="introducción-1"><span class="header-section-number">1.4.1</span> Introducción</h3>
<p>Esta arquitectura de redes de neuronas convolucionales, CNN, Convolutional Neural Networks es en la actualidad el campo de investigación más fecundo dentro de las redes neuronales artificiales de Deep learning y donde los investigadores, empresas e instituciones están dedicando más recursos e investigación. Para apoyar esta aseveración, en google trend se observa que el término convolutional neural network en relación con el concepto de artificial neural network crece y está por encima desde el año 2016. Es en este último lustro donde el Deep learning ha tomado una importancia considerable.</p>
<div id="fig-busqueda-google" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-busqueda-google-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/busqueda_google.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-busqueda-google-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.2: búsqueda de términos de redes neuronales en google trend
</figcaption>
</figure>
</div>
<p>Las <strong>redes convolucionales</strong> son actualmente utilizadas para diferentes propósitos: tratamiento de imágenes(visión por computador, extracción de características, segmentación, etc.), generación y clasificación de texto(o audio), predicción de series temporales, etc. En este capítulo veremos su aplicación en clasificación de imágenes y de texto.</p>
</section>
<section id="clasificación-de-imágenes" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="clasificación-de-imágenes"><span class="header-section-number">1.4.2</span> Clasificación de imágenes</h3>
<p>En este modelo de redes convolucionales las neuronas se corresponden a campos receptivos similares a las neuronas en la corteza visual de un cerebro humano. Este tipo de redes se han mostrado muy efectivas para tareas de detección y categorización de objetos y en la clasificación y segmentación de imágenes. Por ejemplo, estas redes en la década de 1990 las aplicó AT &amp; T para desarrollar un modelo para la lectura de cheques. También más tarde se desarrollaron muchos sistemas OCR basados en CNN. En esta arquitectura cada neurona de una capa no recibe conexiones entrantes de todas las neuronas de la capa anterior, sino sólo de algunas. Esta estrategia favorece que una neurona se especialice en una región del conjunto de números (píxeles) de la capa anterior, lo que disminuye notablemente el número de pesos y de operaciones a realizar. Lo más normal es que neuronas consecutivas de una capa intermedia se especialicen en regiones solapadas de la capa anterior.</p>
<p>Una forma intuitiva para entender cómo trabajan estas redes neuronales es ver cómo nos representamos y vemos las imágenes. Para reconocer una cara primero tenemos que tener una imagen interna de lo que es una cara. Y a una imagen de una cara la reconocemos porque tiene nariz, boca, orejas, ojos, etc. Pero en muchas ocasiones una oreja está tapada por el pelo, es decir, los elementos de una cara se pueden ocultar de alguna manera. Antes de clasificarla, tenemos que saber la proporción y disposición y también cómo se relacionan la partes entre sí.</p>
<p>Para saber si las partes de la cara se encuentran en una imagen tenemos que identificar previamente líneas bordes, formas, texturas, relación de tamaño, etcétera. En una red convolucional, cada capa lo que va a ir aprendiendo son los diferentes niveles de abstracción de la imagen inicial. Para comprender mejor el concepto anterior hemos seleccionado esta imagen de Raschka y Mirjalili (2019) donde se observa como partes del perro se transforman en neuronas del mapa de características</p>
<div id="fig-correspondencia-features" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-correspondencia-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/correspondencia_featrures.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-correspondencia-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.3: Correspondencia de zonas de la imagen y mapa de características
</figcaption>
</figure>
</div>
<p>El objetivo de las redes CNN es aprender características de orden superior utilizando la operación de convolución.</p>
<p>Puesto que las redes neuronales convolucionales pueden aprender relaciones de entrada-salida (donde la entrada es una imagen en este caso), en la convolución, cada pixel de salida es una combinación lineal de los pixeles de entrada.</p>
<p>La <strong>convolución</strong> consiste en <strong>filtrar</strong> una imagen utilizando una <strong>máscara</strong>. Diferentes máscaras producen distintos resultados. Las máscaras representan las conexiones entre neuronas de capas anteriores. Estas capas aprenden progresivamente las características de orden superior de la entrada sin procesar.</p>
<p>Las redes neuronales convolucionales se forman usando dos tipos de capas: convolucionales y pooling. La capa de convolución transforma los datos de entrada a través de una operación matemática llamada convolución. Esta operación describe cómo fusionar dos conjuntos de información diferentes. A esta operación se le suele aplicar una función de transformación, generalmente la RELU. Después de la capa o capas de convolución se usa una capa de pooling, cuya función es resumir las respuestas de las salidas cercanas. Antes de obtener el output unimos la última capa de pooling con una red densamente conectada. Previamente se ha aplanado (Flatering) la última capa de pooling para obtener un vector de entrada a la red neural final que nos ofrecerá los resultados.</p>
<div id="fig-arquitectura_convolucional" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-arquitectura_convolucional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/arquitectura_convolucional.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-arquitectura_convolucional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.4: Arquitectura de una CNN
</figcaption>
</figure>
</div>
<p>Las redes neuronales convolucionales debido a su forma de concebirse son aptas para poder aprender a clasificar todo tipo de datos donde éstos estén distribuidos de una forma continua a lo largo del mapa de entrada, y a su vez sean estadísticamente similares en cualquier lugar del mapa de entrada. Por esta razón, son especialmente eficaces para clasificar imágenes. También pueden ser aplicadas para la clasificación de series de tiempo o señales de audio.</p>
<p>En relación con el color y la forma de codificarse, en las redes convolucionales se realiza en tensores 3D, dos ejes para el ancho (width) y el alto (height) y el otro eje llamado de profundidad (depht) que es el canal del color con valor tres si trabajamos con imágenes de color RGB (Red, Green y Blue) rojo, verde y azul. Si disponemos de imágenes en escala de grises el valor de depht es uno. La base de datos MNIST (National Institute of Standards and Technology database) con la que trabajaremos en este epígrafe contiene imágenes de 28 x 28 pixeles, los valores de height y de widht son ambos 28, y al ser una base de datos en blanco y negro el valor de depht es 1.</p>
<p>Las imágenes son matrices de píxeles que van de cero a 255 y que para la red neuronal se normalizan para que sus valores oscilen entre cero y uno.</p>
<section id="convolución" class="level4" data-number="1.4.2.1">
<h4 data-number="1.4.2.1" class="anchored" data-anchor-id="convolución"><span class="header-section-number">1.4.2.1</span> Convolución</h4>
<p>En las redes convolucionales todas las neuronas de la capa de entrada (los píxeles de las imágenes) no se conectan con todas las neuronas de la capa oculta del primer nivel como lo hacen las redes clásicas del tipo perceptrón multicapa o las redes que conocemos de forma genérica como redes densamente conectadas. Las conexiones se realizan por pequeñas zonas de la capa de entrada.</p>
<div id="fig-conexion-neuronas-convolucional" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-conexion-neuronas-convolucional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/conexion_neuronas_convolucional.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-conexion-neuronas-convolucional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.5: Conexión de las neuronas de la capa de entrada con la capa oculta
</figcaption>
</figure>
</div>
<p>Veamos un ejemplo para la base de datos de los dígitos del 1 a 9. Vamos a conectar cada neurona de la capa oculta con una región de 5 x 5 neurona, es decir, con 25 neuronas de la capa de entrada, que podemos denominarla ventana. Esta ventana va a ir recorriendo todo el espacio de entrada de 28 x 28 empezando por arriba y desplazándose de izquierda a derecha y de arriba abajo. Suponemos que los desplazamientos de la ventana son de un paso (un pixel) aunque este es un parámetro de la red que podemos modificar (en la programación lo llamaremos stride).</p>
<p>Para conectar la capa de entrada con la de salida utilizaremos una matriz de pesos (W) de tamaño 3 x 3 que recibe el nombre de filtro (filter) y el valor del sesgo. Para obtener el valor de cada neurona de la capa oculta realizaremos el producto escalar entre el filtro y la ventana de la capa de entrada. Utilizamos el mismo filtro para obtener todas las neuronas de la capa oculta, es decir en todos los productos escalares siempre utilizamos la misma matriz, el mismo filtro.</p>
<p>Se definen matemáticamente estos productos escalares a través de la siguiente expresión:</p>
<p><span class="math display">\[
Y=X * W \rightarrow Y[i, j]=\sum_{k_1=-\infty}^{+\infty} \sum_{k_2=-\infty}^{+\infty} X\left[i-k_1, j-k_2\right] W\left[k_1, k_2\right]
\]</span></p>
<div id="fig-producto_convolucion" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-producto_convolucion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/producto_convolucion.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-producto_convolucion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.6: Convolución
</figcaption>
</figure>
</div>
<p>Como en este tipo de red un filtro sólo nos permite revelar una característica muy concreta de la imagen, lo que se propone es usar varios filtros simultáneamente, uno para cada característica que queramos detectar. Una forma visual de representarlo (si suponemos que queremos aplicar 32 filtros) es como se muestra a continuación:</p>
<div id="fig-primera-capa-convolucional" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-primera-capa-convolucional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/primera_capa_convolucional.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-primera-capa-convolucional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.7: Primera capa de la red convolucional con 32 filtros
</figcaption>
</figure>
</div>
<p>Al resultado de la aplicación de los diferentes filtros se les suele aplicar la función de activación denominada RELU y que ya se comentó en la introducción.</p>
<p>Una interesante fuente de información es la documentación del software gratuito GIMP donde expone diferentes efectos que se producen en las imágenes al aplicar diversas convoluciones.</p>
<p>Un ejmplo claro y didáctico lo podemos obtener de la docuemntación del software libre de dibujo y tratamiento de imágenes denominado GIMP (https://docs.gimp.org/2.6/es/plug-in-convmatrix.html). Algunos de estos efectos nos ayudan a entender la operación de los filtros en las redes convolucionales y cómo afectan a las imágenes, en concreto, el ejemplo que presenta lo realiza sobre la figura del Taj Mahal.</p>
<p>El filtro enfocar lo que consigue es afinar los rasgos, los contornos lo que nos permite agudizar los objetos de la imagen. Toma el valor central de la matriz de cinco por cinco lo multiplica por cinco y le resta el valor de los cuatro vecinos. Al final hace una media, lo que mejora la resolución del pixel central porque elimina el ruido o perturbaciones que tiene de sus pixeles vecinos.</p>
<p><strong>El filtro enfocar (Sharpen)</strong></p>
<div id="fig-filtro_enfocar" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-filtro_enfocar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/filtro_enfocar.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-filtro_enfocar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.8: Filtro Enfocar
</figcaption>
</figure>
</div>
<p>Lo contario al filtro enfocar lo obtenemos a través de la matriz siguiente, difuminando la imagen al ser estos píxeles mezclados o combinados con los pixeles cercanos. Promedia todos los píxeles vecinos a un pixel dado lo que implica que se obtienen bordes borrosos.</p>
<p><strong>Filtro desenfocar</strong></p>
<div id="fig-filtro_desenfocar" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-filtro_desenfocar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/filtro_desenfocar.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-filtro_desenfocar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.9: Filtro DesEnfocar
</figcaption>
</figure>
</div>
<p><strong>Filtro Detectar bordes (Edge Detect)</strong></p>
<p>Este efecto se consigue mejorando los límites o las aristas de la imagen. En cada píxel se elimina su vecino inmediatamente anterior en horizontal y en vertical. Se eliminan las similitudes vecinas y quedan los bordes resaltados. Al pixel central se le suman los cuatro píxeles vecinos y lo que queda al final es una medida de cómo de diferente es un píxel frente a sus vecinos. En el ejemplo, al hacer esto da un valor de cero de ahí que se observen tantas zonas oscuras.</p>
<div id="fig-filtro_bordes" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-filtro_bordes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/filtro_bordes.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-filtro_bordes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.10: Filtro Detectar Bordes
</figcaption>
</figure>
</div>
<p><strong>Filtro Repujado (Emboss)</strong></p>
<p>En este filto se observa que la matriz es simétrica y lo que intenta a través del diseño del filtro es mejorar los píxeles centrales y de derecha abajo restándole los anteriores. Se obtiene lo que en fotografía se conoce como un claro oscuro. Trata de mejorar las partes que tienen mayor relevancia.</p>
<div id="fig-filtro_emboss" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-filtro_emboss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/filtro_emboss.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-filtro_emboss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.11: Filtro Emboss
</figcaption>
</figure>
</div>
</section>
<section id="pooling" class="level4" data-number="1.4.2.2">
<h4 data-number="1.4.2.2" class="anchored" data-anchor-id="pooling"><span class="header-section-number">1.4.2.2</span> Pooling</h4>
<p>Con la operación de <strong>pooling</strong> se trata de condensar la información de la capa convolucional. A este procedimiento también se le conoce como <strong>submuestreo</strong>.</p>
<p>Es simplemente una operación en la que reducimos los parámetros de la red. Se aplica normalmente a través de dos operaciones: <strong>max-pooling</strong> y <strong>mean-pooling</strong>, que también es conocido como average-pooling. Tal y como se observa en la imagen siguiente, desde la capa de convolución se genera una nueva capa aplicando la operación a todas las agrupaciones, donde previamente hemos elegido el tamaño de la región; en la figura siguiente es de tamaño 2, con lo que pasamos de un espacio de 24 x 24 neuronas a la mitad, 12 x 12 en la capa de pooling.</p>
<div id="fig-pooling" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/pooling.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.12: Etapa de pooling de tamaño 2 x 2
</figcaption>
</figure>
</div>
<p>Vamos a estudiar el pooling suponiendo que tenemos una imagen de 5 x 5 píxeles y que queremos efectuar una agrupación max-pooling. Es la más utilizada, ya que obtiene buenos resultados. Observamos los valores de la matriz y se escoge el valor máximo de los cuatro bloques de matrices de dos por dos. Max Pooling</p>
<p>En la agrupación Average Pooling la operación que se realiza es sustituir los valores de cada grupo de entrada por su valor medio. Esta transformación es menos utilizada que el max-pooling.</p>
<p>La transformación max-pooling presenta un tipo de invarianza local: pequeños cambios en una región local no varían el resultado final realizado con el max – pooling: se mantiene la relación espacial. Para ilustrar este concepto hemos escogido la imagen que presenta Torres (2020) donde se ilustra como partiendo de una matriz de 12 x 12 que representa al número 7, al aplicar la operación de max-pooling con una ventana de 2 x 2 se conserva la relación espacial.</p>
<div id="fig-max-pooling" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-max-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/max_pooling.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-max-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.13: Max Pooling
</figcaption>
</figure>
</div>
<div id="fig-average-pooling" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-average-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/average_pooling.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-average-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.14: Average Pooling
</figcaption>
</figure>
</div>
<div id="fig-transformacion-pooling" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformacion-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/transformacion_pooling.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformacion-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.15: Mantenimiento del pooling con la transformación
</figcaption>
</figure>
</div>
</section>
<section id="padding" class="level4" data-number="1.4.2.3">
<h4 data-number="1.4.2.3" class="anchored" data-anchor-id="padding"><span class="header-section-number">1.4.2.3</span> Padding</h4>
<p>Para explicar el concepto del <strong>Padding</strong> vamos a suponer que tenemos una imagen de 5 x 5 píxeles, es decir 25 neuronas en la capa de entrada, y que elegimos, para realizar la convolución, una ventana de 3 x 3. El número de neuronas de la capa oculta resultará ser de nueve. Enumeramos los píxeles de la imagen de forma natural del 1 al 25 para que resulte más sencillo de entender.</p>
<div id="fig-sin-padding" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sin-padding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/sin_padding.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sin-padding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.16: Operación de convolución con una ventana de 3 x 3
</figcaption>
</figure>
</div>
<p>Pero si queremos obtener un tensor de salida que tenga las mismas dimensiones que la entrada podemos rellenar la matriz de ceros antes de deslizar la ventana por ella. Vemos la figura siguiente donde ya se ha rellenado de valores cero y obtenemos, después de deslizar la ventana de 3 x3 de izquierda a derecha y de arriba abajo, las veinticinco matrices de la figura nº 71</p>
<div id="fig-relleno-ceros" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-relleno-ceros-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/relleno_ceros.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-relleno-ceros-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.17: Imagen con relleno de ceros
</figcaption>
</figure>
</div>
<div id="fig-con-padding" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-con-padding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/con_padding.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-con-padding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.18: Operación de convolución con ventana 3 x 3 y padding
</figcaption>
</figure>
</div>
</section>
<section id="stride" class="level4" data-number="1.4.2.4">
<h4 data-number="1.4.2.4" class="anchored" data-anchor-id="stride"><span class="header-section-number">1.4.2.4</span> Stride</h4>
<p>Hasta ahora, la forma de recorrer la matriz a través de la ventana se realiza desplazándola de <strong>un solo paso</strong>, pero podemos cambiar este hiperparámetro conocido como <strong>stride</strong>. Al aumentar el paso se decrementa la información que pasará a la capa posterior. A continuación, se muestra el resultado de las cuatro matrices que obtenemos con un stride de valor 3.</p>
<div id="fig-con-stride2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-con-stride2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/con_stride2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-con-stride2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.19: Operación de convolución con una ventana de 3 x 3 y stride 2
</figcaption>
</figure>
</div>
<p>Finalmente, para resumir, una <strong>red convolucional</strong> contiene los siguientes elementos:</p>
<ul>
<li><strong>Entrada</strong>: Son el número de pixeles de la imagen. Serán alto, ancho y profundidad. Tenemos un solo color (escala de grises) o tres: rojo, verde y azul.</li>
<li><strong>Capa de convolución</strong>: procesará la salida de neuronas que están conectadas en «regiones locales» de entrada (es decir pixeles cercanos), calculando el producto escalar entre sus pesos (valor de pixel) y una pequeña región a la que están conectados. En este epígrafe se presentan las imágenes con 32 filtros, pero puede realizarse con la cantidad que deseemos.</li>
<li><strong>Capa RELU</strong> Se aplicará la función de activación en los elementos de la matriz.</li>
<li><strong>Pooling</strong> (agrupar) o Submuestreo: Se procede normalmente a una reducción en las dimensiones alto y ancho, pero se mantiene la profundidad.</li>
<li><strong>Capa tradicional</strong>. Se finalizará con la red de neuronas feedforward (Perceptrón multicapa que se denomina normalmente como red densamente conectada) que vinculará con la última capa de subsampling y finalizará con la cantidad de neuronas que queremos clasificar. En el gráfico siguiente se muestran todas las fases de una red neuronal convolucional.</li>
</ul>
<div id="fig-convolucion-completa" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-convolucion-completa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/convolucion_completa.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-convolucion-completa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.20: Operación de convolución completa
</figcaption>
</figure>
</div>
</section>
<section id="redes-convolucionales-con-nombre-propio" class="level4" data-number="1.4.2.5">
<h4 data-number="1.4.2.5" class="anchored" data-anchor-id="redes-convolucionales-con-nombre-propio"><span class="header-section-number">1.4.2.5</span> Redes convolucionales con nombre propio</h4>
<p>Existen en la actualidad muchas arquitecturas de redes neuronales convolucionales que ya están preparadas, probadas, disponibles e incorporadas en el software de muchos programas como Keras y Tensorflow.</p>
<p>Vamos a comentar algunos de estos modelos, bien por ser los primeros, o por sus excelentes resultados en concursos como el ILSVRC (Large Scale Visual Recognition Challenge).</p>
<p>Estas estructuras merecen atención dado que son excelentes para estudiarlas e incorporarlas por su notable éxito. El ILSVRC fue un concurso celebrado de 2011 a 2016 de donde nacieron las principales aportaciones efectuadas en las redes convolucionales. Este concurso fue diseñado para estimular la innovación en el campo de la visión computacional. Actualmente se desarrollan este tipo de concursos a través de la plataforma web: https://www.kaggle.com/ Para ver más prototipos de redes convolucionales y los últimos avances y consejos sobre las redes convolucionales se puede consultar el siguiente artículo “Recent Advances in Convolutional Neural Networks” de Jiuxiang. G. et al.&nbsp;(2019)</p>
<p>Los cinco modelos más destacados hasta el año 2017 son los siguientes: LeNet-5, Alexnet, GoogLeNet, VGG y Restnet.</p>
<ul>
<li><p><strong>LeNet-5</strong>. Este modelo de Yann LeCun de los años 90 consiguió excelentes resultados en la lectura de códigos postales consta de imágenes de entrada de 32 x 32 píxeles seguida de dos etapas de convolución – pooling, una capa densamente conectada y una capa softmax final que nos permite conocer los números o las imágenes.</p></li>
<li><p><strong>AlexNet</strong>. Fue la arquitectura estrella a partir del año 2010 en el ILSVRC y popularizada en el documento de 2012 de Alex Krizhevsky, et al.&nbsp;titulado”Clasificación de ImageNet con redes neuronales convolucionales profundas”. Podemos resumir los aspectos clave de la arquitectura relevantes en los modelos modernos de la siguiente manera: • Empleo de la función de activación ReLU después de capas convolucionales y softmax para la capa de salida. • Uso de la agrupación máxima en lugar de la agrupación media. • Utilización de la regularización de Dropout entre las capas totalmente conectadas. • Patrón de capa convolucional alimentada directamente a otra capa convolucional. • Uso del aumento de datos (Data Aumentation,)</p></li>
<li><p><strong>VGG</strong>. Este prototipo fue desarrollado por un grupo de investigación de Geometría Visual en Oxford. Obtuvo el segundo puesto en la competición del año 2014 del ILSVRC. Las aportaciones principales de la investigación se pueden encontrar en el documento titulado “&nbsp;Redes convolucionales muy profundas para el reconocimiento de imágenes a gran escala&nbsp;” desarrollado por Karen Simonyan y Andrew Zisserman. Este modelo contribuyó a demostrar que la profundidad de la red es una componente crítica para alcanzar unos buenos resultados. Otra diferencia importante con los modelos anteriores y que actualmente es muy utilizada es el uso de un gran número de filtros y de tamaño reducido. Estas redes emplean ejemplos de dos, tres e incluso cuatro capas convolucionales apiladas antes de usar una capa de agrupación máxima. En esta arquitectura el número de filtros aumenta con la profundidad del modelo. El modelo comienza con 64 y aumenta a través de los filtros de 128, 256 y 512 al final de la parte de extracción de características del modelo. Los investigadores evaluaron varias variantes de la arquitectura si bien en los programas sólo se hace referencia a dos de ellas que son las que aportan un mayor rendimiento y que son nombradas por las capas que tienen: VGG-16 y VGG-19.</p></li>
<li><p><strong>GoogLeNet</strong>. GoogLeNet fue desarrolado por investigadores de Google Research. de Google, que con su módulo denominado de inception reduce drásticamente los parámetros de la red (10 veces menos que AlexNet) y de ella han derivado varias versiones como la Inception-v4. Esta arquitectura ganó la competición en el año 2014 y su éxito se debió a que la red era mucho más profunda (muchas más capas) y como ya se ha indicado introdujeron en el modelo las subredes llamadas inception. Las aportaciones principales en el uso de capas convolucionales fueron propuestos en el documento de 2015 por Christian Szegedy, et al.&nbsp;titulado “&nbsp;Profundizando con las convoluciones&nbsp;”. Estos autores introducen una arquitectura llamada “inicio” y un modelo específico denominado GoogLenet. El módulo inicio es un bloque de capas convolucionales paralelas con filtros de diferentes tamaños y una capa de agrupación máxima de 3 × 3, cuyos resultados se concatenan. Otra decisión de diseño fundamental en el modelo inicial fue la conexión de la salida en diferentes puntos del modelo que lograron realizar con la creación de pequeñas redes de salida desde la red principal y que fueron entrenadas para hacer una predicción. La intención era proporcionar una señal de error adicional de la tarea de clasificación en diferentes puntos del modelo profundo para abordar el problema de los gradientes de fuga.</p></li>
<li><p><strong>Red Residual o ResNet</strong>. Esta arquitectura gano la competición de 2015 y fue creada por el grupo de investigación de Microsoft. Se puede ampliar la información en He, et al.&nbsp;en su documento de 2016 titulado “&nbsp;Aprendizaje profundo residual para el reconocimiento de la imagen&nbsp;”. Esta red es extremadamente profunda con 152 capas, confirmando al pasar los años que las redes son cada vez más profundas, más capas, pero con menos parámetros que estimar. La cuestión clave del diseño de esta red es la incorporación de la idea de bloques residuales que hacen uso de conexiones directa. Un bloque residual, según los autores, “es un patrón de dos capas convolucionales con activación ReLU donde la salida del bloque se combina con la entrada al bloque, por ejemplo, la conexión de acceso directo” Otra clave, en este caso para el entrenamiento de la red tan profunda es lo que llamaron skip connections que implica que la señal con la que se alimenta una capa también se agregue a una capa que se encuentre más adelante. Resumiendo, las tres principales aportaciones de este modelo son:</p>
<ul>
<li>Empleo de conexiones de acceso directo.</li>
<li>Desarrollo y repetición de los bloques residuales.</li>
<li>Modelos muy profundos (152 capas) Aunque se encuentran otros modelos que también son muy populares con 34, 50 y 101 capas.</li>
</ul></li>
</ul>
<p>Una buena parte de los modelos comentados se incluyen en la librería de Keras y se pueden encontrar en la siguiente dirección de internet: https://keras.io/api/applications/ Según los autores del programa Keras: “Las aplicaciones Keras son modelos de aprendizaje profundo que están disponibles junto con pesos preentrenados. Estos modelos se pueden usar para predicción, extracción de características y ajustes. Los pesos se descargan automáticamente cuando se crea una instancia de un modelo. Se almacenan en ~ / .keras / models /. Tras la creación de instancias, los modelos se construirán de acuerdo con el formato de datos de imagen establecido en su archivo de configuración de Keras en ~ / .keras / keras.json. Por ejemplo, si ha configurado image_data_format = channel_last, cualquier modelo cargado desde este repositorio se construirá de acuerdo con la convención de formato de datos TensorFlow,”Altura-Ancho-Profundidad”.</p>
<div id="fig-modelos-entrenados" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-modelos-entrenados-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/modelos_entrenados.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-modelos-entrenados-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.21: Modelos preentrenados en Keras
</figcaption>
</figure>
</div>
</section>
<section id="ejemplos-de-redes-convolucionales-con-keras" class="level4" data-number="1.4.2.6">
<h4 data-number="1.4.2.6" class="anchored" data-anchor-id="ejemplos-de-redes-convolucionales-con-keras"><span class="header-section-number">1.4.2.6</span> Ejemplos de Redes Convolucionales con keras</h4>
<p><strong>Red Convolucional con imágenes importadas a memoria</strong></p>
<p>En este ejemplo vamos a usar imágenes que vienen preparadas dentro de un array de datos que se carga directamente en memoria.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Importamos las librerías de keras/tensorflow</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Importamos la librería de los datasets de keras y cogemos el de mnist</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.datasets <span class="im">import</span> mnist</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtenemos los datos de entrenamiento y test</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># separados en las imagenes y las etiquetas de las mismas</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>(train_images, train_labels), (test_images, test_labels) <span class="op">=</span> mnist.load_data()</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Reestructuramos los datos de las imágenes para que se traten como imagen</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>train_images <span class="op">=</span> train_images.reshape((<span class="dv">60000</span>, <span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividimos entre 255 para "normalizar" el dato y dejarlo entre 0 y 1</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>train_images <span class="op">=</span> train_images.astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Reestructuramos los datos de las imágenes para que se traten como imagen</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>test_images <span class="op">=</span> test_images.reshape((<span class="dv">10000</span>, <span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividimos entre 255 para "normalizar" el dato y dejarlo entre 0 y 1</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>test_images <span class="op">=</span> test_images.astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Creamos el modelo</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Capa de entrada formato 28x28 pixels y sólo un canal de color (escala de grises</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadimos capa de convolución con 32 filtros de tamaño 3 y activación relu</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span><span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(inputs)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadimos capa de pooling, tipo max y de tamaño 2</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.MaxPooling2D(pool_size<span class="op">=</span><span class="dv">2</span>)(x)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadimos capa de convolución con 64 filtros de tamaño 3 y activación relu</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span><span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadimos capa de pooling, tipo max y de tamaño 2</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.MaxPooling2D(pool_size<span class="op">=</span><span class="dv">2</span>)(x)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadimos capa de convolución con 128 filtros de tamaño 3 y activación relu</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span><span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Aplanamos los datos</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Flatten()(x)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Ponemos una capa densamente conectada</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="co"># La salida la hacemos de tipo softmax con 10 neuronas (números de clases diferentes)</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)(x)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Construimos el modelo de la Red Neuronal Convolucional</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostramos el Modelo creado</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Compilamos el modelo definiendo el optimizador, función de pérdida y métrica</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="co"># RMSProp, sparse_categorical_crossentropy, accuracy</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"rmsprop"</span>,</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Realizamos el entrenamiento</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 épocos (iteraciones), con tamaño de batch de 64</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(train_images, train_labels, epochs<span class="op">=</span><span class="dv">5</span>, batch_size<span class="op">=</span><span class="dv">64</span>)</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize history for accuracy</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'accuracy'</span>])</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'model accuracy'</span>)</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'accuracy'</span>)</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'epoch'</span>)</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'train'</span>], loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize history for loss</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'loss'</span>])</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'model loss'</span>)</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'loss'</span>)</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'epoch'</span>)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'train'</span>], loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluamos el modelo con los datos de test</span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> model.evaluate(test_images, test_labels)</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span>test_acc<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Red Convolucional con imágenes importadas desde un directorio</strong></p>
<p>Ahora vamos a ver un ejemplo en el que descargamos las imágenes y las desempaquetamos en un directorio.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importamos las librerías de keras/tensorflow</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> PIL</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> PIL.Image</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow.keras.datasets <span class="im">as</span> tfds</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pathlib</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>dataset_url <span class="op">=</span> <span class="st">"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>data_dir <span class="op">=</span> tf.keras.utils.get_file(origin<span class="op">=</span>dataset_url,</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>                                   fname<span class="op">=</span><span class="st">'flower_photos'</span>,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>                                   untar<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>data_dir <span class="op">=</span> pathlib.Path(data_dir)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>image_count <span class="op">=</span> <span class="bu">len</span>(<span class="bu">list</span>(data_dir.glob(<span class="st">'*/*.jpg'</span>)))</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(image_count)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>img_height <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>img_width <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> tf.keras.utils.image_dataset_from_directory(</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>  data_dir,</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>  validation_split<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>  subset<span class="op">=</span><span class="st">"training"</span>,</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>  seed<span class="op">=</span><span class="dv">123</span>,</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>  image_size<span class="op">=</span>(img_height, img_width),</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>  batch_size<span class="op">=</span>batch_size)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>val_ds <span class="op">=</span> tf.keras.utils.image_dataset_from_directory(</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>  data_dir,</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>  validation_split<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>  subset<span class="op">=</span><span class="st">"validation"</span>,</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>  seed<span class="op">=</span><span class="dv">123</span>,</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>  image_size<span class="op">=</span>(img_height, img_width),</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>  batch_size<span class="op">=</span>batch_size)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> train_ds.class_names</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_names)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> images, labels <span class="kw">in</span> train_ds.take(<span class="dv">1</span>):</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">9</span>):</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">3</span>, <span class="dv">3</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>    plt.imshow(images[i].numpy().astype(<span class="st">"uint8"</span>))</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    plt.title(class_names[labels[i]])</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Creamos el modelo</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Capa de entrada formato 180x180 pixels y 3 canales de color RGB</span></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(img_width, img_height, <span class="dv">3</span>))</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividimos entre 255 para "normalizar" el dato y dejarlo entre 0 7 1</span></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Rescaling(<span class="fl">1.</span><span class="op">/</span><span class="dv">255</span>)(inputs),</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadimos capa de convolución con 32 filtros de tamaño 3 y activación relu</span></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span><span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(inputs)</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadimos capa de pooling, tipo max y de tamaño 2</span></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.MaxPooling2D(pool_size<span class="op">=</span><span class="dv">2</span>)(x)</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadimos capa de convolución con 64 filtros de tamaño 3 y activación relu</span></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span><span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadimos capa de pooling, tipo max y de tamaño 2</span></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.MaxPooling2D(pool_size<span class="op">=</span><span class="dv">2</span>)(x)</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadimos capa de convolución con 128 filtros de tamaño 3 y activación relu</span></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span><span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Aplanamos los datos</span></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Flatten()(x)</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a><span class="co"># Ponemos una capa densamente conectada</span></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a><span class="co"># La salida la hacemos de tipo softmax con 5 neuronas (números de clases diferentes)</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> layers.Dense(num_classes, activation<span class="op">=</span><span class="st">"softmax"</span>)(x)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a><span class="co"># Construimos el modelo de la Red Neuronal Convolucional</span></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"rmsprop"</span>,</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>history<span class="op">=</span>model.fit(</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>  train_ds,</span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>  validation_data<span class="op">=</span>val_ds,</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>  epochs<span class="op">=</span><span class="dv">3</span></span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize history for accuracy</span></span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'accuracy'</span>])</span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'val_accuracy'</span>])</span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'model accuracy'</span>)</span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'accuracy'</span>)</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'epoch'</span>)</span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'train'</span>, <span class="st">'test'</span>], loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize history for loss</span></span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'loss'</span>])</span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'val_loss'</span>])</span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'model loss'</span>)</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'loss'</span>)</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'epoch'</span>)</span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'train'</span>, <span class="st">'test'</span>], loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="clasificación-de-textos" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="clasificación-de-textos"><span class="header-section-number">1.4.3</span> Clasificación de textos</h3>
<p>Las redes convolucionales son actualmente utilizadas para diferentes propósitos: tratamiento de imágenes (visión por computador, extracción de características, segmentación, etc.), generación y clasificación de texto (o audio), predicción de series temporales, etc. En este caso, veremos en detalle un ejemplo de clasificación de texto.</p>
<p>Se presenta a continuación una aplicación práctica de clasificación de texto multiclase a partir de redes Convolucionales de una dimensión. Para ello, se utiliza una bbdd referida a las reclamaciones de los usuarios ante una entidad bancaria en función del tipo de producto.</p>
<p>En primer lugar, se importan las librerías a utililizar y se le el fichero:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>El fichero de trabajo contiene una serie de reclamaciones que no vienen acompañadas con su texto asociado. Se considera que lo más adecuado es excluir tales instancias del dataset de partida.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>datos <span class="op">=</span> pd.read_csv(<span class="st">'C:/DEEP LEARNING/consumer_complaints.csv'</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>datos <span class="op">=</span> datos[[<span class="st">'product'</span>, <span class="st">'consumer_complaint_narrative'</span>]] <span class="co"># variables de</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>interés</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>datos <span class="op">=</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>datos.dropna(subset<span class="op">=</span>[<span class="st">'product'</span>,<span class="st">'consumer_complaint_narrative'</span>]).reset_index</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>(drop<span class="op">=</span><span class="va">True</span>) <span class="co"># registros con texto no informado son eliminados de la muestra</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Tamaño de los datos:'</span>, datos.shape)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>Tamaño de los datos: (<span class="dv">66806</span>, <span class="dv">2</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>sns.countplot(y<span class="op">=</span><span class="st">'product'</span>, data<span class="op">=</span>datos, order <span class="op">=</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>datos[<span class="st">'product'</span>].value_counts().index)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Reclamaciones'</span>), plt.ylabel(<span class="st">'Producto'</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Como puede verse, se parte de once tipos de productos diferentes; si bien, para varios de ellos el número de reclamaciones no es considerado significativo por el área legal de la entidad. Por ello, y en base a la similitud de los productos, se agrupan las cuatro categorías con un menor número reclamaciones en</p>
<ul>
<li>Prepaid card: se incluye en la categoría de “Credit card”</li>
<li>Payday loan: se incluye en la categoría “Bank account or service”</li>
<li>Money transfers y Other financial service: forman un grupo conjunto denominado “Money transfers and Other financial service”</li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># agrupaciones</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>datos[<span class="st">'product'</span>] <span class="op">=</span> np.where(datos[<span class="st">'product'</span>]<span class="op">==</span><span class="st">'Payday loan'</span>, <span class="st">'Bank account</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="er">or service', datos['product']) # préstamos</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>datos[<span class="st">'product'</span>] <span class="op">=</span> np.where(datos[<span class="st">'product'</span>]<span class="op">==</span><span class="st">'Prepaid card'</span>, <span class="st">'Credit</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="er">card', datos['product']) # créditos</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>tipo_producto <span class="op">=</span> [<span class="st">'Money transfers'</span>,<span class="st">'Other financial service'</span>] <span class="co"># otros</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>servicios financieros</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>datos[<span class="st">'product'</span>] <span class="op">=</span> np.where(datos[<span class="st">'product'</span>].isin(tipo_producto), <span class="st">'Money</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="er">transfers and Other', datos['product'])</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>sns.countplot(y<span class="op">=</span><span class="st">'product'</span>, data<span class="op">=</span>datos, order <span class="op">=</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>datos[<span class="st">'product'</span>].value_counts().index)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Reclamaciones'</span>), plt.ylabel(<span class="st">'Producto'</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>De esta forma, el número de grupos ha sido distribuido de una forma más equitativa. A modo de ejemplo, se muestra una de las reclamaciones:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_reclamaciones(df, elemento):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.loc[elemento].to_list()</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Producto:'</span>, plot_reclamaciones(datos, <span class="dv">100</span>)[<span class="dv">0</span>])</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Reclamacion:'</span>, plot_reclamaciones(datos, <span class="dv">100</span>)[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Como puede verse, la reclamación 101 del dataset está asociada a un préstamo al consumo. Sin embargo, lo verdaderamente interesante del texto de ejemplo es la necesidad de realizar un preprocesado a los textos puesto que algunos símbolos, caracteres o, incluso palabras, no son relevantes para que la red sea capaz de interpretar el contenido del mismo. Por tanto, se lleva a cabo lo siguiente:</p>
<ul>
<li>Conversión del texto a minúsculas</li>
<li>Exclusión del texto el contenido cifrado (XXXX)</li>
<li>Eliminación de caracteres extraños</li>
<li>Para poder hacer este preprocesado de textos se hace uso del paquete re de Python.</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocesado(reclamacion):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    reclamacion <span class="op">=</span> reclamacion.lower() <span class="co"># texto en minúsculas</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    reclamacion <span class="op">=</span> reclamacion.replace(<span class="st">'x'</span>,<span class="st">''</span>) <span class="co"># cambio X por espacio</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    reclamacion <span class="op">=</span> re.<span class="bu">compile</span>(<span class="st">'[/()</span><span class="sc">{}</span><span class="st">\[\]\|@,;]'</span>).sub(<span class="st">''</span>, reclamacion) <span class="co"># símbolos extraños (1)</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    reclamacion <span class="op">=</span> re.<span class="bu">compile</span>(<span class="st">'[^0-9a-z #+_]'</span>).sub(<span class="st">''</span>, reclamacion) <span class="co"># símbolos extraños (2)</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> reclamacion</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>datos[<span class="st">'consumer_complaint_narrative'</span>] <span class="op">=</span> datos[<span class="st">'consumer_complaint_narrative'</span>].<span class="bu">apply</span>(preprocesado) <span class="co"># aplicación de la función</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Se presenta de nuevo el ejemplo anterior para ver el resultado del procesamiento de textos realizado.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Producto:'</span>, plot_reclamaciones(datos, <span class="dv">100</span>)[<span class="dv">0</span>])</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Reclamacion:'</span>, plot_reclamaciones(datos, <span class="dv">100</span>)[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>seed<span class="op">=</span><span class="dv">123</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(seed)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>X_texto <span class="op">=</span> datos[<span class="st">'consumer_complaint_narrative'</span>]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>Y_label <span class="op">=</span> pd.get_dummies(datos[<span class="st">'product'</span>]).values <span class="co"># las categorías son</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>convertidas a variable dummy</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>X_train_text, X_test_text, Y_train, Y_test <span class="op">=</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>train_test_split(X_texto,Y_label, test_size <span class="op">=</span> <span class="fl">0.2</span>, random_state <span class="op">=</span> seed)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Entrenamiento:'</span>, X_train_text.shape)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test:'</span>, X_train_text.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Antes de definir la arquitectura de la red, se lleva a cabo la conversión del texto a variables numéricas que es el input que puede leer la red. Para ello, se realiza:</p>
<ul>
<li>La vectorización del texto asociado a las reclamaciones.</li>
<li>El truncamiento y rellenado de las secuencias de entrada para igualar la longitud en la modelización.</li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>MAX_NB_WORDS <span class="op">=</span> <span class="dv">25000</span> <span class="co"># frecuencia de palabras</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>MAX_SEQUENCE_LENGTH <span class="op">=</span> <span class="dv">200</span> <span class="co"># número de palabras en cada reclamacion</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>EMBEDDING_DIM <span class="op">=</span> <span class="dv">150</span> <span class="co"># dimensión del embedding</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> tf.keras.preprocessing.text.Tokenizer(num_words<span class="op">=</span>MAX_NB_WORDS,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>filters<span class="op">=</span><span class="st">'!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~'</span>, lower<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>tokenizer.fit_on_texts(X_train_text.values)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>word_index <span class="op">=</span> tokenizer.word_index</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Tokens:'</span>, <span class="bu">len</span>(word_index))</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> tokenizer.texts_to_sequences(X_train_text.values)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> tf.keras.preprocessing.sequence.pad_sequences(X_train,</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>maxlen<span class="op">=</span>MAX_SEQUENCE_LENGTH)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Datos de entrada:'</span>, X_train.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Por último, se crea la red neuronal siguiendo el método funcional. La red tiene las siguientes capas: - Entrada: de 200 neuronas pues corresponde con la longitud de las secuencias - Embedding: de dimensión 200 y toma como input el número máximo de palabras (25.000) - Convolucional: de 64 neuronas - MaxPooling: - Densa: de 32 neuronas y con función de activación “relu” - Salida: capa densa con 8 neuronas (número de categorías del target) y función de activación “softmax”</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># capa de entrada</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(X_train.shape[<span class="dv">1</span>],))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> tf.keras.layers.Embedding(input_dim<span class="op">=</span>MAX_NB_WORDS,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>output_dim<span class="op">=</span>EMBEDDING_DIM)(inputs)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>capa_conv <span class="op">=</span> tf.keras.layers.Conv1D(filters<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>padding<span class="op">=</span><span class="st">'valid'</span>,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>activation<span class="op">=</span><span class="st">'relu'</span>)(embedding)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>max_pooling <span class="op">=</span> tf.keras.layers.GlobalMaxPooling1D()(capa_conv)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>capa_densa <span class="op">=</span> tf.keras.layers.Dense(units<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>activation<span class="op">=</span><span class="st">'relu'</span>,</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>kernel_regularizer<span class="op">=</span>tf.keras.regularizers.l2(<span class="fl">0.01</span>))(max_pooling)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> tf.keras.layers.Dense(units<span class="op">=</span>Y_train.shape[<span class="dv">1</span>],</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>activation<span class="op">=</span><span class="st">'softmax'</span>)(capa_densa)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>modelo <span class="op">=</span> tf.keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>El summary nos muestra el número de parámetros por capa y el número de parámetros total. Puede verse que el alto número de parámetros viene, principalmente, por la capa de Embedding.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>modelo.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>La métrica utilizada para evaluar el desempeño es el accuracy y, como es un problema de clasificación multiclase, como función de pérdida categorical_crossentropy. Por su parte, se emplea Adam para la utilización del algoritmo de propagación del error hacia atrás (parámetros por defecto).</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>modelo.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>, optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>&lt;&lt;&lt; Para el proceso de entrenamiento de la red destacar:</p>
<ul>
<li>Un máximo de 10 épocas y actualización de los pesos cada 128 muestras</li>
<li>Reserva del 20% del dataset para ser usado como validación</li>
<li>Uso de parada temprana para recoger el mejor modelo posible en el proceso iterativo</li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> modelo.fit(X_train, Y_train,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>epochs<span class="op">=</span>epochs,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>batch_size<span class="op">=</span>batch_size,</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>validation_split<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>callbacks<span class="op">=</span>[tf.keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">'val_loss'</span>,</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Una vez realizado el entrenamiento, se visualiza el proceso para conocer su convergencia</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># construcción de un data.frame</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>df_train<span class="op">=</span>pd.DataFrame(history.history)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>df_train[<span class="st">'epochs'</span>]<span class="op">=</span>history.epoch</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>ax1.plot(df_train[<span class="st">'epochs'</span>], df_train[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'train_loss'</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>ax1.plot(df_train[<span class="st">'epochs'</span>], df_train[<span class="st">'val_loss'</span>], label<span class="op">=</span><span class="st">'val_loss'</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>ax2.plot(df_train[<span class="st">'epochs'</span>], df_train[<span class="st">'accuracy'</span>], label<span class="op">=</span><span class="st">'train_acc'</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>ax2.plot(df_train[<span class="st">'epochs'</span>], df_train[<span class="st">'val_accuracy'</span>], label<span class="op">=</span><span class="st">'val_acc'</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Finalmente, se estima la bondad de ajuste con la muestra de test. Para ello, como esta muestra hace referencia a la puesta en producción del modelo, es necesario crear las secuencias de este nuevo dataset en función de la tokenización del modelo.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> tokenizer.texts_to_sequences(X_test_text)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> tf.keras.preprocessing.sequence.pad_sequences(X_test,</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>maxlen<span class="op">=</span>MAX_SEQUENCE_LENGTH)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Y ahora ya sí, se realizan las predicciones y se evaluar el performance del modelo creado en la muestra de test.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># uso de argmax para pasar de probabilidad a estimación final</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>Y_test_pred <span class="op">=</span> np.argmax(modelo.predict(X_test), axis<span class="op">=</span><span class="dv">1</span>) <span class="co"># predicción de la</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>etiqueta</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>Y_test_label <span class="op">=</span> np.argmax(Y_test, axis<span class="op">=</span><span class="dv">1</span>) <span class="co"># obtención de las etiquetas sin</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>dummy</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'accuracy - test:'</span>, np.<span class="bu">round</span>(accuracy_score(Y_test_label,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>Y_test_pred),<span class="dv">5</span>))</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>sns.heatmap(confusion_matrix(Y_test_pred, Y_test_label), annot <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>fmt<span class="op">=</span><span class="st">'.0f'</span>) <span class="co"># matriz de confusión</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Matriz de confusión - test'</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="redes-recurrentes" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="redes-recurrentes"><span class="header-section-number">1.5</span> Redes Recurrentes</h2>
</section>
<section id="redes-recurrentes-elman-jordan-lstm-y-gru" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="redes-recurrentes-elman-jordan-lstm-y-gru"><span class="header-section-number">1.6</span> Redes recurrentes: Elman, Jordan, LSTM y GRU</h2>
<section id="forecasting-en-series-de-tiempo" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="forecasting-en-series-de-tiempo"><span class="header-section-number">1.6.1</span> Forecasting en series de tiempo</h3>
</section>
<section id="clasificación-y-generación-de-textos" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="clasificación-y-generación-de-textos"><span class="header-section-number">1.6.2</span> Clasificación y generación de textos</h3>
</section>
</section>
<section id="autoencoders" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="autoencoders"><span class="header-section-number">1.7</span> Autoencoders</h2>
<section id="bases-del-autoencoder" class="level3" data-number="1.7.1">
<h3 data-number="1.7.1" class="anchored" data-anchor-id="bases-del-autoencoder"><span class="header-section-number">1.7.1</span> Bases del Autoencoder</h3>
<p>Los <strong>Autoencoders (AE)</strong> son uno de los tipos de redes neuronales que caen dentro del ámbito del Deep Learning, en la que nos encontramos con un modelo de <strong>aprendizaje no supervisado</strong>. Ya se empezó a hablar de AE en la década de los 80 (Bourlard and Kamp 1988), aunque es en estos últimos años donde más se está trabajando con ellos.</p>
<p>La arquitectura de un AE es una Red Neuronal Artificial (ANN por sus siglas en inglés) que se encuentra dividida en dos partes, <strong>encoder</strong> y <strong>decoder</strong> (Charte et al.&nbsp;2018), (Goodfellow, Bengio, and Courville 2016). El encoder va a ser la parte de la ANN que va codificar o comprimir los datos de entrada, y el decoder será el encargado de regenerar de nuevo los datos en la salida. Esta estructura de codificación y decodificación le llevará a tener una estructura simétrica. El AE es entrenado para ser capaz de reconstruir los datos de entrada en la capa de salida de la ANN, implementando una serie de restricciones (la reducción de elementos en las capas ocultas del encoder) que van a evitar que simplemente se copie la entrada en la salida.</p>
<p>Si recordamos la estructura de una ANN clásica o también llamada <strong>Red Neuronal Densamente</strong> <strong>Conectada</strong> (ya que cada neurona conecta con todas las de la siguiente capa) nos encontramos en que en esta arquitectura, generalmente, el número de neuronas por capa se va reduciendo hasta llegar a la capa de salida que debería ser normalmente un número (si estamos en un problema regresión), un dato binario (si es un problema de clasificación).</p>
<p>Figura nº 86: Red Neuronal Clasica</p>
<p>Ya se empezó a hablar de AE en la década de los 80 (Bourlard and Kamp 1988), aunque es en estos últimos años donde más se está trabajando con ellos.</p>
<p>La arquitectura de un AE es una Red Neuronal Artificial (ANN por sus siglas en inglés) que se encuentra dividida en dos partes, <strong>encoder</strong> y <strong>decoder</strong> (Charte et al.&nbsp;2018), (Goodfellow, Bengio, and Courville 2016). El encoder va a ser la parte de la ANN que va codificar o comprimir los datos de entrada, y el decoder será el encargado de regenerar de nuevo los datos en la salida. Esta estructura de codificación y decodificación le llevará a tener una estructura simétrica. El AE es entrenado para ser capaz de reconstruir los datos de entrada en la capa de salida de la ANN, implementando una serie de restricciones (la reducción de elementos en las capas ocultas del encoder) que van a evitar que simplemente se copie la entrada en la salida.</p>
<p>Si recordamos la estructura de una ANN clásica o también llamada <strong>Red Neuronal Densamente Conectada</strong> (ya que cada neurona conecta con todas las de la siguiente capa) nos encontramos en que en esta arquitectura, generalmente, el número de neuronas por capa se va reduciendo hasta llegar a la capa de salida que debería ser normalmente un número (si estamos en un problema regresión), un dato binario (si es un problema de clasificación).</p>
<div id="fig-redneuronal" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-redneuronal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/Redneuronal.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-redneuronal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.22: Red Neuronal Clasica
</figcaption>
</figure>
</div>
<p>Si pensamos en una estructura básica de AE en la que tenemos una capa de entrada, una capa oculta y una capa de salida, ésta sería su representación:</p>
<div id="fig-autoencoder_basico" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-autoencoder_basico-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/autoencoder_basico.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-autoencoder_basico-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.23: Autoencoder Básico
</figcaption>
</figure>
</div>
<p>Donde los valores de son los datos de entrada y los datos son la reconstrucción de los mismos después de pasar por la capa oculta que tiene sólo dos dimensiones. El objetivo del entrenamiento de un AE será que estos valores de sean lo más parecidos posibles a los .</p>
<p>Según (Charte et al.&nbsp;2018) los AE se puden clasificar según el <strong>tipo de arquitectura de red</strong> en:</p>
<ul>
<li><p>Incompleto simple</p></li>
<li><p>Incompleto profundo</p></li>
<li><p>Extra dimensionado simple</p></li>
<li><p>Extra dimensionado profundo</p></li>
</ul>
<div id="fig-tipos_autoencoders" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tipos_autoencoders-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/tipos_autoencoders.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tipos_autoencoders-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.24: Tipos Arquitectura Autoencoders
</figcaption>
</figure>
</div>
<p>Cuando hablamos de <strong>Incompleto</strong> nos referimos a que tenemos una reducción de dimensiones que permite llegar a conseguir una “compresión” de los datos iniciales como técnica para que aprenda los patrones internos. En el caso de Extra dimensionado es cuando subimos de dimensión para conseguir que aprenda esos patrones. En este último caso sería necesario aplicar técnicas de regularización para evitar que haya un sobreajuste en el aprendizaje.</p>
<p>Cuando hablamos de <strong>Simple</strong> estamos haciendo referencia a que hay una única capa oculta, y en el caso de <strong>Profundo</strong> es que contamos con más de una capa oculta.</p>
<p>Normalmente se trabaja con las arquitecturas de tipo Incompleto profundo, sobre todo cuando se está trabajando con tipos de datos que son imágenes. Aunque también podríamos encontrar una combinación de Incompleto con Extra dimensionado profundo cuando trabajamos con tipos de datos que no son imágenes y así crecer en la primera o segunda capa oculta, para luego reducir. Esto nos permitiría por ejemplo adaptarnos a estructuras de AE en las que trabajemos con número de neuronas en una capa que sean potencia de 2, y poder construir arquitecturas dinámicas en función del tamaño de los datos, adaptándolos a un tamaño prefijado.</p>
<p>A continuación, vemos un gráfico de una estructura mixta <strong>Extra dimensionado - Incompleto profundo.</strong></p>
<div id="fig-autoencoder_mixto" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-autoencoder_mixto-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/autoencoder_mixto.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-autoencoder_mixto-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.25: Autoencoder Mixto (Incompleto y Extra dimensionado)
</figcaption>
</figure>
</div>
</section>
<section id="idea-intuitiva-del-uso-de-autoencoders" class="level3" data-number="1.7.2">
<h3 data-number="1.7.2" class="anchored" data-anchor-id="idea-intuitiva-del-uso-de-autoencoders"><span class="header-section-number">1.7.2</span> Idea intuitiva del uso de Autoencoders</h3>
<p>Si un AE trata de reproducir los datos de entrada mediante un encoder y decoder, <em>¿que nos puede aportar si ya tenemos los datos de entrada?</em></p>
<p>Ya hemos comentado que la red neuronal de un AE es simétrica y está formada por un <strong>encoder</strong> y un <strong>decoder</strong>, además cuando trabajamos con los AE que son incompletos, se está produciendo una <strong>reducción</strong> del <strong>tamaño</strong> de los datos en la fase de codificación y de nuevo una regeneración a partir de esos datos más pequeños al original. Ya tenemos uno de los conceptos más importantes de los AE que es la reducción de dimensiones de los datos de entrada. Estas nuevas variables que se generan una vez pasado el encoder se les suele llamar el <strong>espacio latente</strong>.</p>
<p>Este concepto de reducción de dimensiones en el mundo de la minería de datos lo podemos asimiliar rápidamente a técnicas como el <strong>Análisis de Componentes Principales (PCA)</strong>, que nos permite trabajar con un número más reducido de dimensiones que las originales. Igualmente, esa reducción de los datos y la capacidad de poder reconstruir el original podemos asociarlo al concepto de <strong>compresión de datos</strong>, de forma que con el encoder podemos comprimir los datos y con el decoder los podemos descomprimir. En este caso habría que tener en cuenta que sería una técnica de compresión de datos con pérdida de información (JPG también es un formato de compresión con pérdida de compresión). Es decir, con los datos codificados y el AE (pesos de la red neuronal), seríamos capaces de volver a regenerar los datos originales.</p>
<p>Otra de las ideas alrededor de los AE es que, si nosotros tenemos un conjunto de <strong>datos de la misma naturaleza</strong> y los entrenamos con nuestro AE, somos capaces de construir una red neuronal (pesos en la red neuronal) que es capaz de reproducir esos datos a través del AE. Que ocurre si nosotros metemos un dato que no era de la misma naturaleza que los que entrenaron el AE, lo que tendremos entonces es que al recrear los datos originales no va a ser posible que se parezca a los datos de entrada. De forma que el error que vamos a tener va a ser mucho mayor por no ser datos de la misma naturaleza. Esto nos puede llevar a construir un AE que permita detectar anomalías, es decir, que seamos capaces de detectar cuando un dato es una anomalía porque realmente el AE no consigue tener un error lo bastante pequeño.</p>
<p>Según lo visto de forma intuitiva vamos a tener el <strong>encoder</strong> <span class="math inline">\(f(X)\)</span> que será el encargado de codificar los datos de entrada y luego tendremos el <strong>decoder</strong> <span class="math inline">\(g(H)\)</span> que será el encargado de realizar la decodificación y conseguir acercarnos al dato original . Es decir intentamos conseguir <span class="math inline">\(g(f(X))=\hat{Y} \approx X\)</span> . Si suponemos un <strong>Simple Autoencoder</strong> en el que tenemos una única capa oculta, con una función de activación intermedia y una función de activación de salida y los parámetros y represetan los parámetros de la red neuronal en cada capa, tendríamos la siguiente expresión:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; f(X)=\delta^{(1)}\left(W^{(1)} * X+b^{(1)}\right) \\
&amp; g(H)=\delta^{(2)}\left(W^{(2)} * H+b^{(2)}\right)
\end{aligned}
\]</span></p>
<p>donde <span class="math inline">\(f(g(H))=\delta^{(2)}\left(W^{(2)} *\left(\delta^{(1)}\left(W^{(1)} * H+b^{(1)}\right)+b^{(2)}\right)\right)=\hat{Y} \approx X\)</span></p>
<p>Así tendremos que <span class="math inline">\(g(f(X))=\hat{Y}\)</span> donde <span class="math inline">\(\hat{Y}\)</span> será la reconstrucción de <span class="math inline">\(X\)</span>.</p>
<p>Una vez tenemos la idea intuitiva de para qué nos puede ayudar un AE, recopilamos algunos de los <strong>principales usos</strong> sobre los que actualmente se está trabajando.</p>
</section>
<section id="casos-de-uso" class="level3" data-number="1.7.3">
<h3 data-number="1.7.3" class="anchored" data-anchor-id="casos-de-uso"><span class="header-section-number">1.7.3</span> Casos de uso</h3>
<p><strong>Reducción de dimensiones / Compresión de datos</strong></p>
<p>En la idea intuitiva de los AE ya hemos visto claro que se pueden usar para la reducción de dimensiones de los datos de entrada. Si estamos ante unos datos de entrada de tipo estructurado estamos en un caso de reducción de dimensiones clásico, en el que queremos disminuir el número de variables con las que trabajar.</p>
<p>Muchas veces este tipo de trabajo se hace mediante el PCA (Análisis de Componente Principales, por sus siglas en inglés), sabiendo que lo que se realiza es una transformación lineal de los datos, ya que conseguimos unas nuevas variables que son una combinación lineal de las mismas. En el caso de los AE conseguimos mediante las funciones de activación no lineales (simgmoide, ReLu, tanh, etc) combinaciones no lineales de las variables originales para reducir las dimensiones. También existen versiones de PCA no lineales llamadas Kernel PCA que mediante las técnicas de kernel son capaces de construir relaciones no lineales.</p>
<p>En esta línea estamos viendo que cuando el encoder ha actuado, tenemos unos nuevos datos más reducidos y que somos capaces de practicamente volver a reproducir teniendo el decoder. Podríamos pensar en este tipo de técnica para simplemente comprimir información. Hay que tener en cuenta que este tipo de técnicas no se pueden aplicar a cualquier dato que queramos comprimir, ya que debemos haber entrenado al AE con unos datos de entrenamiento que ha sido capaz de obtener ciertos patrones de ellos, y por eso es capaz luego de reproducirlos.</p>
<p><strong>Búsqueda de imágenes</strong></p>
<p>Cuando pensamos en un buscador de imágenes nos podemos hacer a la idea que el buscar al igual que con el texto nos va a mostrar entradas que seán imágenes parecidas a la que estamos buscando.</p>
<p>Si construimos un autoencoder, el encoder nos va a dar unas variables con información para poder recrear de nuevo la imagen. Lo que parece claro es que si hay muy poca distancia entre estas variables y otras la reconstrucción de la imagen será muy parecida.</p>
<p>Así nosotros podemos entrenar el AE con nuestro conjunto de imágenes, una vez tenemos el AE pasamos el encoder a todas las imágenes y las tenemos todas en ese nuevo espacio de variables.</p>
<p>Cuando queremos buscar una imagen, le pasamos el autoencoder, y ya buscamos las más cercanas a nuestra imagen en el espacio de variables generado por el encoder.</p>
<p><strong>Detección de Anomalías</strong></p>
<p>Cuando estamos ante un problema de clasificación y tenemos un conjunto de datos que está muy desbalanceado, es decir, tenemos una clase mayoritaria que es mucho más grande que la minoritaria (posiblemente del orden de más del 95%), muchas veces es complicado conseguir un conjunto de datos balanceado que sea realmente bueno para hacer las predicciones.</p>
<p>Cuando estamos en estos entornos tan desbalanceados muchas veces se dice que estamos ante un sistema para detectar <strong>anomalías</strong>. Un AE nos puede ayudar a detectar estas anomalías de la siguiente forma:</p>
<ul>
<li>Tomamos todos los datos de <strong>entrenamiento</strong> de la <strong>clase mayoritaria</strong> (o normales) y construimos un AE para ser capaces de reproducirlos. Al ser todos estos datos de la misma naturaleza conseguiremos entrenar el AE con un error muy pequeño.</li>
<li>Ahora tomamos los datos de la <strong>clase</strong> <strong>minoritaria</strong> (o a nomalías) y los pasamos a través del AE obteniendo unos <strong>errores</strong> de <strong>reconstrucción</strong>.</li>
<li>Definimos el <strong>umbral</strong> de error que nos separará los datos normales de las anomalías, ya que el AE sólo está entrenado con los normales y conseguirá un error más alto con las anomalías al reconstruirlas.</li>
<li>Cogemos los <strong>datos</strong> de test y los vamos pasando por el AE, si el <strong>error</strong> es <strong>menor</strong> del <strong>umbral</strong>, entonces será de la <strong>clase mayoritaria</strong>. Si el error es mayor que el umbral, entonces estaremos ante una anomalía.</li>
</ul>
<p><strong>Eliminación de ruido</strong></p>
<p>Otra de las formas de uso de los autoencoders en tratamiento de imágenes es para eliminar ruido de las mismas, es decir poder quitar manchas de las imágenes. La forma de hacer esto es la siguiente:</p>
<ul>
<li><p>Partimos de un conjunto de datos de entrenamiento (imágenes) a las que le metemos ruido, por ejemplo, modificando los valores de cada pixel usando una distribución normal, de forma que obtenemos unos datos de entrenamiento con ruido.</p></li>
<li><p>Construimos el AE de forma que los datos de entrada son los que tienen ruido, pero los de salida vamos a forzar que sean los originales. De forma que intentamos que aprendan a reconstruirse como los que no tienen ruido.</p></li>
<li><p>Una vez que tenemos el AE y le pasamos datos de test con ruido, seremos capaces de reconstruirlos sin el ruido.</p></li>
</ul>
<p><strong>Modelos generativos</strong></p>
<p>Cuando hablamos de modelos generativos, nos referimos a AE que son capaces de generar cosas nuevas a las que existían. De forma que mediante técnicas como los Variational Autoencoders, los Adversarial Autoencoders seremos capaces de generar nuevas imágenes que no teníamos inicialmente. Es decir, podríamos pensar en poder tener un AE que sea capaz de reconstruir imágenes de caras, pero que además con toda la información aprendida fuera capaz de generar nuevas caras que realmente no existen.</p>
</section>
<section id="diseño-del-modelo-de-ae" class="level3" data-number="1.7.4">
<h3 data-number="1.7.4" class="anchored" data-anchor-id="diseño-del-modelo-de-ae"><span class="header-section-number">1.7.4</span> Diseño del modelo de AE</h3>
<p><strong>Transformación de datos</strong></p>
<p>Cuando se trabaja con redes neuronales y en particular con AEs, necesitamos representar los valores de las variables de entrada en forma numérica. En una red neuronal todos los datos son siempre numéricos. Esto significa que todas aquellas variables que sean categóricas necesitamos convertirlas en numéricas. Además es muy conveniente normalizar los datos para poder trabajar con valores entre 0 y 1, que van a ayudar a que sea más fácil que se pueda converger a la solución.</p>
<p>Como ya sabemos normalmente nos encontramos que en una red neuronal las variables de salida son:</p>
<ul>
<li>un número (regresión)</li>
<li>una serie de números (regresión múltiple)</li>
<li>un dato binario (clasificación binaria)</li>
<li>un número que representa una categoría (clasifiación múltiple)</li>
</ul>
<p>En el caso de los AE puede que tengamos una gran parte de las veces valores de series de números, ya que necesitamos volver a representar los datos de entrada. Esto significa que tendremos que conseguir en la capa de salida esos datos numéricos que teníamos inicialmente, como si se tuviera una regresión múltiple.</p>
<p><strong>Arquitectura de red</strong></p>
<p>Como ya se ha comentado en las redes neuronales, algunos de los hiperparámetros más importantes en un AE son los relacionados con la arquitectura de la red neuronal.</p>
<p>Para la construcción de un AE vamos a elegir una topología simétrica del encoder y el decoder.</p>
<p>Durante el diseño del AE necesitaremos ir probando y adaptando todos estos hiperparámetros de la ANN para conseguir que sea lo más eficiente posible:</p>
<ul>
<li><p>Número de capas ocultas y neuronas en cada una</p></li>
<li><p>Función de coste y pérdida</p></li>
<li><p>Optimizador</p></li>
<li><p>Función de activación en capas ocultas</p></li>
<li><p>Función de activación en salida</p></li>
</ul>
<p><strong>Número de capas ocultas y neuronas en cada una</strong></p>
<p>La selección del número de capas ocultas y la cantidad de neuronas en cada una va a ser un procedimiento de prueba y error en el que se pueden probar muchas combinaciones. Es cierto que en el caso de trabajar con imágenes y CNN ya hay muchas arquitecturas definidas y probadas que consiguen muy buenos resultados. Por otro lado para tipos de datos estructurados será muy dependiente de esos datos, de forma que será necesario realizar diferentes pruebas para conseguir un buen resultado.</p>
<p><strong>Función de coste y pérdida</strong></p>
<p>En este caso no hay ninguna recomendación especial para las funciones de costes/pérdida y dependerá al igual que en las redes neuronales de la naturaleza de los datos de salida con los que vamos a trabajar.</p>
<p><strong>Optimizador</strong></p>
<p>Se recomienda usar el optimizador <strong>ADAM</strong> (Diederik P. Kingma 2017) que es el que mejores resultados ha dado en las pruebas según (Walia 2017), consiguiendo una convergencia más rápida que con el resto de optimizadores.</p>
<p><strong>Función de activación en capas ocultas</strong></p>
<p>En un AE las funciones de activación en las capas ocultas van a conseguir establecer las restricciones no lineales al pasar de una capa a la siguiente, normalmente se evita usar la función de activación lineal en las capas intermedias ya que queremos conseguir transformaciones no lineales.</p>
<p>Se recomienda usar la función de activación ReLu en las capas ocultas, ya que parece ser que es la que mejores resultados da en la convergencia de la solución y además menor coste computacional tiene a la hora de realizar los cálculos.</p>
<p><strong>Función de activación en salida</strong></p>
<p>En la capa de salida tenemos que tener en cuenta cual es el tipo de datos final que queremos obtener, que en el caso de un AE es el mismo que el tipo de dato de entrada. Normalmente las funciones de activación que se usarán en la última capa seran:</p>
<p>• Lineal con multiples unidades, para regresión de varios datos numéricos</p>
<p>• Sigmoid para valores entre 0 y 1</p>
</section>
<section id="ejemplos-de-autoencoders" class="level3" data-number="1.7.5">
<h3 data-number="1.7.5" class="anchored" data-anchor-id="ejemplos-de-autoencoders"><span class="header-section-number">1.7.5</span> Ejemplos de Autoencoders</h3>
<p>Una vez entendido el funcionamiento de los AE, veamos algunos de los AE que se pueden construir para diversas tareas.</p>
<ul>
<li><p>Simple</p></li>
<li><p>Multicapa o Profundo</p></li>
<li><p>Convolucional</p></li>
<li><p>Denoising</p></li>
</ul>
<p>En la descripción de los tipos de AE vamos a pensar en python y el framework keras con el backend Tensorflow. Todo el código se proporciona aparte. Usaremos como <strong>dataset</strong> a <strong>MINIST</strong>, que contiene 60.000/10.000 (entrenamiento/validación) <strong>imágenes</strong> de los <strong>números del 0 al 9</strong>, escritos a mano. Cada imagen tiene un tamaño de 28x28 = 784 pixels, en <strong>escala de grises</strong>, con lo que para cada pixel tendremos un valor entre <strong>0 y 255</strong> para definir cuál es su intensidad de gris.</p>
<p><strong>Autoencoder Simple</strong></p>
<p>Vamos a describir como construir un <strong>Autoencoder Simple</strong> usando una <strong>red neuronal densamente conectada</strong> en lugar de usar una red neuronal convolucional, para que sea más sencillo comprender el ejemplo.</p>
<p>Es decir, vamos a tratar los datos de entrada como si fueran unos datos numéricos que nos da cualquier variable, que queremos reproducir y no vamos a utilizar ninguna de las técnicas asociadas a las redes convolucionales. Hay que recordar que las redes convolucionales permiten mediante un tratamiento de las imágenes (convolución, pooling, etc) conseguir mejores resultados que si lo hiciéramos directamente con redes densamente conectadas.</p>
<p>En este caso tendremos una capa de <strong>entrada</strong> con <strong>784 neuronas</strong> (correspondientes a los pixels de cada imagen), una capa <strong>intermedia</strong> de <strong>32 neuronas</strong>, y una capa de salida de nuevo de las 784 neuronas para poder volver a obtener de nuevo los datos originales.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># This is the size of our encoded representations</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>encoding_dim <span class="op">=</span> <span class="dv">32</span>  <span class="co"># 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># This is our input image</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">784</span>,))</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># "encoded" is the encoded representation of the input</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> layers.Dense(encoding_dim, activation<span class="op">=</span><span class="st">'relu'</span>)(input_img)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co"># "decoded" is the lossy reconstruction of the input</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>decoded <span class="op">=</span> layers.Dense(<span class="dv">784</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(encoded)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co"># This model maps an input to its reconstruction</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>autoencoder <span class="op">=</span> keras.Model(input_img, decoded)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="co"># This model maps an input to its encoded representation</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> keras.Model(input_img, encoded)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="co"># This is our encoded (32-dimensional) input</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>encoded_input <span class="op">=</span> keras.Input(shape<span class="op">=</span>(encoding_dim,))</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve the last layer of the autoencoder model</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>decoder_layer <span class="op">=</span> autoencoder.layers[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the decoder model</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>decoder <span class="op">=</span> keras.Model(encoded_input, decoder_layer(encoded_input))</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>autoencoder.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>, loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.datasets <span class="im">import</span> mnist</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>(x_train, _), (x_test, _) <span class="op">=</span> mnist.load_data()</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> x_train.astype(<span class="st">'float32'</span>) <span class="op">/</span> <span class="fl">255.</span></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> x_test.astype(<span class="st">'float32'</span>) <span class="op">/</span> <span class="fl">255.</span></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> x_train.reshape((<span class="bu">len</span>(x_train), np.prod(x_train.shape[<span class="dv">1</span>:])))</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> x_test.reshape((<span class="bu">len</span>(x_test), np.prod(x_test.shape[<span class="dv">1</span>:])))</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_train.shape)</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_test.shape)</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>autoencoder.fit(x_train, x_train,</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>                epochs<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>                batch_size<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>                shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>                validation_data<span class="op">=</span>(x_test, x_test))</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode and decode some digits</span></span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that we take them from the *test* set</span></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>encoded_imgs <span class="op">=</span> encoder.predict(x_test)</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>decoded_imgs <span class="op">=</span> decoder.predict(encoded_imgs)</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Use Matplotlib (don't ask)</span></span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span>  <span class="co"># How many digits we will display</span></span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display original</span></span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">2</span>, n, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>    plt.imshow(x_test[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>    plt.gray()</span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a>    ax.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>    ax.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display reconstruction</span></span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">2</span>, n, i <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> n)</span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a>    plt.imshow(decoded_imgs[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a>    plt.gray()</span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a>    ax.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a>    ax.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Autoencoder Multicapa o profundo</strong></p>
<p>Vamos a pasar ahora a una versión del autoencoder donde habilitamos <strong>más capas ocultas</strong> y hacemos que el descenso del número de neuronas sea más gradual hasta llegar a nuestro valor deseado, para luego volver a reconstruirlo.</p>
<p>En este caso seguimos con redes densamente conectadas y aplicamos varias capas intermedias reduciendo el número de neuronas en cada una hasta llegar a la capa donde acaba el encoder para volver a ir creciendo en las sucesivas capas hasta llegar a la de salida.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># This is the size of our encoded representations</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>encoding_dim <span class="op">=</span> <span class="dv">32</span>  <span class="co"># 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># This is our input image</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">784</span>,))</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(input_img)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(encoded)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(encoded)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>decoded <span class="op">=</span> layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(encoded)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>decoded <span class="op">=</span> layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(decoded)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>decoded <span class="op">=</span> layers.Dense(<span class="dv">784</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(decoded)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="co"># This model maps an input to its reconstruction</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>autoencoder <span class="op">=</span> keras.Model(input_img, decoded)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co"># This model maps an input to its encoded representation</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>autoencoder.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>, loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.datasets <span class="im">import</span> mnist</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>(x_train, _), (x_test, _) <span class="op">=</span> mnist.load_data()</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> x_train.astype(<span class="st">'float32'</span>) <span class="op">/</span> <span class="fl">255.</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> x_test.astype(<span class="st">'float32'</span>) <span class="op">/</span> <span class="fl">255.</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> x_train.reshape((<span class="bu">len</span>(x_train), np.prod(x_train.shape[<span class="dv">1</span>:])))</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> x_test.reshape((<span class="bu">len</span>(x_test), np.prod(x_test.shape[<span class="dv">1</span>:])))</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_train.shape)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_test.shape)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>autoencoder.fit(x_train, x_train,</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>                epochs<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>                batch_size<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>                shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>                validation_data<span class="op">=</span>(x_test, x_test))</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode and decode some digits</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that we take them from the *test* set</span></span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>decoded_imgs <span class="op">=</span> autoencoder.predict(x_test)</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(decoded_imgs.shape)</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Use Matplotlib (don't ask)</span></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span>  <span class="co"># How many digits we will display</span></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display original</span></span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">2</span>, n, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>    plt.imshow(x_test[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>    plt.gray()</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>    ax.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>    ax.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display reconstruction</span></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">2</span>, n, i <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> n)</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>    plt.imshow(decoded_imgs[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>    plt.gray()</span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>    ax.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>    ax.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Autoencoder Convolucional</strong></p>
<p>En nuestro ejemplo al estar trabajando con imágenes podemos pasar a trabajar con Redes Convolucionales (CNN) de forma que en lugar de usar las capas densamente conectadas que hemos usado hasta ahora, vamos a pasar a usar las capacidades de las redes convolucionales.</p>
<p>Al trabajar con redes convolucionales necesitaremos trabajar con capas de convolución o pooling para llegar a la capa donde acaba el encoder para volver a ir creciendo aplicando operaciones de convolución y upsampling (contrario al pooling).</p>
<p>En nuestro ejemplo vamos a tener los siguientes elementos.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">16</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(input_img)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">8</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">8</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="co"># at this point the representation is (4, 4, 8) i.e. 128-dimensional</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">8</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(encoded)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.UpSampling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">8</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.UpSampling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">16</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.UpSampling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>decoded <span class="op">=</span> layers.Conv2D(<span class="dv">1</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'sigmoid'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>autoencoder <span class="op">=</span> keras.Model(input_img, decoded)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>autoencoder.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>, loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.datasets <span class="im">import</span> mnist</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>(x_train, _), (x_test, _) <span class="op">=</span> mnist.load_data()</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> x_train.astype(<span class="st">'float32'</span>) <span class="op">/</span> <span class="fl">255.</span></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> x_test.astype(<span class="st">'float32'</span>) <span class="op">/</span> <span class="fl">255.</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> np.reshape(x_train, (<span class="bu">len</span>(x_train), <span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> np.reshape(x_test, (<span class="bu">len</span>(x_test), <span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>autoencoder.fit(x_train, x_train,</span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>                epochs<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>                batch_size<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>                shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>                validation_data<span class="op">=</span>(x_test, x_test))</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>decoded_imgs <span class="op">=</span> autoencoder.predict(x_test)</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display original</span></span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">2</span>, n, i)</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>    plt.imshow(x_test[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>    plt.gray()</span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>    ax.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>    ax.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display reconstruction</span></span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">2</span>, n, i <span class="op">+</span> n)</span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a>    plt.imshow(decoded_imgs[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a>    plt.gray()</span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a>    ax.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a>    ax.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Autoencoder Denoising</strong></p>
<p>Vaamos a usar ahora un autoencoder para hacer limpieza en imagen, es decir, conseguir a partir de una imagen que tiene ruido otra imagen sin ese ruido. Entrenaremos al autoencoder para que limpie “ruido” que hay en la imagen y lo reconstruya sin ello. El ruido lo vamos a generar mediante una distribución normal y modificaremos el valor de los pixels de las imágenes.</p>
<p>Usaremos estas imágenes con ruido para que sea capaz de reconstruir la imagen original sin ruido con el AE. Para realizar este proceso lo que haremos será:</p>
<p>• Crear nuevas imágenes con ruido</p>
<p>• Entrenar el autoencoder con estas nuevas imágenes</p>
<p>• Calcular el error de reconstrucción respecto a las imágenes originales</p>
<p>Al estar trabajando con imágenes vamos a partir del Autoencoder de Convolución para poder aplicar el denosing.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">16</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(input_img)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">8</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">8</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co"># at this point the representation is (4, 4, 8) i.e. 128-dimensional</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">8</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(encoded)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.UpSampling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">8</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.UpSampling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">16</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.UpSampling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>decoded <span class="op">=</span> layers.Conv2D(<span class="dv">1</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'sigmoid'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>autoencoder <span class="op">=</span> keras.Model(input_img, decoded)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>autoencoder.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>, loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.datasets <span class="im">import</span> mnist</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>(x_train, _), (x_test, _) <span class="op">=</span> mnist.load_data()</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> x_train.astype(<span class="st">'float32'</span>) <span class="op">/</span> <span class="fl">255.</span></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> x_test.astype(<span class="st">'float32'</span>) <span class="op">/</span> <span class="fl">255.</span></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> np.reshape(x_train, (<span class="bu">len</span>(x_train), <span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> np.reshape(x_test, (<span class="bu">len</span>(x_test), <span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>noise_factor <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>x_train_noisy <span class="op">=</span> x_train <span class="op">+</span> noise_factor <span class="op">*</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span>x_train.shape) </span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>x_test_noisy <span class="op">=</span> x_test <span class="op">+</span> noise_factor <span class="op">*</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span>x_test.shape) </span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>x_train_noisy <span class="op">=</span> np.clip(x_train_noisy, <span class="fl">0.</span>, <span class="fl">1.</span>)</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>x_test_noisy <span class="op">=</span> np.clip(x_test_noisy, <span class="fl">0.</span>, <span class="fl">1.</span>)</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">2</span>))</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">1</span>, n, i)</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>    plt.imshow(x_test_noisy[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>    plt.gray()</span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>    ax.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>    ax.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a>autoencoder.fit(x_train_noisy, x_train,</span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>                epochs<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a>                batch_size<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a>                shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a>                validation_data<span class="op">=</span>(x_test_noisy, x_test))</span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>decoded_imgs <span class="op">=</span> autoencoder.predict(x_test)</span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb22-67"><a href="#cb22-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display original</span></span>
<span id="cb22-68"><a href="#cb22-68" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">2</span>, n, i)</span>
<span id="cb22-69"><a href="#cb22-69" aria-hidden="true" tabindex="-1"></a>    plt.imshow(x_test_noisy[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb22-70"><a href="#cb22-70" aria-hidden="true" tabindex="-1"></a>    plt.gray()</span>
<span id="cb22-71"><a href="#cb22-71" aria-hidden="true" tabindex="-1"></a>    ax.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb22-72"><a href="#cb22-72" aria-hidden="true" tabindex="-1"></a>    ax.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb22-73"><a href="#cb22-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-74"><a href="#cb22-74" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display reconstruction</span></span>
<span id="cb22-75"><a href="#cb22-75" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">2</span>, n, i <span class="op">+</span> n)</span>
<span id="cb22-76"><a href="#cb22-76" aria-hidden="true" tabindex="-1"></a>    plt.imshow(decoded_imgs[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb22-77"><a href="#cb22-77" aria-hidden="true" tabindex="-1"></a>    plt.gray()</span>
<span id="cb22-78"><a href="#cb22-78" aria-hidden="true" tabindex="-1"></a>    ax.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb22-79"><a href="#cb22-79" aria-hidden="true" tabindex="-1"></a>    ax.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb22-80"><a href="#cb22-80" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="arquitecturas-preentrenadas" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="arquitecturas-preentrenadas"><span class="header-section-number">1.8</span> Arquitecturas preentrenadas</h2>
<p>Las <strong>arquitecturas pre-entrenadas</strong> en Deep Learning son modelos de redes neuronales que han sido previamente entrenados en grandes conjuntos de datos para realizar tareas específicas como clasificación de imágenes, generación de texto o reconocimiento de voz. Durante el entrenamiento, estos modelos aprenden patrones complejos de los datos, lo que les permite realizar tareas relacionadas con alta precisión y generalización.</p>
<p>El funcionamiento de las arquitecturas pre-entrenadas se basa en el concepto de <strong>transferencia de aprendizaje</strong>. La transferencia de aprendizaje consiste en aprovechar el conocimiento aprendido en una tarea para aplicarlo a otra tarea diferente; es decir, se utiliza como base para realizar tareas específicas con mayor facilidad.</p>
<p>Las arquitecturas pre-entrenadas suelen ser diseñadas por investigadores y equipos de desarrollo en instituciones académicas, laboratorios de investigación y empresas de tecnología. Estos expertos desarrollan las arquitecturas, definen el proceso de entrenamiento y seleccionan los conjuntos de datos adecuados para cada tarea. Algunas de las organizaciones e instituciones que lideran el diseño de arquitecturas pre-entrenadas son:</p>
<ul>
<li><em>Google AI</em>: ha desarrollado arquitecturas pre-entrenadas como BERT y Vision Transformers, las cuales han tenido un gran impacto en el procesamiento del lenguaje natural y la visión artificial, respectivamente. Además, en los últimos años, han publicado Gema y Gemini, que permiten el aprendizaje automático en general, ofreciendo flexibilidad, escalabilidad y alto rendimiento en diversas tareas</li>
<li><em>Facebook AI Research</em>: ha contribuido con arquitecturas pre-entrenadas como FAIRSEQ y Detectron2, que han impulsado el rendimiento en tareas de procesamiento del lenguaje natural y detección de objetos, respectivamente. Además, también ha desarrollado Llama, Llama2 y Llama3, modelos que han destacado por su versatilidad y precisión en aplicaciones de reconocimiento de voz y procesamiento de texto.</li>
<li><em>OpenAI</em>: conocido por sus arquitecturas pre-entrenadas líderes como GPT-3 y Whisper, ha lanzado recientemente GPT-4. Este modelo promete mejorar la comprensión contextual y la generación de texto, lo que podría tener un gran impacto en aplicaciones de procesamiento del lenguaje natural. GPT-4 representa un avance significativo en la investigación de inteligencia artificial y abre nuevas posibilidades para sistemas más avanzados.</li>
</ul>
<p>Estos modelos son entrenados en grandes clústeres de servidores con hardware especializado, como GPUs y TPUs, utilizando conjuntos de datos masivos y técnicas de optimización avanzadas. A día de hoy se han convertido en un elemento capital para los desarrolladores de aprendizaje automático puesto que nos permite aprovechar modelos entrenados previamente que, en condiciones normales, sería casi imposible realizar por centros no especializados. Así, podríamos destacar las siguientes ventajas:</p>
<ul>
<li>Ahorro de tiempo y recursos: al aprovechar el conocimiento pre-entrenado, nos permiten a los desarrolladores ahorrar tiempo y recursos computacionales en comparación con entrenar un modelo desde cero</li>
<li>Mayor rendimiento: suelen ofrecer un rendimiento superior a los modelos entrenados desde cero, especialmente para tareas complejas</li>
<li>Facilidad de uso: las librerías y entornos facilitan el acceso y la utilización de arquitecturas pre-entrenadas, incluso para usuarios con poca experiencia en Deep Learning</li>
</ul>
<p>Como se ha dicho, cada vez más se emplean este tipo de arquitecturas para realizar tareas que, con otro tipo de diseños de aprendizaje automático serían más complejas y costosas de abordar. En este aspecto, destacar que a la hora de elegir qué tipo de arquitectura pre-entrenada es adecuada para un caso de uso es necesario tener en cuenta la tarea específica a abordar, el conjunto de datos con los que ha sido entrenada y la similaridad con los nuestros propios así como si disponemos de ciertas limitaciones en cuanto a recursos computacionales.</p>
<p>Por último, destacar que existen diversas librerías y entornos que facilitan el trabajo con arquitecturas pre-entrenadas. Algunas de las opciones más interesantes son:</p>
<p>- TensorFlow Hub: es un repositorio de módulos pre-entrenados para TensorFlow, que incluye una amplia gama de arquitecturas pre-entrenadas para diversas tareas. - Hugging Face Hub: es una plataforma similar a TensorFlow Hub, pero que ofrece soporte para múltiples frameworks de Deep Learning, incluyendo TensorFlow, PyTorch y JAX.</p>
<section id="paquetes-específicos-en-python" class="level3" data-number="1.8.1">
<h3 data-number="1.8.1" class="anchored" data-anchor-id="paquetes-específicos-en-python"><span class="header-section-number">1.8.1</span> Paquetes específicos en Python</h3>
<section id="transformers" class="level4" data-number="1.8.1.1">
<h4 data-number="1.8.1.1" class="anchored" data-anchor-id="transformers"><span class="header-section-number">1.8.1.1</span> Transformers</h4>
<p>Transformers es una biblioteca de código abierto en Python desarrollada por Hugging Face que proporciona un conjunto de herramientas para trabajar con modelos de lenguaje basados en redes neuronales transformadoras.</p>
<p>Esta librería se caracteriza por su gran variedad de modelos pre-entrenados y capacidad de hacer fine-tuning de modelos. Así, entre sus principales ventajas podemos citar: - La facilidad de uso - La flexibilidad y la escalabilidad - La comunidad activa de desarrolladores</p>
<p>Finalmente, indicamos proporcionamos una serie de recursos adicionales:</p>
<ul>
<li>Sitio web de Transformers: <a href="https://huggingface.co/docs/transformers/en/index" class="uri">https://huggingface.co/docs/transformers/en/index</a></li>
<li>Documentación de Transformers: <a href="https://huggingface.co/docs" class="uri">https://huggingface.co/docs</a></li>
<li>Tutoriales de Transformers: <a href="https://www.youtube.com/watch?v=QEaBAZQCtwE" class="uri">https://www.youtube.com/watch?v=QEaBAZQCtwE</a></li>
</ul>
</section>
</section>
<section id="tensorflow-hub" class="level3" data-number="1.8.2">
<h3 data-number="1.8.2" class="anchored" data-anchor-id="tensorflow-hub"><span class="header-section-number">1.8.2</span> Tensorflow-Hub</h3>
<p>Tensorflow-Hub alberga una gran colección de módulos de aprendizaje automático pre-entrenados y reutilizables, creados por Google y la comunidad de TensorFlow.</p>
<p>Esta librería se caracteriza por disponer de una gran cantidad de modelos pre-entrenados para diferentes tareas relacionadas con el aprendizaje profundo. Así, entre sus principales ventajas podemos citar: - El acceso a modelos pre-entrenados de última generación - La flexibilidad y personalización - La facilidad de</p>
<p>Finalmente, indicamos proporcionamos una serie de recursos adicionales:</p>
<ul>
<li>Sitio web de TensorFlow Hub: <a href="https://www.tensorflow.org/hub" class="uri">https://www.tensorflow.org/hub</a></li>
<li>Documentación de TensorFlow Hub: <a href="https://www.tensorflow.org/hub" class="uri">https://www.tensorflow.org/hub</a></li>
<li>Tutoriales de TensorFlow Hub: <a href="https://www.tensorflow.org/hub/tutorials" class="uri">https://www.tensorflow.org/hub/tutorials</a></li>
</ul>
</section>
<section id="arquitecturas-zero-shot" class="level3" data-number="1.8.3">
<h3 data-number="1.8.3" class="anchored" data-anchor-id="arquitecturas-zero-shot"><span class="header-section-number">1.8.3</span> Arquitecturas Zero-shot</h3>
<p>Las <strong>arquitecturas Zero-Shot</strong>, también conocidas como <em>modelos de Aprendizaje por Analogía</em> son capaces de realizar tareas de clasificación sin necesidad de entrenamiento específico para cada categoría. En su lugar, estas arquitecturas aprenden representaciones vectoriales de conceptos a partir de datos no etiquetados, lo que les permite generalizar a nuevas categorías sin haberlas visto nunca antes.</p>
<p><img src="../apuntes/soporte_imagenes/zero-shot.png" width="720" height="720"></p>
<section id="uso-en-imágenes" class="level4" data-number="1.8.3.1">
<h4 data-number="1.8.3.1" class="anchored" data-anchor-id="uso-en-imágenes"><span class="header-section-number">1.8.3.1</span> Uso en imágenes</h4>
<p>En el ámbito de la visión artificial, las arquitecturas Zero-Shot para imágenes han demostrado ser particularmente útiles para tareas como la clasificación de imágenes de escenas naturales, la identificación de animales y la detección de objetos. Entre los modelos más destacados en esta área se encuentran: - <em>ImageNet-pretrained CLIP</em>: basado en la arquitectura CLIP (Contrastive Language-Image Pre-training), este modelo utiliza representaciones de imágenes y texto para realizar clasificación Zero-Shot de imágenes con gran precisión - <em>ResNet-pretrained ZSL</em>: Este modelo combina la arquitectura ResNet, conocida por su rendimiento en tareas de clasificación de imágenes, con un enfoque Zero-Shot basado en la distancia entre representaciones</p>
<p><code>MATERIAL COMPLEMENTARIO - NOTEBOOK</code>: ejemplo en <code>Zero-shot: clasificación animales</code></p>
</section>
<section id="uso-en-texto" class="level4" data-number="1.8.3.2">
<h4 data-number="1.8.3.2" class="anchored" data-anchor-id="uso-en-texto"><span class="header-section-number">1.8.3.2</span> Uso en texto</h4>
<p>En el procesamiento del lenguaje natural, las arquitecturas Zero-Shot para texto han ganado popularidad para tareas como la clasificación de documentos, la extracción de información y la categorización de textos. Algunos modelos representativos en este campo son:</p>
<ul>
<li><em>BERT-pretrained ZSL</em>: basado en la arquitectura BERT (Bidirectional Encoder Representations from Transformers), este modelo utiliza representaciones contextuales de palabras para realizar clasificación Zero-Shot de texto con gran precisión</li>
<li><em>Siamese Networks with Word Embeddings</em>: este enfoque utiliza redes siamesas, un tipo de red neuronal que aprende a comparar pares de entradas, para clasificar texto Zero-Shot utilizando representaciones de palabras pre-entrenadas.</li>
</ul>
</section>
</section>
<section id="detección-de-objetos" class="level3" data-number="1.8.4">
<h3 data-number="1.8.4" class="anchored" data-anchor-id="detección-de-objetos"><span class="header-section-number">1.8.4</span> Detección de objetos</h3>
<p>La <strong>detección de objetos</strong> es una tarea fundamental en el ámbito de la visión artificial, que consiste en identificar y localizar objetos dentro de una imagen o video. Las arquitecturas pre-entrenadas para la detección de objetos han revolucionado este campo, ofreciendo modelos de alta precisión y eficiencia. Entre los modelos más utilizados se encuentran:</p>
<ul>
<li><em>Faster R-CNN</em>: Este modelo combina la arquitectura de redes convolucionales profundas (CNN) con una región de propuesta de regiones (RPN) para detectar y localizar objetos con gran precisión</li>
<li><em>YOLOv5</em>: Este modelo destaca por su velocidad y eficiencia, utilizando una arquitectura basada en convoluciones y bloques de atención para detectar objetos en tiempo real</li>
</ul>
<p><code>MATERIAL COMPLEMENTARIO - VIDEOTUTORIAL</code>: ejemplo en <code>Arquitecturas Tensorflow-Hub</code></p>
</section>
<section id="conversión-de-voz-a-texto" class="level3" data-number="1.8.5">
<h3 data-number="1.8.5" class="anchored" data-anchor-id="conversión-de-voz-a-texto"><span class="header-section-number">1.8.5</span> Conversión de voz a texto</h3>
<p>La <em>conversión de voz a texto</em>, también llamado como <strong>Speech-to-Text</strong> es una tarea crucial para la interacción hombre-máquina. Las arquitecturas pre-entrenadas para este tipo de casos han impulsado el desarrollo de sistemas de reconocimiento de voz de alta precisión, capaces de transcribir audio en texto con gran fluidez. Uno de los principales modelos para esta tarea es <em>Whisper</em> el cual fue desarrollado por OpenAI. Este modelo se basa en la arquitectura <em>Transformer</em> y utiliza aprendizaje supervisado y multitarea para lograr una precisión y robustez excepcionales en una amplia gama de condiciones acústicas.</p>
<p><code>MATERIAL COMPLEMENTARIO - VIDEOTUTORIAL</code>: ejemplo en <code>Speech2Text</code></p>
</section>
</section>
<section id="aprendizaje-por-refuerzo" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="aprendizaje-por-refuerzo"><span class="header-section-number">1.9</span> Aprendizaje por Refuerzo</h2>
<section id="introducción-2" class="level3" data-number="1.9.1">
<h3 data-number="1.9.1" class="anchored" data-anchor-id="introducción-2"><span class="header-section-number">1.9.1</span> Introducción</h3>
<p>Hasta ahora hemos visto como el Deep Learning se usa para el <strong>aprendizaje supervisado</strong> y el <strong>aprendizaje no supervisado</strong>, pero vamos a dar un paso más, en el que veremos como usar Deep Learning en otro tipo de aprendizaje llamado <strong>aprendizaje por refuerzo</strong>.</p>
<p>El <strong>Aprendizaje por Refuerzo</strong> (<strong>RL</strong> por sus siglas en ingles, <strong>Reinforcement Learning</strong>) trata de conseguir que el sistema aprenda mediante recompensa/castigo, en función de si los pasos que da son buenos o malos. De esta manera, cuanta mayor recompensa se tenga es que nuestro sistema se ha acercado a la solución buena. Se trata de aprender mediante la interacción y la retroalimentación de lo que ocurra.</p>
<p>Partiremos de dos elementos clave <strong>agente</strong> (es el que aprende y toma decisiones), y el <strong>entorno</strong> (donde el agente aprende y decide que acciones tomar). Tendremos que el agente podrá realizar <strong>acciones</strong> que normalmente provocarán un cambio de <strong>estado</strong> y a la vez se tendrá una <strong>recompensa</strong> (positiva o negativa) en función de la acción tomada en el entorno en ese momento.</p>
<p>Es decir, nos encontraremos un agente que realizará una acción <span class="math inline">\(a_t\)</span> en el tiempo <span class="math inline">\(t\)</span>, esta acción afectará al entorno que estará en un estado <span class="math inline">\(S_t\)</span> y mediante esta acción cambiará a un estado <span class="math inline">\(S_{t+1}\)</span> y además dará una recompensa <span class="math inline">\(r_{t+1}\)</span> en función de los malo o bueno que haya sido este paso.</p>
<p>El agente volverá a examinar el nuevo estado del entorno <span class="math inline">\(S_{t+1}\)</span> y la nueva recompensa recibida <span class="math inline">\(r_{t+1}\)</span> y volverá a tomar la decisión de realizar una nueva acción <span class="math inline">\(a_{t+1}\)</span>.</p>
<div id="fig-rl_diagrama" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl_diagrama-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/rl-diagrama.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl_diagrama-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.26: Esquema Aprendizaje por Refuerzo - Fuente: Propia
</figcaption>
</figure>
</div>
</section>
<section id="formalismo-matemático" class="level3" data-number="1.9.2">
<h3 data-number="1.9.2" class="anchored" data-anchor-id="formalismo-matemático"><span class="header-section-number">1.9.2</span> Formalismo Matemático</h3>
<p>El formalismo matemático para el Aprendizaje por Refuerzo está basado en los <strong>Procesos de Decisión de Markov</strong> (MDP por sus siglas en ingles). (CS229 Lecture notes).</p>
<section id="propiedad-de-markov" class="level4" data-number="1.9.2.1">
<h4 data-number="1.9.2.1" class="anchored" data-anchor-id="propiedad-de-markov"><span class="header-section-number">1.9.2.1</span> Propiedad de Markov</h4>
<p>Si tenemos una secuencia de estados <span class="math inline">\(s_1, s_2, ..., s_t\)</span> y tenemos la probabilidad de pasar a otro estado <span class="math inline">\(s_{t+1}\)</span>, diremos que se cumple la <strong>Propiedad de Markov</strong> si el <strong>futuro</strong> es independiente del <strong>pasado</strong> y sólo se ve afectado por el <strong>presente</strong>, es decir:</p>
<p><span class="math display">\[
\mathbb P[S_{t+1}| S_t] = \mathbb P[ S_{t+1}| S_t, s_{t-1}, ... S_2, S_1]
\]</span></p>
<p>Tendremos una <strong>Matriz de Probabilidades de Transición</strong> a una matriz con las probabilidades de todos los posibles cambios de estado que se puedan producir, <span class="math display">\[\mathcal P_{ss'}=\mathbb P[S_{t+1}=s'|S_t = s]\]</span></p>
</section>
<section id="proceso-de-markov" class="level4" data-number="1.9.2.2">
<h4 data-number="1.9.2.2" class="anchored" data-anchor-id="proceso-de-markov"><span class="header-section-number">1.9.2.2</span> Proceso de Markov</h4>
<p>Así llamaremos <strong>Proceso de Markov</strong> a un proceso aleatorio sin memmoria, es decir, una secuencia de estados <span class="math inline">\(S_1, S_2, …\)</span> con la propiedad de Markov.</p>
<p>Un Proceso de Markov está formado por una dupla <span class="math inline">\(&lt;\mathcal S,\mathcal P&gt;\)</span>,:</p>
<ul>
<li><span class="math inline">\(\mathcal S\)</span> conjunto finito de Estados</li>
<li><span class="math inline">\(\mathcal P\)</span> matriz de probabilidades de transición</li>
</ul>
</section>
<section id="proceso-de-recompensa-de-markov" class="level4" data-number="1.9.2.3">
<h4 data-number="1.9.2.3" class="anchored" data-anchor-id="proceso-de-recompensa-de-markov"><span class="header-section-number">1.9.2.3</span> Proceso de Recompensa de Markov</h4>
<p>LLamaremos <strong>Proceso de Recompensa de Markov</strong> (<strong>MRP,</strong> por sus siglas en ingles) a una cuádrupla <span class="math inline">\(&lt;\mathcal S,\mathcal P,\mathcal R,\gamma&gt;\)</span>, formada por:</p>
<ul>
<li><span class="math inline">\(\mathcal S\)</span> conjunto finito de Estados</li>
<li><span class="math inline">\(\mathcal P\)</span> matriz de probabilidades de transición</li>
<li><span class="math inline">\(\mathcal R\)</span> Función de recompensa definida como: <span class="math inline">\(\mathcal R_s=E[R_{t+1}|S_t=s]\)</span>, donde <span class="math inline">\(R_{t+1}\)</span>es la recompensa obtenida de pasar al estado <span class="math inline">\(S_{t+1}\)</span> desde el estado <span class="math inline">\(S_t\)</span></li>
<li><span class="math inline">\(\gamma\)</span> Factor de descuento, con <span class="math inline">\(\gamma \in [0,1]\)</span></li>
</ul>
<p>En este contexto llamaremos <strong>Saldo (</strong><span class="math inline">\(G_t\)</span><strong>)</strong> a la suma de todas las recompensas conseguidas a partir del estado <span class="math inline">\(s_t\)</span> con el factor de descuento aplicado.</p>
<p><span class="math display">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^\infty\gamma^kR_{t+k+1}
\]</span></p>
<p>El hecho de usar <span class="math inline">\(\gamma\)</span> (<strong>factor descuento</strong>), nos permite dar grandes recompensas lo antes posible, y no dar tanto valor a futuras recompensas lejanas. También puede haber otras interpretaciones por ejemplo a nivel económico, si la recompensa está basado en un dato monetario real, tendría sentido que el dinero a futuro tendría menos valor. También nos permite asegurar que este valor de <span class="math inline">\(G_t\)</span> es finito ya que produce que la serie sea convergente.</p>
<p>Cuando los valores del factor descuento se acercan a <strong>0</strong> podríamos decir que nos fijamos sólo en los valores más cercanos de la recompensa. En cambio cuando los valores se acercan a <strong>1</strong> entonces les daremos más peso a los valores más lejanos de la recompensa.</p>
<p>Una vez definido el Saldo podemos definir la <strong>Función Valor de Estado</strong> como la función que nos da el <strong>Saldo Esperado</strong> comenzando por el estado <span class="math inline">\(s\)</span>. Es decir:</p>
<p><span class="math display">\[
V(s) = \mathbb E[G_t|S_t=s]
\]</span></p>
<p>Esta función nos dice cómo de bueno es partir de este estado y continuar.</p>
</section>
<section id="proceso-de-decisión-de-markov" class="level4" data-number="1.9.2.4">
<h4 data-number="1.9.2.4" class="anchored" data-anchor-id="proceso-de-decisión-de-markov"><span class="header-section-number">1.9.2.4</span> Proceso de Decisión de Markov</h4>
<p>Un <strong>Proceso de Decisión de Markov (MDP por sus siglas en inglés)</strong> es un tupla <span class="math inline">\(&lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R, \gamma &gt;\)</span> donde:</p>
<ul>
<li><p><span class="math inline">\(\mathcal S\)</span> es el conjunto de posibles <strong>estados</strong>.</p></li>
<li><p><span class="math inline">\(\mathcal A\)</span> es el conjunto de posibles <strong>acciones</strong>.</p></li>
<li><p><span class="math inline">\(\mathcal P\)</span> son las <strong>probabilidades de transición</strong> de un estado a otro en función de la acción realizada. Por cada estado y acción hay una distribución de probabilidad para pasar a otro estado.</p>
<p><span class="math display">\[
\mathcal P_{ss'}^a=\mathbb P[S_{t+1}=s'|S_t=s,A_t=a]\]</span></p></li>
<li><p><span class="math inline">\(\gamma\)</span> es el conocido como <strong>factor de descuento</strong> y tendrá un valor entre <span class="math inline">\([0,1)]\)</span> y nos proporciona cuanto descontamos en las recompensas a futuro.</p></li>
<li><p><span class="math inline">\(\mathcal R\)</span> es la <strong>Función de recompensa</strong> definida como: <span class="math inline">\(\mathcal R_s^a=E[R\_{t+1}|S_t=s, A_t=a]\)</span>, donde <span class="math inline">\(R\_{t+1}\)</span>es la recompensa obtenida de pasar al estado <span class="math inline">\(S_{t+1}\)</span> desde el estado</p></li>
</ul>
<p>Además tenemos que este <strong>proceso estocástico</strong> cumple la <strong>propiedad de Markov</strong> que dice que el futuro es independiente del pasado dado el presente. En términos de nuestro problema, podría decir que pasar de un estado <span class="math inline">\(s_t\)</span> al siguiente <span class="math inline">\(s_{t+1}\)</span> sólo depende de <span class="math inline">\(s_t\)</span> y no de los anteriores estados <span class="math display">\[
\mathbb P(s_{t+1}|s_t)= \mathbb P(s_{t+1}|s_1,s_2,...,s_t)
\]</span></p>
<p>Veamos cual es la <strong>dinámica</strong> de un MDP:</p>
<ul>
<li>Empezamos con un estado <span class="math inline">\(s_0 \in \mathcal S\)</span></li>
<li>Elegimos una acción <span class="math inline">\(a_0 \in \mathcal A\)</span> (la política será la que la elija)</li>
<li>Obtenemos una recompensa <span class="math inline">\(R_1 = R(s_0) = R(s_0, a_0)\)</span></li>
<li>Elegimos una acción <span class="math inline">\(a_1 \in \mathcal A\)</span>(la política será la que la elija)</li>
<li>Se transiciona aleatoriamente a un estado <span class="math inline">\(s_1\)</span> en un función del valor de <span class="math inline">\(P_{s_0s_1}^{a_1}\)</span></li>
<li>Obtenemos una recompensa <span class="math inline">\(R_2 = R(s_1) = R(s_1, a_1)\)</span></li>
<li>Se transiciona a aleatoriamente a un estado <span class="math inline">\(s_12\)</span> en un función del valor de <span class="math inline">\(P_{s_1s_2}^{a_2}\)</span></li>
<li>…</li>
<li>Repetimos de forma iterativa este proceso</li>
</ul>
<p>La meta en RL es elegir las <strong>acciones</strong> adecuadas en el tiempo para <strong>maximizar</strong>: <span class="math display">\[\mathbb E[G_t] =\mathbb E[R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \gamma^3 R(s_3) + ...]\]</span> Que es conocido como la <strong>hipotesis de la recompensa</strong>.</p>
<p>Vamos a introducir el término de <strong>política</strong> como una función <span class="math inline">\(\pi : \mathcal S \rightarrow \mathcal A\)</span> que mapea los estados a las acciones. Es decir, es la que decide que <strong>acción</strong> hay que <strong>ejecutar</strong> en función de <strong>cual</strong> es el <strong>estado</strong> en el que estamos. Una política podría ser determinística o estocástica. <span class="math display">\[
a = \pi(s) \\
a = \pi (a|s)=\mathbb P[A=a|S=s]
\]</span></p>
<p>Una política define cual va a ser el comportamiento de un <strong>agente</strong>. En un MDP las políticas dependen del estado actual, y no de la historia de los estados pasados.</p>
<p>Diremos que estamos <strong>ejecutando una política</strong> <span class="math inline">\(\pi\)</span> si cuando estamos en un estado <span class="math inline">\(s\)</span> aplicamos la acción <span class="math inline">\(a=\pi(s)\)</span></p>
<p>Definiremos:</p>
<p><span class="math display">\[
\mathcal P_{s,s`}^\pi = \sum_{a \in \mathcal A}\pi(a|s)\mathcal P_{s,s`}^a \\
\mathcal R_{s}^\pi = \sum_{a \in \mathcal A}\pi(a|s)\mathcal R_{s}^a
\]</span></p>
<p>También definiremos la <strong>Función Valor de Estado</strong> para una <strong>política</strong> <span class="math inline">\(\pi\)</span> a la función que nos predice la recompensa a futuro (el <strong>saldo esperado</strong>): <span class="math display">\[V^{\pi}(s)=\mathbb E_\pi[G_t|S_t=s]=
\mathbb E_\pi[R(s_t) + \gamma R(s_{t+1}) + \gamma^2 R(s_{t+2}) + \gamma^3 R(s_{t+3}) + ...|s_t=s]\]</span> Es decir, la esperanza de la suma de las recompensas con factor descuento suponiendo el comienzo en <span class="math inline">\(s_t=s\)</span> y tomando las acciones bajo la política <span class="math inline">\(\pi\)</span>. Nos permite decir cómo de buenos o malos son los estados.</p>
<p>Añadiremos el concepto de la <strong>Función Valor de Acción,</strong> también llamada <strong>Función de Calidad</strong> (por eso se usa la <span class="math inline">\(Q\)</span> (Quality)<strong>,</strong> para una <strong>política</strong> <span class="math inline">\(\pi\)</span> a la función que nos predice la recompensa a futuro (el saldo esperado), suponiendo que se se <strong>parte de una acción</strong> <span class="math inline">\(a\)</span>. <span class="math display">\[
Q^\pi(s,a)=\mathbb E_\pi[G_t|S_t=s,A_t=a]\\
=\mathbb E_\pi[R(s_t) + \gamma R(s_{t+1}) + \gamma^2 R(s_{t+2}) + \gamma^3 R(s_{t+3}) + ...|s_t=s,A_t=a]
\]</span> La función de <strong>Valor de Estado</strong> puede ser descompuesta en la <strong>recompensa inmediata</strong> y el resto de la recompensa: <span class="math display">\[
V^\pi(s) = \mathbb E_\pi[R_{t+1}+\gamma V^\pi(S_{t+1)}|S_t=s]
\]</span> y del mismo modo se puede descomponer la función <strong>Valor de Acción</strong>: <span class="math display">\[
Q^\pi(s,a) = \mathbb E_\pi[R_{t+1}+\gamma Q^\pi(S_{t+1}, A_{t+1})|S_t=s,A_t=a]
\]</span> Luego tenemos <span class="math display">\[
V^\pi(s) = \sum_{a\in A}\pi(a|s)Q^\pi(s,a)
\]</span></p>
<p>y <span class="math display">\[
Q^\pi(s,a) = R_s^a+\gamma \sum_{s'\in S} P_{ss'}^{a}V^\pi(s')
\]</span></p>
<p>Llegando a</p>
<p><span class="math display">\[
V^\pi(s) = \sum_{a\in A}\pi(a|s)(R_s^a+\gamma \sum_{s'\in S} P_{ss'}^{a}V^\pi(s'))
\]</span></p>
<p>y</p>
<p><span class="math display">\[
Q^\pi(s,a) = R_s^a+\gamma \sum_{s'\in S} P_{ss'}^{a}\sum_{a \in \mathcal A} \pi (a|s)Q^\pi(s',a)
\]</span></p>
<p>Dada una política <span class="math inline">\(\pi\)</span> su <strong>función valor de estado</strong> asociada <span class="math inline">\(V^{\pi}(s)\)</span> cumple la <strong>Ecuación de Bellman</strong>: <span class="math display">\[V^{\pi}(s)=R_s+ + \gamma\sum_{s'\in S}P_{s,\pi(s)}(s')V^{\pi}(s')\]</span> Lo que nos dice que la <strong>función valor</strong> está separada en <strong>dos términos</strong>:</p>
<ul>
<li>La recompensa inmediata <span class="math inline">\(R(s)\)</span></li>
<li>La suma de recompensas a futuro con el factor de descuento.</li>
</ul>
<p>Igualmente su <strong>función valor de acción asociada</strong> <span class="math inline">\(Q^\pi(s,a)\)</span>cumple la <strong>Ecuación de Bellman</strong>:</p>
<p><span class="math display">\[
Q^\pi(s,a) = R_s^a+\gamma \sum_{s'\in S} P_{ss'}^{a}\sum_{a \in \mathcal A} \pi (a|s)Q^\pi(s',a)
\]</span></p>
<p>Las <strong>Ecuaciones de Bellman</strong> permiten garantizar una <strong>solución óptima</strong> del problema de forma que dada una <strong>política óptima</strong> (<span class="math inline">\(\pi^*\)</span>), además se cumple:</p>
<p><span class="math display">\[
V^{\pi^*}(s)=V^*(s)=max_\pi V^\pi(s)\\
Q^{\pi^*}(s,a)=Q^*(s,a)=max_\pi Q^\pi(s,a)
\]</span></p>
<p>Es decir, que las funciones de valor de estado y de acción óptimas son las mismas que se general con la <strong>política óptima</strong>.</p>
<p>Como la meta del RL es encontrar una <strong>política óptima</strong> <span class="math inline">\(\pi^*\)</span> la cual maximize el valor del <strong>saldo esperado total (desde el inicio)</strong> <span class="math inline">\(G_0=\sum_{t=0}^\infty\)</span>, es decir, podríamos definir la política óptima como:</p>
<span class="math display">\[\begin{equation}
\pi^*(a|s)= \left\lbrace
\begin{array}{ll}
1 \text{ si } a=\mathop{\mathrm{argmax}}\limits_{a \in \mathcal A} Q^* (s,a) \\
0 \text{ si cualqier otro caso}
\end{array}
\right.
\end{equation}\]</span>
<p>Luego si conocemos <span class="math inline">\(Q^*(s,a)\)</span> inmediatamente tenemos una <strong>política óptima.</strong></p>
</section>
<section id="resolución-de-las-ecuaciones-de-bellman" class="level4" data-number="1.9.2.5">
<h4 data-number="1.9.2.5" class="anchored" data-anchor-id="resolución-de-las-ecuaciones-de-bellman"><span class="header-section-number">1.9.2.5</span> Resolución de las Ecuaciones de Bellman</h4>
<p>Las ecuaciones de Bellman pueden ser usadas para resolver de forma eficiente <span class="math inline">\(V^\pi\)</span>, especialmente en un <strong>MDP</strong> de un número finito de estado, escribiendo una ecuación <span class="math inline">\(V^\pi (s)\)</span> por cada estado.</p>
<p>La mayoría de los algoritmos de RL usan las Ecuaciones de Bellman para resolver el problema. La forma básica de resolverlo es usando <strong>progración dinámica</strong> (PD por sus siglas en inglés), aunque nos encontramos con muchos problemas para resolverla cuando el número de acciones/estados aumenta. También se usan otras técnicas como los <strong>métodos de montecarlo</strong> (MMC, por sus siglas en inglés) o los métodos de <strong>diferencia temporal</strong> (TD, por sus siglas en ingles).</p>
<p>Pasemos a ver una clasificación de los tipos de algoritmos para resolver los problemas de RL.</p>
</section>
</section>
<section id="taxonomía-de-algoritmos" class="level3" data-number="1.9.3">
<h3 data-number="1.9.3" class="anchored" data-anchor-id="taxonomía-de-algoritmos"><span class="header-section-number">1.9.3</span> Taxonomía de Algoritmos</h3>
<p>Desde OpenAI (<a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#citations-below" class="uri">https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#citations-below</a>) obtenemos la siguiente taxonomía de algoritmos de RL que nos servirá como guía para entender como clasificar los algoritmos:</p>
<div id="fig-rl_algorithms" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl_algorithms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/rl_algorithms.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl_algorithms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.27: Modelos Reinforcement Learning
</figcaption>
</figure>
</div>
<p>La primera gran separación se hace sobre si los algoritmos siguen un modelo definido (model-baed) o no (modelo-free).</p>
<p><strong>Model-free</strong></p>
<p>Por otro lado los <strong>model-free</strong> usan la experiencia para aprender o una o ambas de dos cantidades más simples (valores estado/acción o políticas).</p>
<p>Las aproximaciones de estos algorimtos son de tres tipos:</p>
<ul>
<li><strong>Policy Optimization</strong></li>
</ul>
<p>El agente aprende directamente la función política que mapea el estado a una acción. Nos podemos encontrar con dos tipos de políticas, las <strong>políticas deterministicas</strong> (no hay incertidumbre en el mapeo) y las <strong>políticas estocásticas</strong> (tenemos una distribución de probabilidad en las acciones)<strong>.</strong> En este último caso diremos que tenemos un Proceso de Decisión de Markov Parcialmente Observable (POMDP, por sus siglas en ingles).</p>
<ul>
<li><strong>Q-Learning</strong></li>
</ul>
<p>En este caso el agente aprende una función valor de acción <span class="math inline">\(Q(s,a)\)</span> que nos dirá cómo de bueno es tomar una acción dependiendo del estado.</p>
<ul>
<li><strong>Híbridos</strong></li>
</ul>
<p>Estos métodos combinan la fortaleza de los dos métodos anteriores, aprendiendo tanto la función política como la función valor de acción.</p>
<p><strong>Model-based</strong></p>
<p>Los algoritmos <strong>model-based</strong> usan la experiencia para construir un modelo interno de transiciones y resultados inmediatos en el entorno. Las acciones son elegidas mediante búsqueda o planificación en este modelo construido.</p>
<p>Las aproximaciones de estos algorimtos son de dos tipos:</p>
<ul>
<li>Aprender el Modelo</li>
</ul>
<p>Para aprender el modelo se ejecuta una política base,</p>
<ul>
<li>Aprender dado el Modelo</li>
</ul>
<p>Nos centraremos en los algoritmos de tipo <strong>Model-Free</strong> que son los más utilizados ya que no requieren del modelo. Si se quieren profundizar en los diferentes algoritmos, se puede consultar las documentación en: <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#links-to-algorithms-in-taxonomy" class="uri">https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#links-to-algorithms-in-taxonomy</a>.</p>
<p>Vamos a ver 2 de los algoritmos de tipo <strong>Model-free</strong> que nos van a permitir el ver el paso de un algoritmo sin <strong>Deep Learning</strong> y otro en el que se aplica <strong>Deep Learning</strong> para obtener el objetivo final de tener un <strong>agente</strong> capaz de aprender por sí solo a realizar las tareas específicas que se tengan que realizar.</p>
</section>
<section id="q-learning-value" class="level3" data-number="1.9.4">
<h3 data-number="1.9.4" class="anchored" data-anchor-id="q-learning-value"><span class="header-section-number">1.9.4</span> Q-Learning (value)</h3>
<p>Q-Learning es un método basado en valor y que usa el <strong>sistema TD</strong> (actualización su función valor en cada paso) para el entrenamiento y su función de valor de estado.</p>
<p>El nombre de <strong>Q</strong> viende de <strong>Quality</strong> (calidad), por que nos da la calidad de la acción en un determinado estado. Lo que tenemos es que vamos a tener una <strong>función de valor de acción (Q-función)</strong> que nos da un valor numérico de cómo de buena es a partir de un estado <strong>s</strong> y una acción <strong>a</strong>.</p>
<p>En este caso tenemos que internamente nuestra <strong>Q-función (</strong><span class="math inline">\(Q(s,a)\)</span><strong>)</strong> es una <strong>Q-tabla</strong>, de forma que cada fila corresponde a un estado, y cada columna a una de las posibles acciones.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imagenes/capitulo1/rl_qlearning.png" class="img-fluid figure-img"></p>
<figcaption>Q-Learning - Fuente: https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python</figcaption>
</figure>
</div>
<p>Es decir, esta tabla va a contener la información de <strong>recompensa total esperada</strong> para cada valor de estado y acción. Cuando nosotros realizamos el <strong>entrenamiento</strong> de la Q-función, nosotros conseguimos una función que <strong>optimice</strong> esta <strong>Q-tabla</strong>.</p>
<p>Si nosotros tenemos una Q-función óptima (<span class="math inline">\(Q^*(s,a)\)</span>), entonces podremos obtener la <strong>política óptima</strong> a partir de ella:</p>
<p><span class="math display">\[
\pi^*(s) = \mathop{\mathrm{argmax}}\limits_{a \in \mathcal A}Q^*(s,a)
\]</span> Veamos cuales serían los pasos que deberíamos dar:</p>
<p><strong>Inicializamos</strong> nuestra <strong>Q-Tabla</strong> con valores a 0. Conforme avanece nuestro entrenamiento estos valores irán cambiando en función de los datos que se obtengan al <strong>porbar</strong> a realizar <strong>acciones</strong> y obtener las <strong>recompensas</strong> correspondientes.</p>
<p>El siguiente elemento que necesitamos es una <strong>política de entrenamiento</strong> (función que nos permita elegir que acción tomar en función del estado en el que estemos), en este caso nuestra política estará basada en los valores de la <strong>Q-tabla</strong>, es lo que llamaremos <strong>explotación</strong> (explotamos la información que tenemos cogiendo la acción con mejor valor Q) o elegiremos otra acción, es lo que llamaremos <strong>exploración</strong> (exploramos nuevos caminos cogiendo una acción de forma aleatoria).</p>
<p>Esto es lo que se llama una política <span class="math inline">\(\epsilon\)</span>-greedy, ya que se usa un parámetro <span class="math inline">\(\epsilon\)</span>, valor entre 0 y 1, que nos permite decidir si elegimos <strong>explorar</strong> o si queremos <strong>explotar</strong> los datos que ya tenemos.</p>
<p>XXXXX Imagen del gráfico epsilon (epsilon respecto al número de epsisodios)</p>
<p>Tendremos que:</p>
<ul>
<li>con probabilidad 1-<span class="math inline">\(\epsilon\)</span> nosotros haremos <strong>explotación</strong> y</li>
<li>con probabilidad <span class="math inline">\(\epsilon\)</span> nosotros haremos <strong>exploración</strong>.</li>
</ul>
<p>Es decir, inicialmente le damos valor 1 a <span class="math inline">\(\epsilon\)</span> de forma que empezaremos haciendo <strong>exploración</strong> e iremos bajando este valor de epsilon conforme avance el entrenamiento para que cada vez usemos más la <strong>explotación</strong>.</p>
<p>La idea base es que al principio del entrenamiento, lo prioritario es <strong>explorar</strong>, es decir, seleccionar una acción al azar y obtener su recompensa, ya que nuestra <strong>Q-Tabla</strong> está inicializada a 0. Conforme avance el entrenamiento nos tendremos que ir fiando más de los datos que ya tenemos y tendrá que primar la <strong>explotación</strong> de nuestros datos de la <strong>Q-Tabla</strong>. Para hacer ésto de una forma efectiva, usaremos un parámetro <strong>decay_epsion</strong> que conforme avancemos en entrenamiento se encargará de ir reduciendo el valor de <span class="math inline">\(\epsilon\)</span> para conseguir este efecto.</p>
<p>Una vez que tenemos nuestros elementos base, pasaremos al <strong>entrenamiento</strong>, de forma que para todos los <strong>episodios</strong> (iteraciones de partidas) que definamos haremos lo siguiente:</p>
<ul>
<li>Partimos de un <strong>estado inicial</strong>, y obtenemos una <strong>acción</strong> a partir de nuestra <strong>política de entrenamiento</strong></li>
<li>Actualizmos <span class="math inline">\(\epsilon\)</span> con el nuevo valor en este episodio</li>
<li>Iteramos para un número máximo de pasos dentro de este episodio</li>
<li>Obtenemos el nuevo estado, así como la recompensa obtenida</li>
<li>Actulizamos el valor de la <strong>Q-Tabla</strong> correspondiente según la fórmula basada en los métodos de <strong>TD</strong> (Diferencias temporales) <span class="math display">\[
Q(s,a) = Q(s,a) + \alpha(R(s,a)+\gamma argmax_aQ(s',a) - Q(s,a))\\
\text{donde }s\text{ es el estado actual y }s'\text{ es el nuevo estado}
\]</span></li>
<li>Verificamos si se ha llegado al final del juego para salir de este espisodio si es el caso</li>
<li>Cambiamos el <strong>estado</strong> como el <strong>nuevo estado</strong></li>
</ul>
<p>Una vez acabemos nuestro entrenamiento, obtendremos nuestra <strong>política óptima</strong> como: <span class="math display">\[
\pi^*(s) = \mathop{\mathrm{argmax}}\limits_{a \in \mathcal A}Q^*(s,a)
\]</span></p>
<p><strong>Pseudo código Q-Learning</strong></p>
<div id="fig-rl_algoritmo_qlearning" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl_algoritmo_qlearning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/rl_algoritmo_qlearning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl_algoritmo_qlearning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.28: Algoritmo Q-Learning - Fuente: Propia
</figcaption>
</figure>
</div>
</section>
<section id="dqn-deep-q-learning" class="level3" data-number="1.9.5">
<h3 data-number="1.9.5" class="anchored" data-anchor-id="dqn-deep-q-learning"><span class="header-section-number">1.9.5</span> DQN (Deep Q-Learning)</h3>
<p><a href="https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/" class="uri">https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/</a></p>
<p>Hemos visto el algoritmo de <strong>Q-Learning</strong> en el que usábamos una <strong>Q-Tabla</strong>, es decir una tabla donde guardábamos todos los valores de la función <span class="math inline">\(Q(s,a)\)</span> y que entrenando el agente, éramos capaces de conseguir aproximar a la función <strong>Q óptima</strong>, con lo cual teníamos una <strong>Política Óptima</strong>.</p>
<p>Este tipo de algoritmos son válidos cuando nos encontramos con un número “limitado” de estados y acciones, de forma que la tabla es relativamente manejable y somos capaces de entrenarla. Si nos encontramos ante un problema en el que tenemos miles o cientos de miles de estados no va a ser efectivo construir una tabla y entrenarla para todas las posibles combinaciones <strong>etado-acción</strong>. Para abordar este tipo de problemas, la mejor solución es buscar un <strong>aproximador</strong> de la función <span class="math inline">\(Q(s,a)\)</span>, que nos permita obtener la mejor solución sin necesidad de entrenar todas las posibles combinaciones.</p>
<p>Para realizar este trabajo una de las posibles opciones es usar <strong>redes neuronales</strong> como función aproximadora y que nos abrirá la posibilidad de trabajar con problemas en los que existan grandes cantidades de estados/acciones.</p>
<p>Fue el equipo de <strong>Deepmind</strong> en 2013 (<a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" class="uri">https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf</a>), en su artítulo <strong>“Human-level control through deep reinforcementlearning”</strong>, los primeros que decidieron atacar los problemas de alta dimensionalidad de <strong>estados/acciones</strong> mediante el uso de <strong>Redes Neuronales Profundas</strong>. La forma de probar su código fue mediante la implementación de <strong>agentes</strong> que fueran capaces de aprender a jugar a los clásicos <strong>juegos de Atari 2600</strong>. De forma que el agente, recibiendo la información de entrada de los pixels que hay en cada momento en pantalla y el marcador del juego, eran capaces de sobrepasar el rendimiento de algoritmos actuales que hacían ese trabajo. En estte caso usaron la misma red neuronal, con la misma arquitectura e hiperparámetros para los 49 juegos con los que se probaron.</p>
<div id="fig-rl_dqlearning" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl_dqlearning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/rl_dqlearning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl_dqlearning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.29: Deep Q-Learning - Fuente: https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/
</figcaption>
</figure>
</div>
<p>Con nuestro algoritmo de <strong>Q-Learning</strong> teníamos una función <span class="math inline">\(Q(s,a)\)</span> que implementábamos con un tabla y nos daba para cada <strong>estado</strong> y cada <strong>acción</strong> cual era el valor de <strong>Q</strong> (Quality) de la recompensa esperada. Ahora, con <strong>Deep Q-Learning</strong> nos encontramos que vamos a tener una red neuronal que será la encargada de para cada <strong>estado</strong> obtener el valor de <strong>Q</strong> para cada posible <strong>acción</strong>.</p>
<p><strong>DQN (Deep Q-Network) Arquitectura</strong></p>
<p>Para poder implementar nuestro trabajo con redes neuronales nos vamos a encontrar con el problema de entrenar la red neuronal (obtener los pesos) que permitan alcancar nuestra función <strong>Q-Óptima</strong> que nos daría la <strong>Política Òptima</strong> que es lo que realmente buscamos.</p>
<p>Básicamente para realizar el trabajo usaremos 2 redes neuronales que tendrán la misma arquitectura de forma que el entrenamiento sea estable.</p>
<ul>
<li><strong>DQN</strong> que será la red de predicción, y que será la que entrenaremos para minimizar el valor del error <span class="math inline">\((R+\gamma argmax_{a'}Q(s',a',w')-Q(s,a,w))^2\)</span></li>
<li><strong>DQN_Target</strong> que será la red que calculará <span class="math inline">\(R+\gamma argmax_a'Q(s',a',w')\)</span></li>
</ul>
<div id="fig-rl_rl_arquitectura_redes" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl_rl_arquitectura_redes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/rl_arquitectura_redes.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl_rl_arquitectura_redes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.30: Arquitectura Redes DQN - Fuente: https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/
</figcaption>
</figure>
</div>
<p><strong>Experience Replay</strong></p>
<p>El mecanismo del <strong>Experience Replay</strong> nos va a permitir entrenar nuestra red <strong>DQN</strong> con minibatchs que vamos a extraer de forma aleatoria de la memoria en la que vamos a ir guardando los resultados que vamos obteniendo <strong>&lt;s,a,r,s’&gt;</strong>.</p>
<p>Ésto nos va a permitir por un lado <strong>entrenar</strong> nuestra <strong>red de predicción</strong> y además va a servirnos para evitar <strong>correlaciones</strong> de secuencias consecutivas que pudieran producir un sesgo en nuestros resultados. De esta manera, al elegir al azar los elementos que vamos a usar para entrenar la red, no tendrán ninguna relación con los datos consecutivos que se van produciendo en los pasos de los episodios.</p>
<p><strong>Algoritmo Deep Q-Learning</strong></p>
<ul>
<li>Obtenemos los datos de entrada, que es el estado.</li>
<li>Seleccionamos la acción usando nuestra política de entrenamiento epsilon-greedy</li>
<li>Ejecutamos la acción y obtenemos el siguiente estado así como la recompensa obtenida</li>
<li>Almacenamos en memoria &lt;s,a,r,s’&gt;</li>
<li>Si tenemos bastantes elementos en la memoria
<ul>
<li>Hacemos un minibatch aleatorio y enteramos la red siendo <span class="math inline">\(R+\gamma argmax_{a'}Q(s',a',w')\)</span> el <strong>target de la red</strong> y <span class="math inline">\(Q(s,a,w)\)</span> el valor predicho.</li>
<li>La función de pérdida será la de Diferencia de Cuadrados <span class="math inline">\(L = (R+\gamma argmax_a'Q(s',a',w')-Q(s,a,w))^2\)</span></li>
</ul></li>
<li>Después de cada C iteraciones, copiaremos los pesos de la red DQN a la DQN_Target</li>
<li>Repetiremos estos pasos durante M episodios</li>
</ul>
<p><strong>Pseudo-código Deep Q-Learning</strong></p>
<div id="fig-rl_algoritmo_dqn" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl_algoritmo_dqn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="imagenes/capitulo1/rl_algoritmo_dqn.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl_algoritmo_dqn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.31: Algoritmo Deep Q-Learning - Fuente: Propia
</figcaption>
</figure>
</div>
<p><strong>Variantes de Deep Q-Learning</strong></p>
<ul>
<li>Double Deep Q Network (DDQN) – 2015</li>
<li>Deep Recurrent Q Network (DRQN) – 2015</li>
<li>Dueling Q Network – 2015</li>
<li>Persistent Advantage Learning (PAL) – 2015</li>
<li>Bootstrapped Deep Q Network – 2016</li>
<li>Normalized Advantage Functions (NAF) = Continuous DQN – 2016</li>
<li>N-Step Q Learning – 2016</li>
<li>Noisy Deep Q Network (NoisyNet DQN) – 2017</li>
<li>Deep Q Learning for Demonstration (DqfD) – 2017</li>
<li>Categorical Deep Q Network = Distributed Deep Q Network = C51 – 2017
<ul>
<li>Rainbow – 2017</li>
</ul></li>
<li>Quantile Regression Deep Q Network (QR-DQN) – 2017</li>
<li>Implicit Quantile Network – 2018</li>
</ul>
</section>
<section id="listado-algoritmos" class="level3" data-number="1.9.6">
<h3 data-number="1.9.6" class="anchored" data-anchor-id="listado-algoritmos"><span class="header-section-number">1.9.6</span> Listado Algoritmos</h3>
<p><strong>1. Model-Free</strong></p>
<p><strong>Value-based</strong></p>
<p><a href="https://link.springer.com/content/pdf/10.1007/BF00992698.pdf">Q-learning = SARSA max</a> – 1992</p>
<p><a href="http://mi.eng.cam.ac.uk/reports/svr-ftp/auto-pdf/rummery_tr166.pdf">State Action Reward State-Action (SARSA)</a>– 1994</p>
<p><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Deep Q Network (DQN)</a> – 2013</p>
<p><a href="https://arxiv.org/pdf/1509.06461.pdf">Double Deep Q Network (DDQN)</a> – 2015</p>
<p><a href="https://arxiv.org/abs/1507.06527">Deep Recurrent Q Network (DRQN)</a> – 2015</p>
<p><a href="https://arxiv.org/abs/1511.06581">Dueling Q Network</a> – 2015</p>
<p><a href="https://arxiv.org/abs/1512.04860">Persistent Advantage Learning (PAL)</a> – 2015</p>
<p><a href="https://arxiv.org/abs/1602.04621">Bootstrapped Deep Q Network</a> – 2016</p>
<p><a href="https://arxiv.org/abs/1603.00748">Normalized Advantage Functions (NAF) = Continuous DQN</a> – 2016</p>
<p><a href="https://arxiv.org/abs/1602.01783">N-Step Q Learning</a> – 2016</p>
<p><a href="https://arxiv.org/abs/1706.10295">Noisy Deep Q Network (NoisyNet DQN)</a> – 2017</p>
<p><a href="https://arxiv.org/abs/1704.03732">Deep Q Learning for Demonstration (DqfD)</a> – 2017</p>
<p><a href="https://arxiv.org/abs/1707.06887">Categorical Deep Q Network = Distributed Deep Q Network = C51</a> – 2017</p>
<ul>
<li><a href="https://arxiv.org/abs/1710.02298">Rainbow</a> – 2017</li>
</ul>
<p><a href="https://arxiv.org/pdf/1710.10044v1.pdf">Quantile Regression Deep Q Network (QR-DQN)</a> – 2017</p>
<p><a href="https://arxiv.org/abs/1806.06923">Implicit Quantile Network</a>– 2018</p>
<p><a href="https://arxiv.org/abs/1703.01310">Mixed Monte Carlo (MMC)</a> – 2017</p>
<p><a href="https://arxiv.org/abs/1703.01988">Neural Episodic Control (NEC)</a> – 2017</p>
<p><strong>Policy-based</strong></p>
<p><a href="https://link.springer.com/article/10.1023/A:1010091220143">Cross-Entropy Method (CEM)</a>– 1999</p>
<p>Policy Gradient</p>
<ul>
<li><a href="https://people.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf">REINFORCE = Vanilla Policy Gradient</a>(VPG)- 1992</li>
<li>Policy gradient softmax</li>
<li><a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">Natural Policy Gradient (Optimisation) (NPG) / (NPO)</a> – 2002</li>
<li><a href="https://arxiv.org/abs/1604.06778">Truncated Natural Policy Gradient (TNPG)</a> – 2016</li>
</ul>
<p><strong>Actor-Critic</strong></p>
<p><a href="https://arxiv.org/abs/1602.01783">Advantage Actor Critic (A2C)</a> – 2016</p>
<p><a href="https://arxiv.org/abs/1602.01783">Asynchronous Advantage Actor-Critic (A3C)</a>&nbsp; – 2016</p>
<p><a href="https://arxiv.org/abs/1506.02438">Generalized Advantage Estimation (GAE)</a> – 2015</p>
<p><a href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization (TRPO)</a> – 2015</p>
<p><a href="http://proceedings.mlr.press/v32/silver14.pdf">Deterministic Policy Gradient (DPG)</a> – 2014</p>
<p><a href="https://arxiv.org/abs/1509.02971">Deep Deterministic Policy Gradients (DDPG)</a>&nbsp; – 2015</p>
<ul>
<li><a href="https://arxiv.org/abs/1804.08617">Distributed Distributional Deterministic Policy Gradients (D4PG)</a> – 2018</li>
<li><a href="https://arxiv.org/pdf/1802.09477.pdf">Twin Delayed Deep Deterministic Policy Gradient (TD3)</a> – 2018</li>
</ul>
<p><a href="https://arxiv.org/abs/1611.01224">Actor-Critic with Experience Replay (ACER)</a> – 2016</p>
<p><a href="https://arxiv.org/abs/1708.05144">Actor Critic using Kronecker-Factored Trust Region (ACKTR)</a> – 2017</p>
<p><a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization (PPO)&nbsp;</a>– 2017</p>
<ul>
<li><a href="https://arxiv.org/abs/1707.02286">Distributed PPO (DPPO)</a> – 2017</li>
<li><a href="https://arxiv.org/pdf/1707.06347.pdf">Clipped PPO (CPPO)</a>&nbsp; – 2017</li>
<li><a href="https://arxiv.org/abs/1911.00357">Decentralized Distributed PPO (DD-PPO)</a>– 2019</li>
</ul>
<p><a href="https://arxiv.org/abs/1801.01290">Soft Actor-Critic (SAC)</a>&nbsp; – 2018</p>
<p><strong>General Agents</strong></p>
<ul>
<li><a href="https://ieeexplore.ieee.org/document/542381">Covariance Matrix Adaptation Evolution Strategy (CMA-ES)</a>– 1996</li>
<li><a href="https://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf">Episodic Reward-Weighted Regression (ERWR)</a> – 2009</li>
<li><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1851/2264">Relative Entropy Policy Search (REPS)</a>– 2010</li>
<li><a href="https://arxiv.org/abs/1611.01779">Direct Future Prediction (DFP)</a> – 2016</li>
</ul>
<p><strong>Imitation Learning Agents</strong></p>
<p>Behavioral Cloning (BC)</p>
<p><a href="https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf">Dataset Aggregation (Dagger)</a> (i.e.&nbsp;query the expert) – 2011</p>
<p>Adversarial Reinforcement Learning</p>
<ul>
<li><a href="https://arxiv.org/abs/1606.03476">Generative Adversarial Imitation Learning (GAIL)</a> – 2016</li>
<li><a href="https://arxiv.org/abs/1710.11248">Adverserial Inverse Reinforcement Learning (AIRL)</a>– 2017</li>
</ul>
<p><a href="https://arxiv.org/abs/1710.02410">Conditional Imitation Learning</a> – 2017</p>
<p><a href="https://arxiv.org/abs/1905.11108">Soft Q-Imitation Learning (SQIL)</a> – 2019</p>
<p><strong>Hierarchical Reinforcement Learning Agents</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1712.00948.pdf">Hierarchical Actor Critic (HAC)</a> – 2017</li>
</ul>
<p><strong>Memory Types</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay (PER)</a> – 2015</li>
<li><a href="https://arxiv.org/abs/1707.01495.pdf">Hindsight Experience Replay (HER)</a> – 2017</li>
</ul>
<p><strong>Exploration Techniques</strong></p>
<ul>
<li>E-Greedy</li>
<li>Boltzmann</li>
<li>Ornstein–Uhlenbeck process</li>
<li>Normal Noise</li>
<li>Truncated Normal Noise</li>
<li><a href="https://arxiv.org/abs/1602.04621">Bootstrapped Deep Q Network</a>&nbsp;</li>
<li><a href="https://arxiv.org/abs/1706.01502">UCB Exploration via Q-Ensembles (UCB)</a>&nbsp;</li>
<li><a href="https://arxiv.org/abs/1706.10295">Noisy Networks for Exploration</a>&nbsp;</li>
<li><a href="https://pathak22.github.io/noreward-rl/">Intrinsic Curiosity Module (ICM)</a> – 2017</li>
</ul>
<p><strong>Meta Learning</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1703.03400">Model-agnostic meta-learning (MAML)</a>– 2017</li>
<li><a href="https://openreview.net/pdf?id=S1evHerYPr">Improving Generalization in Meta Reinforcement Learning using Learned Objectives</a> (MetaGenRLis) – 2020</li>
</ul>
<p><strong>2. Model-Based</strong></p>
<p><strong>Dyna-Style Algorithms / Model-based data generation</strong></p>
<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.7362&amp;rep=rep1&amp;type=pdf">Dynamic Programming (DP) = DYNA-Q</a> – 1990</li>
<li><a href="https://arxiv.org/abs/1506.07365">Embed to Control (E2C)</a>– 2015</li>
<li><a href="https://arxiv.org/abs/1802.10592">Model-Ensemble Trust-Region Policy Optimization (ME-TRPO)</a> – 2018</li>
<li><a href="https://arxiv.org/abs/1807.03858">Stochastic Lower Bound Optimization (SLBO)</a> – 2018</li>
<li><a href="https://arxiv.org/abs/1809.05214">Model-Based Meta-Policy-Optimzation (MB-MPO)</a> (meta learning) – 2018</li>
<li><a href="https://arxiv.org/abs/1803.00101">Stochastic Ensemble Value Expansion (STEVE)</a> – 2018</li>
<li><a href="https://arxiv.org/abs/1803.00101">Model-based Value Expansion (MVE)</a> – 2018</li>
<li><a href="https://arxiv.org/abs/1903.00374">Simulated Policy Learning (SimPLe)</a> – 2019</li>
<li><a href="https://arxiv.org/abs/1906.08253">Model Based Policy Optimization (MBPO)</a> – 2019</li>
</ul>
<p><strong>Policy Search with Backpropagation through Time / Analytic gradient computation</strong></p>
<ul>
<li><a href="https://www.jstor.org/stable/3613752?origin=crossref&amp;seq=1">Differential Dynamic Programming (DDP)</a> – 1970</li>
<li><a href="http://users.cecs.anu.edu.au/~john/papers/BOOK/B03.PDF">Linear Dynamical Systems and Quadratic Cost (LQR)</a> – 1989</li>
<li><a href="https://homes.cs.washington.edu/~todorov/papers/LiICINCO04.pdf">Iterative Linear Quadratic Regulator (ILQR)</a> – 2004</li>
<li><a href="https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Deisenroth_ICML_2011.pdf">Probabilistic Inference for Learning Control (PILCO)</a> – 2011</li>
<li><a href="https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf">Iterative Linear Quadratic-Gaussian (iLQG)</a> – 2012</li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.716.4271&amp;rep=rep1&amp;type=pdf">Approximate iterative LQR with Gaussian Processes (AGP-iLQR)</a> – 2014</li>
<li><a href="https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf">Guided Policy Search (GPS)</a> – 2013</li>
<li><a href="https://arxiv.org/abs/1510.09142">Stochastic Value Gradients (SVG)</a> – 2015</li>
<li><a href="https://dl.acm.org/doi/10.5555/3306127.3331874">Policy search with Gaussian Process</a> – 2019</li>
</ul>
<p><strong>Shooting Algorithms / sampling-based planning</strong></p>
<p><a href="https://arxiv.org/pdf/1708.02596.pdf">Random Shooting (RS)</a> – 2017</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/B9780444538598000035">Cross-Entropy Method (CEM)</a>– 2013</p>
<ul>
<li><a href="https://arxiv.org/abs/1811.04551">Deep Planning Network (DPN)</a>-2018</li>
<li><a href="https://arxiv.org/abs/1805.12114">Probabilistic Ensembles with Trajectory Sampling (PETS-RS and PETS-CEM)</a> – 2018</li>
<li><a href="https://arxiv.org/abs/1610.00696">Visual Foresight</a> – 2016</li>
</ul>
<p><a href="https://arxiv.org/abs/1509.01149">Model Predictive Path Integral (MPPI)</a> – 2015</p>
<ul>
<li><a href="https://arxiv.org/abs/1909.11652">Planning with Deep Dynamics Models (PDDM)</a> – 2019</li>
</ul>
<p><a href="https://hal.inria.fr/inria-00116992/document">Monte-Carlo Tree Search (MCTS)</a> – 2006</p>
<ul>
<li><a href="https://arxiv.org/abs/1712.01815">AlphaZero</a> – 2017</li>
</ul>
<p>&nbsp;</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Introducción">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introducción</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./capitulo2.html" class="pagination-link" aria-label="Modelos Gráficos Probabilísticos y Análisis Causal">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Modelos Gráficos Probabilísticos y Análisis Causal</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>