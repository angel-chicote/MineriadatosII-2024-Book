[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Módulo 8. Minería de Datos II",
    "section": "",
    "text": "Introducción\n\n\n\n\n\n\nFigure 1: Diagrama Conceptual",
    "crumbs": [
      "Introducción"
    ]
  },
  {
    "objectID": "capitulo1.html",
    "href": "capitulo1.html",
    "title": "1  Deep Learning",
    "section": "",
    "text": "1.1 Introducción\nEl deep learning, también conocido como aprendizaje profundo, es una disciplina que busca emular el funcionamiento del cerebro mediante el uso de hardware y software, generando inteligencia artificial. Este enfoque se materializa en redes neuronales artificiales (RNA), que emplean una abstracción jerárquica para representar datos en múltiples niveles. El proceso implica la utilización de arquitecturas de varias capas, donde cada una aprende patrones más complejos, favoreciendo el aprendizaje útil. Generalmente, se emplea aprendizaje no supervisado para guiar el entrenamiento de las capas intermedias. Aunque derivado del machine learning, el deep learning se distingue por su arquitectura en capas, inCluyendo redes convolucionales y recurrentes, en contraste con métodos más simples como el Perceptrón Multicapa de una sola capa. Su avance se vio inicialmente obstaculizado por problemas de estancamiento en mínimos locales, resueltos mediante preentrenamiento no supervisado de las capas. Este enfoque ha impulsado un rápido crecimiento en el desarrollo de arquitecturas y algoritmos de RNA en los últimos años, manteniendo la esencia del aprendizaje jerárquico y profundo.\nLas redes neuronales tienen una amplia gama de aplicaciones en diversos campos, desde la clasificación y regresión de datos hasta la identificación de imágenes, texto y audio.\nEn la identificación de imágenes, por ejemplo, pueden reconocer animales, señales de tráfico, frutas, caras humanas e incluso tumores malignos en radiografías. A medida que se combinan estas capacidades, se pueden abordar problemas más complejos como la detección de objetos y personas en imágenes o el etiquetado de escenas. Con el análisis de videos, las redes neuronales pueden contar personas, reconocer objetos y señales de tráfico, o detectar comportamientos como llevar un arma.\nCuando se trata de datos de texto, las redes neuronales se utilizan en sistemas de traducción, chatbots y conversión de texto a audio. En el caso de datos de audio, se emplean en sistemas de traducción, altavoces inteligentes y conversión de audio a texto.\nPara trabajar con redes neuronales, es crucial representar los datos de entrada numéricamente, incluso convirtiendo variables categóricas en valores numéricos y normalizando los datos entre 0 y 1. Esto facilita la convergencia hacia soluciones óptimas. Es importante que los datos seán números en coma flotante, sobre todo si se van a trabajar con GPUs, Graphics Process Units, ya que permitirán hacer un mejor uso de los multiples cores que les permiten operar en coma flotante de forma paralela. Actualmente, hay toda una serie de mejoras en las GPUs que permite aumentar el rendimiento de las redes neuronales como son el uso de operaciones en FP16 (Floating Point de 16 bits en lugar de 32) de forma que pueden hacer dos operaciones de forma simultánea (el formato estándar es FP32) y además con la reducción de memoria (punto muy importante) al meter en los 32 bits 2 datos en lugar de sólo uno. También se han añadido técnicas de Mixed Precision (Narang et al. 2018), los Tensor Cores (para las gráficas de NVIDIA) son otra de las mejoras que se han ido incorporando a la GPUs y que permiten acelerar los procesos tanto de entrenamiento como de predicción con las redes neuronales.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "capitulo1.html#principales-arquitecturas-y-software-de-deep-learning",
    "href": "capitulo1.html#principales-arquitecturas-y-software-de-deep-learning",
    "title": "1  Deep Learning",
    "section": "1.2 Principales arquitecturas y software de Deep Learning",
    "text": "1.2 Principales arquitecturas y software de Deep Learning\n\n1.2.1 Principales arquitecturas\nActualmente existen muchos tipos de estructuras de redes neuronales artificiales dado que logran resultados extraordinarios en muchos campos del conocimiento. Los primeros éxitos en el aprendizaje profundo se lograron a través de las investigaciones y trabajos de Geoffre Hinton (2006) que introduce las Redes de Creencia Profunda en cada capa de la red de una Máquina de Boltzmann Restringida (RBM) para la asignación inicial de los pesos sinápticos. Hace tiempo que se está trabajando con arquitecturas como los Autoencoders, Hinton y Zemel (1994), las RBMs de Hinton y Sejnowski (1986) y las DBNs (Deep Belief Networks), Hinton et al. (2006) y otras como las redes recurrentes y convolucionales. Estas técnicas constituyen en sí mismas arquitecturas de redes neuronales, aunque también algunas de ellas, como se ha afirmado en la introducción, se están empleando para inicializar los pesos de arquitecturas profundas de redes neuronales supervisadas con conexiones hacia adelante.\nRedes Convolucionales\nLas redes neuronales convolucionales (CNNs) han transformado el panorama del Deep Learning, destacándose por su habilidad para extraer características de alto nivel a través de la operación de convolución. Diseñadas específicamente para el procesamiento de imágenes, las CNNs son altamente eficientes en tareas de clasificación y segmentación en el ámbito de la visión artificial.\nInspiradas en el funcionamiento de la corteza visual del cerebro humano, estas redes representan una evolución del perceptrón multicapa. Aunque su uso se popularizó en la década de 1990 con el desarrollo de sistemas de lectura de cheques por parte de AT&T, las CNNs han experimentado una evolución significativa desde entonces.\nSu arquitectura se compone de capas de convolución, responsables de transformar los datos de entrada, y capas de pooling, encargadas de resumir la información relevante. Posteriormente, se aplican capas densamente conectadas para obtener el resultado final.\nEl auge de las CNNs se vio impulsado por iniciativas como la competencia ILSVRC, que propiciaron avances considerables en este campo. Entre los modelos más destacados se encuentran LeNet-5, AlexNet, VGG, GoogLeNet y ResNet, muchos de los cuales están disponibles como modelos preentrenados para su integración en diversas aplicaciones. Estos modelos, con estructuras de capas más complejas, representan el estado del arte en reconocimiento visual y están al alcance de cualquier investigador interesado en el Deep Learning.\nMás allá de las arquitecturas conocidas, han surgido modelos más avanzados como DenseNet y EfficientNet, que optimizan el rendimiento y la eficiencia computacional. La transferencia de aprendizaje se ha convertido en una herramienta fundamental, permitiendo adaptar modelos preentrenados a tareas específicas con conjuntos de datos más pequeños, agilizando el entrenamiento y mejorando la generalización.\nLas CNNs encuentran un amplio uso en tareas de segmentación semántica y detección de objetos, impulsadas por técnicas como U-Net y Mask R-CNN. Adicionalmente, métodos de aprendizaje débilmente supervisado y autoetiquetado están permitiendo entrenar modelos con datos etiquetados de manera menos precisa o incluso sin etiquetar.\nPara mejorar la interpretabilidad de las CNNs, se han propuesto técnicas de visualización de atención visual, que permiten identificar las partes de una imagen que son más relevantes para la predicción del modelo.\nEstos avances impulsan el continuo desarrollo de las CNNs, expandiendo su aplicación a diversos campos como el diagnóstico médico, la conducción autónoma y la robótica. La investigación activa en este campo sigue explorando nuevas formas de mejorar la eficiencia, la precisión y la interpretabilidad de las CNNs para abordar desafíos cada vez más complejos en el procesamiento de imágenes y otros tipos de datos.\nAutoencoders\nLos Autoencoders (AE) son una clase de redes neuronales dentro del ámbito del Deep Learning, caracterizadas por su enfoque en el aprendizaje no supervisado. Aunque se mencionaron por primera vez en la década de 1980, ha sido en los últimos años donde han experimentado un notable interés y desarrollo. La arquitectura de un AE consiste en dos partes principales: el encoder y el decoder. El encoder se encarga de codificar o comprimir los datos de entrada, mientras que el decoder se encarga de regenerar los datos originales en la salida, lo que resulta en una estructura simétrica.\nDurante el entrenamiento, el AE aprende a reconstruir los datos de entrada en la capa de salida de la red, generalmente implementando restricciones como la reducción de elementos en las capas ocultas del encoder. Esto evita simplemente copiar la entrada en la salida y obliga al modelo a aprender representaciones más significativas de los datos. Entre las aplicaciones principales de los AE se encuentran la reducción de dimensiones y compresión de datos, la búsqueda de imágenes, la detección de anomalías y la eliminación de ruido.\nAdemás de los autoencoders estándar, existen varias variaciones que han surgido para abordar diferentes desafíos y aplicaciones específicas, como los Variational Autoencoders (VAE), los Sparse Autoencoders, los Denoising Autoencoders y los Contractive Autoencoders. Estas variaciones amplían el alcance y la versatilidad de los autoencoders en una variedad de contextos de aprendizaje automático, desde la compresión de datos hasta la generación de nuevas muestras y la detección de anomalías en conjuntos de datos complejos.\nRedes Recurrentes\nLas redes neuronales recurrentes (RNNs) revolucionaron el panorama del machine learning, posicionándose como una herramienta fundamental para procesar y analizar datos secuenciales. A diferencia de las redes neuronales tradicionales con una estructura de capas fija, las RNNs poseen una arquitectura flexible que les permite incorporar información del pasado, presente y futuro, lo que las convirtió en una gran apuesta ante tareas omo el procesamiento del lenguaje natural, el reconocimiento de voz y la predicción de series temporales.\nGracias a su capacidad de memoria interna, las RNNs pueden capturar dependencias temporales en los datos secuenciales, una característica crucial para modelar el comportamiento de fenómenos que evolucionan con el tiempo. Esta característica las diferencia de las redes neuronales clásicas, que no tienen en cuenta el contexto temporal de la información.\nLa familia de las RNNs abarca diversas arquitecturas, cada una con sus propias fortalezas y aplicaciones. Entre las más populares encontramos las redes de Elman, las redes de Jordan, las redes Long Short-Term Memory (LSTM) y las redes Gated Recurrent Unit (GRU) que, introducidas en 2015 son una alternativa más ligera y eficiente a las LSTM.\nEl campo de las RNNs ha experimentado un rápido crecimiento en los últimos años, impulsado por avances en investigación y la disponibilidad de conjuntos de datos masivos. Entre las mejoras más notables encontramos las redes neuronales convolucionales recurrentes (CRNNs), las redes neuronales con atención y la integración del aprendizaje por refuerzo. Estas mejoras han ampliado aún más las capacidades de las RNNs, permitiéndolas abordar tareas cada vez más complejas y desafiantes.\nRedes Generativas Adversarias\nLas Generative Adversarial Networks (GAN) representan una innovadora aplicación del deep learning en la generación de contenido sintético, incluyendo imágenes, videos, música y caras extremadamente realistas. La arquitectura de una GAN consiste en dos componentes principales: un generador y un discriminador. El generador se encarga de crear nuevos datos sintéticos, como imágenes, a partir de un vector aleatorio en el espacio latente. Por otro lado, el discriminador tiene la tarea de distinguir entre datos reales y sintéticos, es decir, determinar si una imagen proviene del conjunto de datos original o si fue creada por el generador.\nEl generador se implementa típicamente utilizando una red neuronal convolucional profunda, con capas especializadas que aprenden a generar características de imágenes en lugar de extraerlas de una imagen de entrada. Algunas de las capas más comunes utilizadas en el modelo del generador son la capa de muestreo (UpSampling2D) que duplica las dimensiones de la entrada, y la capa convolucional de transposición (Conv2DTranspose) que realiza una operación de convolución inversa para generar datos sintéticos.\nLa idea clave detrás de las GAN es el entrenamiento adversarial, donde el generador y el discriminador compiten entre sí en un juego de suma cero. Mientras el generador trata de engañar al discriminador generando datos cada vez más realistas, el discriminador mejora su capacidad para distinguir entre datos reales y sintéticos. Este proceso de competencia continua lleva a la generación de datos sintéticos de alta calidad que son indistinguibles de los datos reales para el discriminador.\nEn los últimos años, las GAN han experimentado avances significativos en términos de nuevas arquitecturas y técnicas de entrenamiento. Por ejemplo, se han desarrollado variantes como las Conditional GAN (cGAN), que permiten controlar las características de los datos generados, y las Progressive GAN (ProgGAN), que generan imágenes de mayor resolución de forma progresiva. Además, se han propuesto técnicas de regularización, como la penalización del gradiente o la normalización espectral, para mejorar la estabilidad y la calidad de las GAN generadas.\nLas GANs han abierto un abanico de posibilidades en diversos campos como el ámbito de la generación de texto así como aplicaciones en la realidad aumentada donde permiten integrar elementos sintéticos en el mundo real de forma realista, como la creación de avatares virtuales o la superposición de información sobre objetos físicos. Asimismo, de los videojuegos, las GANs se utilizan para desarrollar personajes, escenarios y objetos virtuales de alta calidad para experiencias de juego más inmersivas.\nBoltzmann Machine y Restricted Boltzmann Machine\nEl aprendizaje de la denominada máquina de Boltzmann (BM) se realiza a través de un algoritmo estocástico que proviene de ideas basadas en la mecánica estadística. Este prototipo de red neuronal tiene una característica distintiva y es que el uso de conexiones sinápticas entre las neuronas es simétrico.\nLas neuronas son de dos tipos: visibles y ocultas. Las neuronas visibles son las que interactúan y proveen una interface entre la red y el ambiente en el que operan, mientras que las neuronas actúan libremente sin interacciones con el entorno. Esta máquina dispone de dos modos de operación. El primero es la condición de anclaje donde las neuronas están fijas por los estímulos específicos que impone el ambiente. El otro modo es la condición de libertad, donde tanto las neuronas ocultas como las visibles actúan libremente sin condiciones impuestas por el medio ambiente. Las maquinas restringidas de Boltzmann (RBM) solamente toman en cuenta aquellos modelos en los que no existen conexiones del tipo visible-visible y oculta-oculta. Estas redes también asumen que los datos de entrenamiento son independientes y están idénticamente distribuidos.\nUna forma de estimar los parámetros de un modelo estocástico es calculando la máxima verosimilitud. Para ello, se hace uso de los Markov Random Fiels (MRF), ya que al encontrar los parámetros que maximizan los datos de entrenamiento bajo una distribución MRF, equivale a encontrar los parámetros \\(\\theta\\) que maximizan la verosimilitud de los datos de entrenamiento, Fischer e Igel (2012). Maximizar dicha verosimilitud es el objetivo que persigue el algoritmo de entrenamiento de una RBM. A pesar de utilizar la distribución MRF, computacionalmente hablando se llega a ecuaciones inviables de implementar. Para evitar el problema anterior, las esperanzas que se obtienen de MRF pueden ser aproximadas por muestras extraídas de distribuciones basadas en las técnicas de Markov Chain Monte Carlo Techniques (MCMC). Las técnicas de MCMC utilizan un algoritmo denominado muestreo de Gibbs con el que obtenemos una secuencia de observaciones o muestras que se aproximan a partir de una distribución de verosimilitud de múltiples variables aleatorias. La idea básica del muestreo de Gibss es actualizar cada variable posteriormente en base a su distribución condicional dado el estado de las otras variables.\nDeep Belief Network\nUna red Deep Belief Network tal como demostró Hinton se puede considerar como un “apilamiento de redes restringidas de Boltzmann”. Tiene una estructura jerárquica que, como es sabido, es una de las características del deep learning. Como en el anterior modelo, esta red también es un modelo en grafo estocástico, que aprende a extraer una representación jerárquica profunda de los datos de entrenamiento. Cada capa de la RBM extrae un nivel de abstracción de características de los datos de entrenamiento, cada vez más significativo; pero para ello, la capa siguiente necesita la información de la capa anterior lo que implica el uso de las variables latentes.\nEstos modelos caracterizan la distribución conjunta \\(h_k\\) entre el vector de observaciones x y las capas ocultas, donde \\(x=h_0\\), es una distribución condicional para las unidades visibles limitadas sobre las unidades ocultas que pertenecen a la RBM en el nivel k, y es la distribución conjunta oculta visible en la red RBM del nivel superior o de salida.\nEl entrenamiento de esta red puede ser híbrido, empezando por un entrenamiento no supervisado para después aplicar un entrenamiento supervisado para un mejor y más óptimo ajuste, aunque pueden aplicarse diferentes tipos de entrenamiento, Bengio et al. (2007) y Salakhutdinov (2014) Para realizar un entrenamiento no supervisado se aplica a las redes de creencia profunda con Redes restringidas de Boltzmann el método de bloque constructor que fue presentado por Hinton (2006) y por Bengio (2007).\n\n\n1.2.2 Software\nComo se verá en los siguientes epígrafes, la opción preferida para este módulo de Deep Learning es el software llamado Keras que está programado en Python. En términos de eficiencia y de aprendizaje Keras presenta unas ventajas importantes que se especifican más adelante.\nAunque nuestra preferencia a nivel formativo es el uso de Keras y Tensorflow, a continuación serán descritos los principales softwares con los que poder realizar implementaciones de arquitecturas de aprendizaje profundo: TensorFlow, Keras, Pytorch, MXNET, Caffe y JAX.\nPor su parte, también presentaremos Colaboratory Environment (Colab), una herramienta de Google que dispone en la web y que no requiere ninguna instalación en nuestros ordenadores. Esta propuesta de Google resulta muy interesante dado que no requiere coste alguno, se puede ejecutar desde cualquier lugar aumentando nuestros recursos a la hora de trabajar con Deep Learning y admitiendo a su vez la implementación tanto de código Python como de R.\nTensorFlow\nTensorFlow es una biblioteca de código abierto para el cálculo numérico desarrollada por Google. Es una de las herramientas de Deep Learning más populares y ampliamente utilizadas, conocida por su flexibilidad, escalabilidad y comunidad activa. TensorFlow ofrece una amplia gama de funciones para construir, entrenar y desplegar modelos de Deep Learning, incluyendo:\n\nSoporte para una variedad de arquitecturas de redes neuronales: permite construir una amplia gama de arquitecturas de redes neuronales, desde redes convolucionales y recurrentes hasta modelos de atención y redes generativas adversarias (GANs)\nEscalabilidad a grandes conjuntos de datos: está diseñado para manejar grandes conjuntos de datos y puede distribuirse en múltiples GPUs o TPU para acelerar el entrenamiento de modelos\nAmplia gama de herramientas de visualización y depuración: proporciona una variedad de herramientas para visualizar y depurar modelos de Deep Learning, lo que facilita la identificación y resolución de problemas\nGran comunidad y recursos: cuenta con una gran y activa comunidad de desarrolladores y usuarios que proporcionan soporte y comparten recursos\n\nPytorch\nPyTorch es una biblioteca de código abierto para el aprendizaje automático desarrollada por Facebook. Es conocida por su sintaxis intuitiva y facilidad de uso, lo que la convierte en una opción popular para investigadores y desarrolladores principiantes. PyTorch ofrece características similares a TensorFlow, incluyendo:\n\nSoporte para una variedad de arquitecturas de redes neuronales: permite construir una amplia gama de arquitecturas de redes neuronales, desde redes convolucionales y recurrentes hasta modelos de atención y GANs\nEjecución dinámica de gráficos: utiliza un motor de ejecución de gráficos dinámico, lo que permite modificar los modelos durante el entrenamiento, lo que facilita la experimentación y el ajuste fino\nAmplia gama de bibliotecas y herramientas de terceros: se beneficia de un ecosistema rico de bibliotecas y herramientas de terceros que amplían sus capacidades\nFacilidad de uso: tiene una sintaxis similar a Python, lo que la hace fácil de aprender y usar para desarrolladores con experiencia en Python\n\nKeras\nKeras es una biblioteca de código abierto para el aprendizaje automático de alto nivel que se ejecuta sobre TensorFlow o PyTorch. Es conocida por su simplicidad y facilidad de uso, lo que la convierte en una opción popular para principiantes y para desarrollar prototipos de modelos rápidamente. Keras ofrece una interfaz de alto nivel que abstrae las complejidades de las bibliotecas subyacentes, como TensorFlow o PyTorch, lo que permite a los usuarios centrarse en la construcción y el entrenamiento de modelos sin necesidad de profundizar en los detalles de implementación. Entre las principales características de Keras destaca:\n\nSimplicidad: tiene una sintaxis intuitiva y fácil de aprender, lo que la hace ideal para principiantes y para desarrollar prototipos de modelos rápidamente\nFacilidad de uso: ofrece una API de alto nivel que abstrae las complejidades de las bibliotecas subyacentes, como TensorFlow o PyTorch, lo que permite a los usuarios centrarse en la construcción y el entrenamiento de modelos sin necesidad de profundizar en los detalles de implementación\nFlexibilidad: permite construir una amplia gama de modelos de Deep Learning, desde redes neuronales convolucionales y recurrentes hasta modelos de atención y redes generativas adversarias (GANs)\nModularidad: al ser una biblioteca modular que permite a los usuarios combinar diferentes componentes para construir sus modelos personalizados\nSoporte para múltiples plataformas: se puede utilizar en una variedad de plataformas, incluyendo Windows, macOS y Linux.\n\nJAX\nJAX, desarrollada por Google Research, se posiciona como una biblioteca de Python para el aprendizaje automático y el cálculo numérico, diseñada para ofrecer un rendimiento y una flexibilidad excepcionales, especialmente en el entrenamiento de modelos de deep learning en aceleradores como GPUs y TPUs.\nSu enfoque se basa en la composición de funciones puras y transformaciones automáticas de gradiente, lo que la convierte en una herramienta ideal para implementar algoritmos de aprendizaje automático diferenciables y de alto rendimiento. Entre sus características destacadas encontramos:\n\nAutodiferenciación: calcula automáticamente gradientes (autodiferenciación), simplificando el desarrollo de modelos de deep learning\nComposición eficiente de transformaciones: combina operaciones elementales en funciones compuestas para un procesamiento eficiente\nIntegración con frameworks: se integra con frameworks de deep learning como TensorFlow y PyTorch, aprovechando las ventajas de cada uno\nParalelización y distribución: permite ejecutar operaciones en paralelo y de manera distribuida, ideal para grandes conjuntos de datos\nAltas prestaciones para el entrenamiento de modelos: sobresale por su capacidad de computación de alto rendimiento, haciéndola ideal para entrenar modelos de deep learning complejos de manera eficiente. Así, se ha convertido en una opción atractiva para aquellos que manejan grandes conjuntos de datos y buscan optimizar el tiempo de entrenamiento\nFlexibilidad para la investigación y experimentación: facilita la implementación de nuevas arquitecturas y algoritmos, permitiendo explorar diferentes enfoques y optimizar el rendimiento de los modelos\nPersonalización de flujos de trabajo: permite definir funciones y transformaciones personalizadas, proporcionando un control preciso sobre el pipeline de trabajo. Esto resulta útil para adaptar el proceso de entrenamiento a necesidades específicas y optimizar el rendimiento para tareas concretas\n\nMxnet\nMXNet es una biblioteca de código abierto para el aprendizaje automático desarrollada por Apache Software Foundation. Es conocida por su escalabilidad, flexibilidad y soporte para múltiples lenguajes de programación, incluyendo Python, R y C++. MXNet ofrece características similares a TensorFlow y PyTorch, incluyendo:\n\nSoporte para una variedad de arquitecturas de redes neuronales: permite construir una amplia gama de arquitecturas de redes neuronales, desde redes convolucionales y recurrentes hasta modelos de atención y GANs\nEscalabilidad a grandes conjuntos de datos: está diseñado para manejar grandes conjuntos de datos y puede distribuirse en múltiples GPUs o TPU para acelerar el entrenamiento de modelos\nSoporte para múltiples lenguajes de programación: se puede utilizar con Python, R y C++, lo que lo hace accesible a una amplia gama de desarrolladores\nFlexibilidad: permite a los usuarios personalizar y extender la biblioteca para satisfacer sus necesidades específicas\n\nCaffe\nCaffe es un marco de código abierto para el aprendizaje profundo desarrollado por la Universidad de California, Berkeley. Es conocido por su simplicidad, velocidad y eficiencia, lo que lo convierte en una opción popular para aplicaciones de Deep Learning en tiempo real. Caffe ofrece características similares a TensorFlow y PyTorch, incluyendo:\n\nSoporte para una variedad de arquitecturas de redes neuronales: permite construir una amplia gama de arquitecturas de redes neuronales, desde redes convolucionales y recurrentes hasta modelos de atención y GANs\nEntrenamiento rápido y eficiente: está optimizado para el rendimiento y la eficiencia, lo que lo hace ideal para aplicaciones de aprendizaje profundo\n\n\n1.2.2.1 Google Colab\nEl entorno Colab (Google Colaboratory) es una potente herramienta de google para ejecutar código incluido el deep Dearning y que está disponible en la web (https://colab.research.google.com/). Se ha desarrollado para Python, pero actualmente también se puede ejecutar código de R. Esta funcionalidad puede importar un conjunto de datos de imágenes, entrenar un clasificador con este conjunto de datos y evaluar el modelo con tan solo usar unas pocas líneas de código. Los cuadernos de Colab ejecutan código en los servidores en la nube de Google, lo que nos permite aprovechar la potencia del hardware de Google, incluidas las GPU y TPU, independientemente de la potencia de tu equipo. Lo único que se necesita es un navegador.\nCon Colab se puede aprovechar toda la potencia de las bibliotecas más populares de Python para analizar y visualizar datos. La celda de código de abajo utiliza NumPy para generar datos aleatorios y Matplotlib para visualizarlos. Para editar el código, solo se tiene que hacer clic en la celda.\n{#fig-colab_1]\nEste es el menú principal de colab desde donde podemos gestionar nuestros proyectos:\n{#fig-colab_nuevo_fichero]\nDesde el menú Archivo, como en la mayor parte de los programas, podemos llevar a cabo las operaciones habituales de abrir y guardar los ficheros en diferentes formatos. En este caso se pueden abrir ficheros de Jupyter/Python desde cualquier dispositivo externo, desde el repositorio Drive o de Github:\n{#fig-colab_importa]\nSi queremos subir un fichero que tenemos en nuestro ordenador vamos a Archivo/Subir cuaderno y podemos elegir nuestro archivo cuando se despliegue la siguiente pantalla:\n{#fig-colab_importa_archivo]\nComo se ha comentado también se pueden importar archivos desde GitHub introduciendo la url de GitHub:\n{#fig-colab_importa_github]\nPor último, colab nos permite tener acceso tanto a GPUs de forma como a CPUs más potentes que nuestro ordenador de escritorio de forma gratuita.\n{#fig-colab_entorno_ejecucion_inicio]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "capitulo1.html#conceptos-básicos-de-las-redes-neuronales",
    "href": "capitulo1.html#conceptos-básicos-de-las-redes-neuronales",
    "title": "1  Deep Learning",
    "section": "1.3 Conceptos básicos de las Redes Neuronales",
    "text": "1.3 Conceptos básicos de las Redes Neuronales\nVamos a hacer una revisión de las redes neuronales para posteriormente poder abordar los diferentes tipos de redes neuronales que se utilizan en Deep Learning. Algunos de los avances más recientes en varios de los diferentes componentes que forman parte de las redes neuronales están recopilados en (Gu et al. 2017)\nLas redes neuronales artificiales tienen sus orígenes en el Perceptrón, que fue el modelo creado por Frank Rosenblatt en 1957 y basado en los trabajos que previamente habían realizado Warren McCullon (neurofisiólogo) y Walter Pitts (matemático).\nEl Perceptrón está construido por una neurona artificial cuyas entradas y salida pueden ser datos numéricos, no como pasaba con la neurona de McCulloch y Pitts (eran sólo datos lógicos). Las neuronas pueden tener pesos y además se le aplica una función de activación Sigmoid (a diferencia de la usada anteriormente al Paso binario).\nEn esta neurona nos encontramos que se realizan los siguientes cálculos: \\[ z = \\sum_{i=1}^{n}w_ix_i+b_i\\] \\[\\hat{y} = \\delta (z)\\] donde representan los datos numéricos de entrada, son los pesos, es el sesgo (bias), es la función de activación y finalmente es el dato de salida.\nEl modelo de perceptrón es el más simple, en el que hay una sola capa oculta con una única neurona.\nEl siguiente paso nos lleva al Perceptrón Multicapa donde ya pasamos a tener más de una capa oculta, y además podemos tener múltiples neuronas en cada capa oculta.\nCuando todas las neuronas de una capa están interconectadas con todas las de la siguiente capa estamos ante una red neuronal densamente conectada. A lo largo de las siguientes secciones nos encontraremos con redes en las que no todas las neuronas de una capa se conectan con todas de la siguiente.\nVeamos como describiríamos ahora los resultados de las capas \\[\nz_j^{(l)}=\\sum_{i=1}^{n_j} w_{i j}^{(l)} a_i^{(l-1)}+b_i^{(l)} \\\\\na_j^{(l)}=\\delta^{(l)}\\left(z_j^{(l)}\\right)\n\\] donde \\(a_i^{(l-1)}\\) representan los datos de la neurona \\(i\\) en la capa \\(l-1\\) ( siendo \\(a_i^0=x_i\\) los valores de entrada), \\(w_{i j}^{(l)}\\) son los pesos en la capa \\(l\\), \\(b_i^{(l)}\\) es el sesgo (bias) en la capa \\(l\\), \\(\\delta^{(l)}\\) es la función de activación en la capa \\(l\\) (puede que cada capa tenga una función de activación diferente), \\(n_j\\) es el número de neurona de la capa anterior que conectan con la \\(j\\) y finalmente \\(a_j^{(l)}\\) es el dato de salida de la capa \\(l\\). Es decir, en cada capa para calcular el nuevo valor necesitamos usar los valores de la capa anterior.\nAplicaciones de las Redes Neuronales\nCada día las redes neuronales están más presentes en diferentes campos y ayudan a resolver una gran variedad de problemas. Podríamos pensar que de forma más básica una red neuronal nos puede ayudar a resolver problemas de regresión y clasificación, es decir, podríamos considerarlo como otro modelo más de los existentes que a partir de unos datos de entrada somos capaces de obtener o un dato numérico (o varios) para hacer una regresión (calcular en precio de una vivienda en función de diferentes valores de la misma) o que somos capaces de conseguir que en función de los datos de entrada nos deje clasificada una muestra (decidir si conceder o no una hipoteca en función de diferentes datos del cliente).\nSi los datos de entrada son imágenes podríamos estar usando las redes neuronales como una forma de identificar esa imagen:\n\nIdentificando que tipo de animal es\nIdentificando que señal de tráfico es\nIdentificando que tipo de fruta es\nIdentificando que una imagen es de exterior o interior de una casa\nIdentificando que es una cara de una persona\nIdentificando que una imagen radiográfica represente un tumor maligno\nIdentificando que haya texto en una imagen\n\nLuego podríamos pasar a revolver problemas más complejos combinando las capacidades anteriores:\n\nDetectar los diferentes objetos y personas que se encuentran en una imagen\nEtiquedado de escenas (aula con alumnos, partido de futbol, etc…)\n\nDespués podríamos dar el paso al video que lo podríamos considerar como una secuencia de imágenes:\n\nContar el número de personas que entran y salen de una habitación\nReconocer que es una carretera\nIdentificar las señales de tráfico\nDetectar si alguien lleva un arma\nSeguimiento de objetos\nDetección de estado/actitud de una persona\nReconocimiento de acciones (interpretar lenguaje de signos, interpretar lenguaje de banderas)\nVehículos inteligentes\n\nSi los datos de entrada son secuencias de texto\n\nSistemas de traducción - Chatbots (resolución de preguntas a usuarios)\nConversión de texto a audio\n\nSi los datos de entrada son audios\n\nSistemas de traducción\nAltavoces inteligentes\nConversión de audio a texto\n\nA continuación, pasamos a revisar diferentes elementos de las redes neuronales que suelen ser comunes a todos los tipos de redes neuronales.\n\n1.3.1 Datos\nCuando se trabaja con redes neuronales necesitamos representar los valores de las variables de entrada en forma numérica. En una red neuronal todos los datos son siempre numéricos. Esto significa que todas aquellas variables que sean categóricas necesitamos convertirlas en numéricas.\nAdemás, es muy conveniente normalizar los datos para poder trabajar con valores entre 0 y 1, que van a ayudar a que sea más fácil que se pueda converger a la solución. Es importante que los datos seán números en coma flotante, sobre todo si se van a trabajar con GPUs (Graphics Process Units), ya que permitirán hacer un mejor uso de los multiples cores que les permiten operar en coma flotante de forma paralela. Actualmente, hay toda una serie de mejoras en las GPUs que permite aumentar el rendimiento de las redes neuronales como son el uso de operaciones en FP16 (Floating Point de 16 bits en lugar de 32) de forma que pueden hacer dos operaciones de forma simultánea (el formato estándar es FP32) y además con la reducción de memoria (punto muy importante) al meter en los 32 bits 2 datos en lugar de sólo uno. También se han añadido técnicas de Mixed Precision (Narang et al. 2018), los Tensor Cores (para las gráficas de NVIDIA) son otra de las mejoras que se han ido incorporando a la GPUs y que permiten acelerar los procesos tanto de entrenamiento como de predicción con las redes neuronales.\nEl primer objetivo será convertir las variables categóricas en variables numéricas, de forma la red neuronal pueda trabajar con ellas. Para realizar la conversión de categórica a numérica básicamente tenemos dos métodos para realizarlo:\n\nCodificación one-hot.\nCodificación entera.\n\nLa codificación one-hot consiste en crear tantas variables como categorías tenga la variable, de forma que se asigna el valor 1 si tiene esa categoría y el 0 si no la tiene.\nLa codificación entera lo que hace es codificar con un número cada categoría. Realmente esta asignación no tiene ninguna interpretación numérica ya que en general las categorías no tienen porque representar un orden al que asociarlas.\nNormalmente se trabaja con codificación one-hot para representar los datos categóricos de forma que será necesario preprocesar los datos de partida para realizar esta conversión, creando tantas variables como categorías haya por cada variable.\nSi nosotros tenemos nuestra muestra de datos que tiene \\(n\\) variables \\(x=\\{x_1,x_2,...,x_n\\}\\) de forma que \\(x_{n-2},x_{n-1},x_n\\) son variables categóricas que tienen \\(k,l,m\\) número de categorías respectivamente, tendremos finalmente las siguientes variables sólo numéricas: \\[ x=\\{x_1,x_2,...,x_{(n-2)_1},...,x_{(n-2)_k},x_{(n-1)_1},...,x_{(n-1)_l},x_{n_1},...,x_{n_m}\\} \\]\nDe esta forma, se aumentarán el número de variables con las que vamos a trabajar en función de las categorías que tengan las variables categóricas. Normalmente nos encontramos que en una red neuronal las variables de salida son:\n\nun número (regresión)\nuna serie de números (regresión múltiple)\nun dato binario (clasificación binaria)\nuna serie de datos binarios que representa una categoría de varias (clasifiación múltiple)\n\n\n\n1.3.2 Arquitectura de red\nPara la construcción de una red neuronal necesitamos definir la arquitectura de esa red. Esta arquitectura, si estamos pensando en una red neuronal densamente conectada, estará definida por la cantidad de capas ocultas y el número de neuronas que tenemos en cada capa. Más adelante veremos que dependiendo del tipo de red neuronal podrá haber otro tipo de elementos en estas capas.\n\n\n1.3.3 Función de coste y pérdida\nOtro de los elementos clave que tenemos que tener en cuenta a la hora de usar nuestra red neuronal son las funciones de pérdida y funciones de coste (objetivo).\nLa función de pérdida va a ser la función que nos dice cómo de diferente es el resultado del dato que nosotros queríamos conseguir respecto al dato original. Normalmente se suelen usar diferentes tipos de funciones de pérdida en función del tipo de resultado con el que se vaya a trabajar.\nLa función de coste es la función que vamos a tener que optimizar para conseguir el mínimo valor posible, y que recoge el valor de la función de pérdida para toda la muestra.\nTanto las funciones de pérdida como las funciones de coste, son funciones que devuelven valores de de \\(\\mathbb{R}\\)..\nSi tenemos un problema de regresión en el que tenemos que predecir un valor o varios valores numéricos, algunas de las funciones a usar son:\n\nError medio cuadrático \\(\\left(\\mathrm{L}_2^2\\right)\\)\n\n\\[\n\\mathcal{L}_{\\text {MSE }}(\\mathrm{y}, \\hat{\\mathrm{y}})=\\|\\hat{\\mathrm{y}}-\\mathrm{y}\\|^2=\\sum_{\\mathrm{i}=1}^{\\mathrm{n}}\\left(\\hat{\\mathrm{y}}_{\\mathrm{i}}-\\mathrm{y}_{\\mathrm{i}}\\right)^2\n\\] donde \\(\\hat{y}\\) y y son vectores de tamaño \\(n, y\\) es el valor real e \\(\\hat{y}\\) es el valor predicho\n\nError medio absoluto ( \\(\\mathrm{L}_1\\) )\n\n\\[\n\\mathcal{L}_{\\text {MAE }}(\\mathrm{y}, \\hat{y})=|\\hat{y}-y|=\\sum_{i=0}^n\\left|\\hat{y}_i-y_i\\right|\n\\] donde \\(\\hat{y}\\) y y son vectores de tamaño \\(n, y\\) es el valor real e \\(\\hat{y}\\) es el valor predicho\nPara los problemas de clasifiación:\n\nBinary Crossentropy (Sólo hay dos clases) \\[\n\\mathcal{L}_{\\text {CRE }}(\\mathrm{y}, \\hat{y})=-(\\mathrm{y} \\log (\\hat{y})+(1-\\mathrm{y}) \\log (1-\\hat{y}))\n\\]\n\n\\(\\mathrm{y}\\) es el valor real e \\(\\hat{y}\\) es el valor predicho\n\nCategorical Crosentropy (Múltiples clases representadas como one-hot)\n\n\\[\n\\mathcal{L}_{\\text {CAE }}\\left(\\mathrm{y}_{\\mathrm{c}}, \\hat{\\mathrm{y}}_{\\mathrm{c}}\\right)=-\\sum_{\\mathrm{c}=1}^{\\mathrm{k}} \\mathrm{y}_{\\mathrm{c}} \\log \\left(\\hat{y}_c\\right)\n\\]\n\\(y_c\\) es el valor real para la clase \\(c\\) e \\(\\hat{y}_c\\) es el valor predicho para la clase \\(c\\)\n\nSparse Categorical Crossentropy (Múltiples clases representadas comp un entero)\n\n\\[\n\\mathcal{L}_{\\text {SCAE }}\\left(\\mathrm{y}_{\\mathrm{c}}, \\hat{\\mathrm{y}}_{\\mathrm{c}}\\right)=-\\sum_{\\mathrm{c}=1}^{\\mathrm{k}} \\mathrm{y}_{\\mathrm{c}} \\log \\left(\\hat{y}_{\\mathrm{c}}\\right)\n\\]\n\\(\\mathrm{y}_c\\) es el valor real para la clase \\(c\\) e \\(\\hat{y}_c\\) es el valor predicho para la clase \\(c\\)\n\nKullback-Leibler Divergence\n\nEsta función se usa para calcular la diferencia entre dos distribuciones de probabilidad se usa por ejemplo en algunas redes como Variational Autoencoders (Doersch 2016 Modelos GAN (Generative Adversarial Networks)\n\\[\n\\mathcal{D}_{\\mathrm{KL}}(\\mathrm{p} \\| \\mathrm{q})=-\\mathrm{H}(\\mathrm{p}(\\mathrm{x}))-\\mathrm{E}_{\\mathrm{p}}[\\log \\mathrm{q}(\\mathrm{x})]\n\\]\n\\[\n=\\sum_x p(x) \\operatorname{logp}(x)-\\sum_x p(x) \\log q(x)=\\sum_x p(x) \\log \\frac{p(x)}{q(x)}\n\\]\n\\[\n\\mathcal{L}_{\\text {vae }}(y, \\hat{y})=E_{z \\sim q_\\phi(z \\mid x)}\\left[\\operatorname{logp}_\\theta(x \\mid z)\\right]-\\mathcal{D}_{\\text {KL }}\\left(q_\\phi(z \\mid x) \\| p(z)\\right)\n\\]\n\nHinge Loss\n\n\\[\n\\mathcal{L}_{\\text {hinge }}(\\mathrm{y}, \\hat{y})=\\max (0,1-\\mathrm{y} * \\hat{\\mathrm{y}})\n\\]\nLas correspondientes funciones de coste que se usarían, estarían asociadas a todas las muestras que se estén entrenando o sus correpondientes batch, así como posibles términos asociados a la regularización para evitar el sobreajuste del entrenamiento. Es decir, la función de pérdida se calcula para cada muestra, y la función de coste es la media de todas las muestras.\nPor ejemplo, para el Error medio cuadrático \\(\\left(L_2\\right)\\) tendríamos el siguiente valor: \\[\n\\mathcal{J}_{\\text {MSB }}(y, \\hat{y})=\\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}_{\\text {MSE }}(y, \\hat{y})=\\frac{1}{m} \\sum_{i=1}^m|| \\hat{y}_i-y_i \\|^2=\\frac{1}{n} \\sum_{i=1}^m \\sum_{i=1}^n\\left(\\hat{y}_{j i}-y_{j i}\\right)^2\n\\]\n\n\n1.3.4 Optimizador\nEl Descenso del gradiente es la versión más básica de los algoritmos que permiten el aprendizaje en la red neuronal haciendo el proceso de backpropagation (propagación hacia atrás). A continuación veremos una breve explicación del algoritmo así como algunas variantes del mismo recogidas en (Ruder 2017).\nRecordamos que el descenso del gradiente nos permitirá actualizar los parámetros de la red neuronal cada vez que demos una pasada hacia delante con todos los datos de entrada, volviendo con una pasada hacia atrás.\n\\[\\mathrm{w}_{\\mathrm{t}}=\\mathrm{w}_{\\mathrm{t}-1}-\\alpha \\nabla_{\\mathrm{w}} \\mathcal{J}(\\mathrm{w})\\]\ndonde \\(\\mathcal{J}\\) es la función de coste, \\(\\alpha\\) es el parámetro de ratio de aprendizaje que permite definir como de grandes se quiere que sean los pasos en el aprendizaje.\nCuando lo que hacemos es actualizar los parámetros para cada pasada hacia delante de una sola muestra, estaremos ante lo que llamamos Stochastic Gradient Descent (SGD). En este proceso convergerá en menos iteraciones, aunque puede tener alta varianza en los parámetros.\n\\[\\mathrm{W}_{\\mathrm{t}}=\\mathrm{w}_{\\mathrm{t}-1}-\\alpha \\nabla_{\\mathrm{w}} \\mathcal{J}(\\mathrm{w}, x(i),y(i))\\]\ndonde \\(x(i)\\) e \\(y(i)\\) son los valores en la pasada de la muestra \\(i\\).\nPodemos buscar un punto intermedio que sería cuando trabajamos por lotes y cogemos un bloque de datos de la muestra, les aplicamos la pasada hacia delante y aprendemos los parámetros para ese bloque. En este caso lo llamaremos Mini-batch Gradient Descent\n\\[\\mathrm{W}_{\\mathrm{t}}=\\mathrm{w}_{\\mathrm{t}-1}-\\alpha \\nabla_{\\mathrm{w}} \\mathcal{J}(\\mathrm{w}, \\mathrm{B}(\\mathrm{i}))\\]\ndonde \\(\\mathrm{B}(\\mathrm{i})\\) son los valores de ese batch .\nEn general a estos métodos nos referiremos a ellos como SGD.\nSobre este algoritmo base se han hecho ciertas mejoras como:\nLearning rate decay Podemos definir un valor de decenso del ratio de aprendizaje, de forma que normalmente al inicio de las iteraciones de la red neuronal los pasos serán más grandes, pero conforme nos acercamos a la solución optima deberemos dar pasos más pequeños para ajustarnos mejor.\n\\[\\mathrm{W}_{\\mathrm{t}}=\\mathrm{w}_{\\mathrm{t}-1}-\\alpha_{\\mathrm{t}} \\nabla_{\\mathrm{w}} \\mathcal{J}\\left(\\mathrm{w}_{\\mathrm{t}-1}\\right)\\]\ndonde \\(\\alpha _t\\) ahora se irá reduciendo en función del valor del decay.\nMomentum El momentum se introdujo para suavizar la convergencia y reducir la alta varianza de SGD.\n\\[ V_ {t}  =  \\gamma   v_ {t-1}  +  \\alpha  V_ {w} J(  w_ {t-1}  ,x,y)\\] \\[ W_ {t} =  w_ {t-1}  -  v_ {1} \\]\ndonde \\(v_t\\) es lo que se llama el vector velocidad con la dirección correcta.\nNAG (Nesterov Accelerated Gradient) Ahora daremos un paso más con el NAG, calculando la función de coste junto con el vector velocidad.\n\\[ V_ {t}  =  \\gamma   v_ {t-1}  +  \\alpha   V_ {w}  J(  w_ {t-1}  -  \\gamma   v_ {t-1}  ,x,y) \\] \\[ W_ {t}  =  w_ {t-1}  -  v_ {t}  \\]\ndonde ahora vemos que la función de coste se calcula usando los parámetros de \\(w_t\\) sumado a \\(\\gamma   v_ {t-1}\\)\nVeamos algunos algoritmos de optimización más que, aunque provienen del SGD, se consideran independientes a la hora de usarlos y no como parámetros extras del SGD.\nAdagrad (Adaptive Gradient) Esta variante del algoritmo lo que hace es adaptar el ratio de aprendizaje para cada uno de los pesos en lugar de que sea global para todos.\n\\[ W_ {t,i}  =  w_ {t-1,i}  -  \\frac {\\alpha }{\\sqrt {G_ {t-1,i,j}+\\epsilon }}   \\nabla_ {w_{t-1}}  J(  w_ {t-1,i} ,x,y) \\]\ndonde tenemos que \\(G_t \\in R^{dxd}\\)$es una matriz diagonal donde cada elemento es la suma de los cuadrados de los gradientes en el paso \\(t-1\\) , y es un término de suavizado para evitar divisiones por 0.\nRMSEProp (Root Mean Square Propagation) En este caso tenemos una variación del Adagrad en el que intenta reducir su agresividad reduciendo monotonamente el ratio de aprendizaje. En lugar de usar el gradiente acumulado desde el principio de la ejecución, se restringe a una ventana de tamaño fijo para los últimos n gradientes calculando su media. Así calcularemos primero la media en ejecución de los cuadros de los gradientes como:\n\\[\n\\mathrm{E}\\left[g^2\\right]_{t-1}=\\gamma E\\left[g^2\\right]_{t-2}+(1-\\gamma) g_{t-1}^2\n\\]\ny luego ya pasaremos a usar este valor en la actualización\n\\[\nw_{t, i}=w_{t-1, i}-\\frac{\\alpha}{\\sqrt{E\\left[ g^2\\right]_{t-1}+\\epsilon}} \\nabla_{w_{t-1}} \\mathcal{J}\\left(w_{t-1, i}, x, y\\right)\n\\]\nAdaDelta\nAunque se desarrollaron de forma simultánea el AdaDelta y el RMSProp son muy parecidos en su primer paso incial, llegando el de AdaDelta un poco más lejos en su desarrollo.\n\\[\nw_{t, i}=w_{t-1, i}-\\frac{\\alpha}{\\sqrt{E\\left[ g^2\\right]_{t-1}+\\epsilon}} \\nabla_{w_{t-1}} \\mathcal{J}\\left(w_{t-1, i}, x, y\\right)\n\\]\ny luego ya pasaremos a usar este valor en la actualización\n\\[\n\\begin{gathered}w_{t, i}=w_{t-1, i}-\\frac{\\alpha}{\\sqrt{E\\left[g^2\\right]_{t-1}+\\epsilon}} \\nabla_{w_{t-1}} \\mathcal{J}\\left(w_{\\mathrm{t}-1, \\mathrm{i}}, \\mathrm{X}, \\mathrm{y}\\right) \\\\ \\Delta w_{\\mathrm{t}}=-\\frac{\\alpha}{\\sqrt{\\mathrm{E}\\left[g^2\\right]_{\\mathrm{t}}+\\epsilon}} g_t\\end{gathered}\n\\]\nAdam (Adaptive Moment Estimation)\n\\[\n\\begin{gathered}G_t=\\nabla_{w_t} \\mathcal{J}\\left(w_t\\right) \\\\ M_{t-1}=\\beta_1 m_{t-2}+\\left(1-\\beta_1\\right) g_{t-1} \\\\ v_{t-1}=\\beta_2 v_{t-2}+\\left(1-\\beta_2\\right) g_{t-1}^2\\end{gathered}\n\\]\ndonde \\(m_{t-1}\\) y \\(V_{t-1}\\) son estimaciones del primer y segundo momento de los gradientes respectivamente, y \\(\\beta_1\\) y \\(\\beta_2\\) parámetros a asignar.\n\\[\\widehat{M}_{t-1}  =\\frac{m_{t-1}}{1-\\beta_1^{t-1}} \\\\  \\widehat{V}_{t-1}  =\\frac{v_{t-1}}{1-\\beta_2^{t-1}} \\\\  W_t=w_{t-1}  -\\frac{\\alpha}{\\sqrt{\\hat{v}_{t-1}+\\epsilon}} \\widehat{m}_{t-1}\\]\nAdamax\n\\[\nG_t=\\nabla_{w_t} \\mathcal{J}\\left(w_t\\right) \\\\\nM_{t-1}=\\beta_1 m_{t-2}+\\left(1-\\beta_1\\right) g_{t-1} \\\\\n\\mathrm{~V}_{\\mathrm{t}-1}=\\beta_2 \\mathrm{v}_{\\mathrm{t}-2}+\\left(1-\\beta_2\\right) \\mathrm{g}_{\\mathrm{t}-1}^2 \\\\\n\\mathrm{U}_{\\mathrm{t}-1}=\\max \\left(\\beta_2 \\cdot \\mathrm{v}_{\\mathrm{t}-1},\\left|\\mathrm{~g}_{\\mathrm{t}}\\right|\\right)\n\\]\ndonde \\(m_{t-1}\\) y \\(V_{t-1}\\) son estimaciones del primer y segundo momento de los gradientes respectivamente, y \\(\\beta_1\\) y \\(\\beta_2\\) parámetros a asignar.\n\\[\n\\widehat{M}_{t-1}=\\frac{m_{t-1}}{1-\\beta_1^{t-1}} \\\\\nW_t=w_{t-1}-\\frac{\\alpha}{u_{t-1}} \\widehat{m}_{t-1}\n\\]\nNadam (Nesterov-accelerated Adaptive Moment Estimatio) Combina Adam y NAG.\n\\[\n\\begin{aligned} G_t & =\\nabla_{w_t} \\mathcal{J}\\left(w_t\\right) \\\\ M_{t-1} & =\\gamma m_{t-2}+\\alpha g_{t-1} \\\\ w_t & =w_{t-1}-m_{t-1}\\end{aligned}\n\\]\n\n\n1.3.5 Función de activación\nLas funciones de activación dentro de una red neuronal son uno de los elementos clave en el diseño de la misma. Cada tipo de función de activación podrá ayudar a la convergencia de forma más o menos rápida en función del tipo de problema que se plantee. En una red neuronal las funciones de activación en las capas ocultas van a conseguir establecer las restricciones no lineales al pasar de una capa a la siguiente, normalmente se evita usar la función de activación lineal en las capas intermedias ya que queremos conseguir transformaciones no lineales.\nA continuación, exponemos las principales funciones de activación en las capas ocultas:\n\nPaso binario (Usado por los primeros modelos de neuronas)\n\n\\(F(x)= \\begin{cases}0 & \\text { for } x \\leq 0 \\\\ x & \\text { for } x&gt;0\\end{cases}\\)\n\nIdentidad\n\n\\(F(x)=x\\)\n\nSigmoid (Logística)\n\n\\(F(x)=\\frac{1}{1+e^{-x}}\\)\n\nTangente Hiperbólica (Tanh)\n\n\\(F(x)=\\tanh (x)=\\frac{\\left(e^x-e^{-x}\\right)}{\\left(e^x+e^{-x}\\right)}\\)\n\nSoftmax\n\n\\(F\\left(x_i\\right)=\\frac{e^{x_i}}{\\sum_{j=0}^k e^{x_j}}\\)\n\nReLu ( Rectified Linear Unit) \\(\\begin{aligned} & F(x)=\\max (0, x) \\\\ & f(x)= \\begin{cases}0 & \\text { for } x \\leq 0 \\\\ x & \\text { for } x&gt;0\\end{cases} \\end{aligned}\\)\nLReLU (Leaky Rectified Linear Unit) \\(F(\\alpha, x)= \\begin{cases}\\alpha x & \\text { for } x&lt;0 \\\\ x & \\text { for } x \\geq 0\\end{cases}\\)\nPReLU (Parametric Rectified Linear Unit) \\(F(\\alpha, x)= \\begin{cases}\\alpha x & \\text { for } x&lt;0 \\\\ x & \\text { for } x \\geq 0\\end{cases}\\)\nRReLU (Randomized Rectified Linear Unit) \\(F(\\alpha, x)= \\begin{cases}\\alpha x & \\text { for } x&lt;0 \\\\ x & \\text { for } x \\geq 0\\end{cases}\\)\n\n*La diferencia entre LReLu, PReLu y RRLeLu es que en LReLu el parámetro es uno que se asigna fijo, en el caso de PReLu el parámetro también se aprende durante el entrenamiento y finalmente en RReLu es un parámetro con valores entre 0 y 1, que se obtiene de un muestreo en una distribución normal.\nSe puede profundizar en este grupo de funciones de activación en (Xu et al. 2015)\n\nELU (Exponential Linear Unit) \\(F(\\alpha, x)= \\begin{cases}\\alpha\\left(e^{x-1}\\right) & \\text { for } x&lt;0 \\\\ x & \\text { for } x \\geq 0\\end{cases}\\)\n\n\n\n\n\n\n\nFigure 1.1: Funciones ReLU\n\n\n\nFunción de activación en salida\nEn la capa de salida tenemos que tener en cuenta cual es el tipo de datos final que queremos obtener, y en función de eso elegiremos cual es la función de activación de salida que usaremos. Normalmente las funciones de activación que se usarán en la última capa seran:\n\nLineal con una unidad, para regresión de un solo dato numérico \\(F(x)=x\\) donde es un valor escalar.\nLineal con multiples unidades, para regresión de varios datos numéricos \\(F(x)=x\\) donde \\(x\\) es un vector.\nSigmoid para clasifiación binaria \\(F(x)=\\frac{1}{1+e^{-x}}\\)\nSoftmax para calsifiación múltiple \\(F\\left(x_i\\right)=\\frac{e^{x_i}}{\\sum_{j=0}^k e^{x_j}}\\)\n\n\n\n1.3.6 Regularización\nLas técnicas de regularización nos permiten conseguir mejorar los problemas que tengamos por sobreajuste en el entrenamiento de nuestra red neuronal.\nA continuación, vemos algunas de las técnicas de regularización existentes en la actualidad:\n\nNorma LP Básicamente estos métodos tratan de hacer que los pesos de las neuronas tengan valores muy pequeños consiguiendo una distribución de pesos más regular. Esto lo consiguen al añadir a la función de pérdida un coste asociado a tener pesos grandes en las neuronas. Este peso se puede construir o bien con la norma L1 (proporcional al valor absoluto) o con la norma L2 (proporcional al cuadrado de los coeficientes de los pesos). En general se define la norma LP) \\[\n\\begin{gathered}\nE(w, \\mathbf{y}, \\hat{\\mathbf{y}})=\\mathcal{L}(w, \\mathbf{y}, \\hat{\\mathbf{y}})+\\lambda R(w) \\\\\nR(w)=\\sum_j\\left\\|w_j\\right\\|_p^p\n\\end{gathered}\n\\]\n\nPara los casos más habituales tendríamos la norma \\(\\mathbf{L 1}\\) y \\(\\mathbf{L 2}\\). \\[\n\\begin{aligned}\n& R(w)=\\sum_j\\left\\|w_j\\right\\|^2 \\\\\n& R(w)=\\sum_j\\left|w_j\\right|\n\\end{aligned}\n\\]\n\n\n1.3.7 Dropout\nUna de las técnicas de regularización que más se están usando actualmente es la llamada Dropout, su proceso es muy sencillo y consiste en que en cada iteración de forma aleatoria se dejan de usar un porcentaje de las neuronas de esa capa, de esta forma es más difícil conseguir un sobreajuste porque las neuronas no son capaces de memorizar parte de los datos de entrada.\n\n\n1.3.8 Dropconnect\nEl Dropconnect es otra técnica que va un poco más allá del concepto de Dropout y en lugar de usar en cada capa de forma aleatoria una serie de neuronas, lo que se hace es que de forma aleatoria se ponen los pesos de la capa a cero. Es decir, lo que hacemos es que hay ciertos enlaces de alguna neurona de entrada con alguna de salida que no se activan.\n\n\n1.3.9 Inicialización de pesos\nCuando empieza el entrenamiento de una red neuronal y tiene que realizar la primera pasada hacia delante de los datos, necesitamos que la red neuronal ya tenga asignados algún valor a los pesos.\nSe pueden hacer inicializaciones del tipo:\n\nCeros Todos los pesos se inicializan a 0.\nUnos Todos los pesos se inicializan a 1.\nDistribución normal. Los pesos se inicializan con una distribución normal, normalmente con media 0 y una desviación alrededor de 0,05. Es decir, valores bastante cercanos al cero.\nDistribución normal truncada. Los pesos se inicializan con una distribución normal, normalmente con media 0 y una desviación alrededor de 0,05 y además se truncan con un máximo del doble de la desviación. Los valores aun són más cercanos a cero.\nDistribución uniforme. Los pesos se inicializan con una distribución uniforme, normalmente entre el 0 y el 1.\nGlorot Normal (También llamada Xavier normal) Los pesos se inicializan partiendo de una distribución normal truncada en la que la desivación es donde es el número de unidades de entrada y fanout es el número de unidades de salida. Ver (Glorot and Bengio 2010)\nGlorot Uniforme (También llamada Xavier uniforme) Los pesos se inicializan partiendo de una distribución uniforme donde los límites son \\([-\\) limit,+ limit \\(]\\) done limit \\(=\\sqrt{\\frac{6}{\\text { fanin }+ \\text { fanout }}}\\) done \\(fanin\\) y es el número de unidades de entrada y \\(fanout\\) es el número de unidades de salida. Ver (Glorot and Bengio 2010)\n\n\n\n1.3.10 Batch normalization\nHemos comentado que cuando entrenamos una red neuronal los datos de entrada deben ser todos de tipo numérico y además los normalizamos para tener valores “cercanos a cero”, teniendo una media de 0 y varianza de 1, consiguiendo uniformizar todas las variables y conseguir que la red pueda converger más fácilmente.\nCuando los datos entran a la red neuronal y se comienza a operar con ellos, se convierten en nuevos valores que han perdido esa propiedad de normalización. Lo que hacemos con la normalización por lotes (batch normalization) (Ioffe and Szegedy 2015) es que añadimos un paso extra para normalizar las salidas de las funciones de activación. Lo normal es que se aplicara la normalización con la media y la varianza de todo el bloque de entrenamiento en ese paso, pero normalmente estaremos trabajando por lotes y se calculará la media y varianza con ese lote de datos.\n\n\n1.3.11 Ejemplo de Red Neuronal con Keras\n\n# Importamos las librerías de keras/tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Importamos la librería de los datasets de keras y cogemos el de boston_housing\nfrom tensorflow.keras.datasets import boston_housing\n\n# Obtenemos los datos de entrenamiento y test\n# separados en las variables explicativas y la objetivo\n(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\ntrain_data.shape\ntest_data.shape\n\n# Realizamos la \"Normalización\" restando la media y dividiendo por la desviación típica\n# Ahora tendremos valores (-x,x) alredor de 0, pero en general pequeños\nmean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data /= std\ntest_data -= mean\ntest_data /= std\n\n# Creamos el modelo\n\n# Inicializamos el API Secuencial de capas\nmodel = keras.Sequential([\n        # Añadimos capa de entrada con las 13 variables explicativas\n        keras.Input(shape=(13,)),\n        # Añadimos capa densamente conectada con 64 neuronas y activación relu\n        layers.Dense(64, activation=\"relu\"),\n        # Añadimos capa densamente conectada con 64 neuronas y activación relu\n        layers.Dense(64, activation=\"relu\"),\n        # Añadimos capa de salida densamente conectada con 1 neurona y activación lineal (para regresión)\n        layers.Dense(1)\n    ])\n\n# Mostramos el Modelo creado\nmodel.summary()\n\n# Compilamos el modelo definiendo el optimizador, función de pérdida y métrica\n# RMSProp, mse, mae\nmodel.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n\n\n\n# Realizamos el entrenamiento\n# 130 épocos (iteraciones), con tamaño de batch de 16\nhistory = model.fit(train_data, train_targets,\n          epochs=130, batch_size=16, verbose=0)\n\n\n# Importamos la librería de pyplot para pintar gráficas\nimport matplotlib.pyplot as plt\n\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['mae'])\n#plt.plot(history.history['val_mae'])\nplt.title('model mae')\nplt.ylabel('mae')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n\n# Evaluamos el modelo con los datos de test\npredictions = model.predict(test_data)\npredictions[0]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "capitulo1.html#redes-neuronales-convolucionales",
    "href": "capitulo1.html#redes-neuronales-convolucionales",
    "title": "1  Deep Learning",
    "section": "1.4 Redes Neuronales Convolucionales",
    "text": "1.4 Redes Neuronales Convolucionales\n\n1.4.1 Introducción\nEsta arquitectura de redes de neuronas convolucionales, CNN, Convolutional Neural Networks es en la actualidad el campo de investigación más fecundo dentro de las redes neuronales artificiales de Deep learning y donde los investigadores, empresas e instituciones están dedicando más recursos e investigación. Para apoyar esta aseveración, en google trend se observa que el término convolutional neural network en relación con el concepto de artificial neural network crece y está por encima desde el año 2016. Es en este último lustro donde el Deep learning ha tomado una importancia considerable.\n\n\n\n\n\n\nFigure 1.2: búsqueda de términos de redes neuronales en google trend\n\n\n\nLas redes convolucionales son actualmente utilizadas para diferentes propósitos: tratamiento de imágenes(visión por computador, extracción de características, segmentación, etc.), generación y clasificación de texto(o audio), predicción de series temporales, etc. En este capítulo veremos su aplicación en clasificación de imágenes y de texto.\n\n\n1.4.2 Clasificación de imágenes\nEn este modelo de redes convolucionales las neuronas se corresponden a campos receptivos similares a las neuronas en la corteza visual de un cerebro humano. Este tipo de redes se han mostrado muy efectivas para tareas de detección y categorización de objetos y en la clasificación y segmentación de imágenes. Por ejemplo, estas redes en la década de 1990 las aplicó AT & T para desarrollar un modelo para la lectura de cheques. También más tarde se desarrollaron muchos sistemas OCR basados en CNN. En esta arquitectura cada neurona de una capa no recibe conexiones entrantes de todas las neuronas de la capa anterior, sino sólo de algunas. Esta estrategia favorece que una neurona se especialice en una región del conjunto de números (píxeles) de la capa anterior, lo que disminuye notablemente el número de pesos y de operaciones a realizar. Lo más normal es que neuronas consecutivas de una capa intermedia se especialicen en regiones solapadas de la capa anterior.\nUna forma intuitiva para entender cómo trabajan estas redes neuronales es ver cómo nos representamos y vemos las imágenes. Para reconocer una cara primero tenemos que tener una imagen interna de lo que es una cara. Y a una imagen de una cara la reconocemos porque tiene nariz, boca, orejas, ojos, etc. Pero en muchas ocasiones una oreja está tapada por el pelo, es decir, los elementos de una cara se pueden ocultar de alguna manera. Antes de clasificarla, tenemos que saber la proporción y disposición y también cómo se relacionan la partes entre sí.\nPara saber si las partes de la cara se encuentran en una imagen tenemos que identificar previamente líneas bordes, formas, texturas, relación de tamaño, etcétera. En una red convolucional, cada capa lo que va a ir aprendiendo son los diferentes niveles de abstracción de la imagen inicial. Para comprender mejor el concepto anterior hemos seleccionado esta imagen de Raschka y Mirjalili (2019) donde se observa como partes del perro se transforman en neuronas del mapa de características\n\n\n\n\n\n\nFigure 1.3: Correspondencia de zonas de la imagen y mapa de características\n\n\n\nEl objetivo de las redes CNN es aprender características de orden superior utilizando la operación de convolución.\nPuesto que las redes neuronales convolucionales pueden aprender relaciones de entrada-salida (donde la entrada es una imagen en este caso), en la convolución, cada pixel de salida es una combinación lineal de los pixeles de entrada.\nLa convolución consiste en filtrar una imagen utilizando una máscara. Diferentes máscaras producen distintos resultados. Las máscaras representan las conexiones entre neuronas de capas anteriores. Estas capas aprenden progresivamente las características de orden superior de la entrada sin procesar.\nLas redes neuronales convolucionales se forman usando dos tipos de capas: convolucionales y pooling. La capa de convolución transforma los datos de entrada a través de una operación matemática llamada convolución. Esta operación describe cómo fusionar dos conjuntos de información diferentes. A esta operación se le suele aplicar una función de transformación, generalmente la RELU. Después de la capa o capas de convolución se usa una capa de pooling, cuya función es resumir las respuestas de las salidas cercanas. Antes de obtener el output unimos la última capa de pooling con una red densamente conectada. Previamente se ha aplanado (Flatering) la última capa de pooling para obtener un vector de entrada a la red neural final que nos ofrecerá los resultados.\n\n\n\n\n\n\nFigure 1.4: Arquitectura de una CNN\n\n\n\nLas redes neuronales convolucionales debido a su forma de concebirse son aptas para poder aprender a clasificar todo tipo de datos donde éstos estén distribuidos de una forma continua a lo largo del mapa de entrada, y a su vez sean estadísticamente similares en cualquier lugar del mapa de entrada. Por esta razón, son especialmente eficaces para clasificar imágenes. También pueden ser aplicadas para la clasificación de series de tiempo o señales de audio.\nEn relación con el color y la forma de codificarse, en las redes convolucionales se realiza en tensores 3D, dos ejes para el ancho (width) y el alto (height) y el otro eje llamado de profundidad (depht) que es el canal del color con valor tres si trabajamos con imágenes de color RGB (Red, Green y Blue) rojo, verde y azul. Si disponemos de imágenes en escala de grises el valor de depht es uno. La base de datos MNIST (National Institute of Standards and Technology database) con la que trabajaremos en este epígrafe contiene imágenes de 28 x 28 pixeles, los valores de height y de widht son ambos 28, y al ser una base de datos en blanco y negro el valor de depht es 1.\nLas imágenes son matrices de píxeles que van de cero a 255 y que para la red neuronal se normalizan para que sus valores oscilen entre cero y uno.\n\n1.4.2.1 Convolución\nEn las redes convolucionales todas las neuronas de la capa de entrada (los píxeles de las imágenes) no se conectan con todas las neuronas de la capa oculta del primer nivel como lo hacen las redes clásicas del tipo perceptrón multicapa o las redes que conocemos de forma genérica como redes densamente conectadas. Las conexiones se realizan por pequeñas zonas de la capa de entrada.\n\n\n\n\n\n\nFigure 1.5: Conexión de las neuronas de la capa de entrada con la capa oculta\n\n\n\nVeamos un ejemplo para la base de datos de los dígitos del 1 a 9. Vamos a conectar cada neurona de la capa oculta con una región de 5 x 5 neurona, es decir, con 25 neuronas de la capa de entrada, que podemos denominarla ventana. Esta ventana va a ir recorriendo todo el espacio de entrada de 28 x 28 empezando por arriba y desplazándose de izquierda a derecha y de arriba abajo. Suponemos que los desplazamientos de la ventana son de un paso (un pixel) aunque este es un parámetro de la red que podemos modificar (en la programación lo llamaremos stride).\nPara conectar la capa de entrada con la de salida utilizaremos una matriz de pesos (W) de tamaño 3 x 3 que recibe el nombre de filtro (filter) y el valor del sesgo. Para obtener el valor de cada neurona de la capa oculta realizaremos el producto escalar entre el filtro y la ventana de la capa de entrada. Utilizamos el mismo filtro para obtener todas las neuronas de la capa oculta, es decir en todos los productos escalares siempre utilizamos la misma matriz, el mismo filtro.\nSe definen matemáticamente estos productos escalares a través de la siguiente expresión:\n\\[\nY=X * W \\rightarrow Y[i, j]=\\sum_{k_1=-\\infty}^{+\\infty} \\sum_{k_2=-\\infty}^{+\\infty} X\\left[i-k_1, j-k_2\\right] W\\left[k_1, k_2\\right]\n\\]\n\n\n\n\n\n\nFigure 1.6: Convolución\n\n\n\nComo en este tipo de red un filtro sólo nos permite revelar una característica muy concreta de la imagen, lo que se propone es usar varios filtros simultáneamente, uno para cada característica que queramos detectar. Una forma visual de representarlo (si suponemos que queremos aplicar 32 filtros) es como se muestra a continuación:\n\n\n\n\n\n\nFigure 1.7: Primera capa de la red convolucional con 32 filtros\n\n\n\nAl resultado de la aplicación de los diferentes filtros se les suele aplicar la función de activación denominada RELU y que ya se comentó en la introducción.\nUna interesante fuente de información es la documentación del software gratuito GIMP donde expone diferentes efectos que se producen en las imágenes al aplicar diversas convoluciones.\nUn ejmplo claro y didáctico lo podemos obtener de la docuemntación del software libre de dibujo y tratamiento de imágenes denominado GIMP (https://docs.gimp.org/2.6/es/plug-in-convmatrix.html). Algunos de estos efectos nos ayudan a entender la operación de los filtros en las redes convolucionales y cómo afectan a las imágenes, en concreto, el ejemplo que presenta lo realiza sobre la figura del Taj Mahal.\nEl filtro enfocar lo que consigue es afinar los rasgos, los contornos lo que nos permite agudizar los objetos de la imagen. Toma el valor central de la matriz de cinco por cinco lo multiplica por cinco y le resta el valor de los cuatro vecinos. Al final hace una media, lo que mejora la resolución del pixel central porque elimina el ruido o perturbaciones que tiene de sus pixeles vecinos.\nEl filtro enfocar (Sharpen)\n\n\n\n\n\n\nFigure 1.8: Filtro Enfocar\n\n\n\nLo contario al filtro enfocar lo obtenemos a través de la matriz siguiente, difuminando la imagen al ser estos píxeles mezclados o combinados con los pixeles cercanos. Promedia todos los píxeles vecinos a un pixel dado lo que implica que se obtienen bordes borrosos.\nFiltro desenfocar\n\n\n\n\n\n\nFigure 1.9: Filtro DesEnfocar\n\n\n\nFiltro Detectar bordes (Edge Detect)\nEste efecto se consigue mejorando los límites o las aristas de la imagen. En cada píxel se elimina su vecino inmediatamente anterior en horizontal y en vertical. Se eliminan las similitudes vecinas y quedan los bordes resaltados. Al pixel central se le suman los cuatro píxeles vecinos y lo que queda al final es una medida de cómo de diferente es un píxel frente a sus vecinos. En el ejemplo, al hacer esto da un valor de cero de ahí que se observen tantas zonas oscuras.\n\n\n\n\n\n\nFigure 1.10: Filtro Detectar Bordes\n\n\n\nFiltro Repujado (Emboss)\nEn este filto se observa que la matriz es simétrica y lo que intenta a través del diseño del filtro es mejorar los píxeles centrales y de derecha abajo restándole los anteriores. Se obtiene lo que en fotografía se conoce como un claro oscuro. Trata de mejorar las partes que tienen mayor relevancia.\n\n\n\n\n\n\nFigure 1.11: Filtro Emboss\n\n\n\n\n\n1.4.2.2 Pooling\nCon la operación de pooling se trata de condensar la información de la capa convolucional. A este procedimiento también se le conoce como submuestreo.\nEs simplemente una operación en la que reducimos los parámetros de la red. Se aplica normalmente a través de dos operaciones: max-pooling y mean-pooling, que también es conocido como average-pooling. Tal y como se observa en la imagen siguiente, desde la capa de convolución se genera una nueva capa aplicando la operación a todas las agrupaciones, donde previamente hemos elegido el tamaño de la región; en la figura siguiente es de tamaño 2, con lo que pasamos de un espacio de 24 x 24 neuronas a la mitad, 12 x 12 en la capa de pooling.\n\n\n\n\n\n\nFigure 1.12: Etapa de pooling de tamaño 2 x 2\n\n\n\nVamos a estudiar el pooling suponiendo que tenemos una imagen de 5 x 5 píxeles y que queremos efectuar una agrupación max-pooling. Es la más utilizada, ya que obtiene buenos resultados. Observamos los valores de la matriz y se escoge el valor máximo de los cuatro bloques de matrices de dos por dos. Max Pooling\nEn la agrupación Average Pooling la operación que se realiza es sustituir los valores de cada grupo de entrada por su valor medio. Esta transformación es menos utilizada que el max-pooling.\nLa transformación max-pooling presenta un tipo de invarianza local: pequeños cambios en una región local no varían el resultado final realizado con el max – pooling: se mantiene la relación espacial. Para ilustrar este concepto hemos escogido la imagen que presenta Torres (2020) donde se ilustra como partiendo de una matriz de 12 x 12 que representa al número 7, al aplicar la operación de max-pooling con una ventana de 2 x 2 se conserva la relación espacial.\n\n\n\n\n\n\nFigure 1.13: Max Pooling\n\n\n\n\n\n\n\n\n\nFigure 1.14: Average Pooling\n\n\n\n\n\n\n\n\n\nFigure 1.15: Mantenimiento del pooling con la transformación\n\n\n\n\n\n1.4.2.3 Padding\nPara explicar el concepto del Padding vamos a suponer que tenemos una imagen de 5 x 5 píxeles, es decir 25 neuronas en la capa de entrada, y que elegimos, para realizar la convolución, una ventana de 3 x 3. El número de neuronas de la capa oculta resultará ser de nueve. Enumeramos los píxeles de la imagen de forma natural del 1 al 25 para que resulte más sencillo de entender.\n\n\n\n\n\n\nFigure 1.16: Operación de convolución con una ventana de 3 x 3\n\n\n\nPero si queremos obtener un tensor de salida que tenga las mismas dimensiones que la entrada podemos rellenar la matriz de ceros antes de deslizar la ventana por ella. Vemos la figura siguiente donde ya se ha rellenado de valores cero y obtenemos, después de deslizar la ventana de 3 x3 de izquierda a derecha y de arriba abajo, las veinticinco matrices de la figura nº 71\n\n\n\n\n\n\nFigure 1.17: Imagen con relleno de ceros\n\n\n\n\n\n\n\n\n\nFigure 1.18: Operación de convolución con ventana 3 x 3 y padding\n\n\n\n\n\n1.4.2.4 Stride\nHasta ahora, la forma de recorrer la matriz a través de la ventana se realiza desplazándola de un solo paso, pero podemos cambiar este hiperparámetro conocido como stride. Al aumentar el paso se decrementa la información que pasará a la capa posterior. A continuación, se muestra el resultado de las cuatro matrices que obtenemos con un stride de valor 3.\n\n\n\n\n\n\nFigure 1.19: Operación de convolución con una ventana de 3 x 3 y stride 2\n\n\n\nFinalmente, para resumir, una red convolucional contiene los siguientes elementos:\n\nEntrada: Son el número de pixeles de la imagen. Serán alto, ancho y profundidad. Tenemos un solo color (escala de grises) o tres: rojo, verde y azul.\nCapa de convolución: procesará la salida de neuronas que están conectadas en «regiones locales» de entrada (es decir pixeles cercanos), calculando el producto escalar entre sus pesos (valor de pixel) y una pequeña región a la que están conectados. En este epígrafe se presentan las imágenes con 32 filtros, pero puede realizarse con la cantidad que deseemos.\nCapa RELU Se aplicará la función de activación en los elementos de la matriz.\nPooling (agrupar) o Submuestreo: Se procede normalmente a una reducción en las dimensiones alto y ancho, pero se mantiene la profundidad.\nCapa tradicional. Se finalizará con la red de neuronas feedforward (Perceptrón multicapa que se denomina normalmente como red densamente conectada) que vinculará con la última capa de subsampling y finalizará con la cantidad de neuronas que queremos clasificar. En el gráfico siguiente se muestran todas las fases de una red neuronal convolucional.\n\n\n\n\n\n\n\nFigure 1.20: Operación de convolución completa\n\n\n\n\n\n1.4.2.5 Redes convolucionales con nombre propio\nExisten en la actualidad muchas arquitecturas de redes neuronales convolucionales que ya están preparadas, probadas, disponibles e incorporadas en el software de muchos programas como Keras y Tensorflow.\nVamos a comentar algunos de estos modelos, bien por ser los primeros, o por sus excelentes resultados en concursos como el ILSVRC (Large Scale Visual Recognition Challenge).\nEstas estructuras merecen atención dado que son excelentes para estudiarlas e incorporarlas por su notable éxito. El ILSVRC fue un concurso celebrado de 2011 a 2016 de donde nacieron las principales aportaciones efectuadas en las redes convolucionales. Este concurso fue diseñado para estimular la innovación en el campo de la visión computacional. Actualmente se desarrollan este tipo de concursos a través de la plataforma web: https://www.kaggle.com/ Para ver más prototipos de redes convolucionales y los últimos avances y consejos sobre las redes convolucionales se puede consultar el siguiente artículo “Recent Advances in Convolutional Neural Networks” de Jiuxiang. G. et al. (2019)\nLos cinco modelos más destacados hasta el año 2017 son los siguientes: LeNet-5, Alexnet, GoogLeNet, VGG y Restnet.\n\nLeNet-5. Este modelo de Yann LeCun de los años 90 consiguió excelentes resultados en la lectura de códigos postales consta de imágenes de entrada de 32 x 32 píxeles seguida de dos etapas de convolución – pooling, una capa densamente conectada y una capa softmax final que nos permite conocer los números o las imágenes.\nAlexNet. Fue la arquitectura estrella a partir del año 2010 en el ILSVRC y popularizada en el documento de 2012 de Alex Krizhevsky, et al. titulado”Clasificación de ImageNet con redes neuronales convolucionales profundas”. Podemos resumir los aspectos clave de la arquitectura relevantes en los modelos modernos de la siguiente manera: • Empleo de la función de activación ReLU después de capas convolucionales y softmax para la capa de salida. • Uso de la agrupación máxima en lugar de la agrupación media. • Utilización de la regularización de Dropout entre las capas totalmente conectadas. • Patrón de capa convolucional alimentada directamente a otra capa convolucional. • Uso del aumento de datos (Data Aumentation,)\nVGG. Este prototipo fue desarrollado por un grupo de investigación de Geometría Visual en Oxford. Obtuvo el segundo puesto en la competición del año 2014 del ILSVRC. Las aportaciones principales de la investigación se pueden encontrar en el documento titulado “ Redes convolucionales muy profundas para el reconocimiento de imágenes a gran escala ” desarrollado por Karen Simonyan y Andrew Zisserman. Este modelo contribuyó a demostrar que la profundidad de la red es una componente crítica para alcanzar unos buenos resultados. Otra diferencia importante con los modelos anteriores y que actualmente es muy utilizada es el uso de un gran número de filtros y de tamaño reducido. Estas redes emplean ejemplos de dos, tres e incluso cuatro capas convolucionales apiladas antes de usar una capa de agrupación máxima. En esta arquitectura el número de filtros aumenta con la profundidad del modelo. El modelo comienza con 64 y aumenta a través de los filtros de 128, 256 y 512 al final de la parte de extracción de características del modelo. Los investigadores evaluaron varias variantes de la arquitectura si bien en los programas sólo se hace referencia a dos de ellas que son las que aportan un mayor rendimiento y que son nombradas por las capas que tienen: VGG-16 y VGG-19.\nGoogLeNet. GoogLeNet fue desarrolado por investigadores de Google Research. de Google, que con su módulo denominado de inception reduce drásticamente los parámetros de la red (10 veces menos que AlexNet) y de ella han derivado varias versiones como la Inception-v4. Esta arquitectura ganó la competición en el año 2014 y su éxito se debió a que la red era mucho más profunda (muchas más capas) y como ya se ha indicado introdujeron en el modelo las subredes llamadas inception. Las aportaciones principales en el uso de capas convolucionales fueron propuestos en el documento de 2015 por Christian Szegedy, et al. titulado “ Profundizando con las convoluciones ”. Estos autores introducen una arquitectura llamada “inicio” y un modelo específico denominado GoogLenet. El módulo inicio es un bloque de capas convolucionales paralelas con filtros de diferentes tamaños y una capa de agrupación máxima de 3 × 3, cuyos resultados se concatenan. Otra decisión de diseño fundamental en el modelo inicial fue la conexión de la salida en diferentes puntos del modelo que lograron realizar con la creación de pequeñas redes de salida desde la red principal y que fueron entrenadas para hacer una predicción. La intención era proporcionar una señal de error adicional de la tarea de clasificación en diferentes puntos del modelo profundo para abordar el problema de los gradientes de fuga.\nRed Residual o ResNet. Esta arquitectura gano la competición de 2015 y fue creada por el grupo de investigación de Microsoft. Se puede ampliar la información en He, et al. en su documento de 2016 titulado “ Aprendizaje profundo residual para el reconocimiento de la imagen ”. Esta red es extremadamente profunda con 152 capas, confirmando al pasar los años que las redes son cada vez más profundas, más capas, pero con menos parámetros que estimar. La cuestión clave del diseño de esta red es la incorporación de la idea de bloques residuales que hacen uso de conexiones directa. Un bloque residual, según los autores, “es un patrón de dos capas convolucionales con activación ReLU donde la salida del bloque se combina con la entrada al bloque, por ejemplo, la conexión de acceso directo” Otra clave, en este caso para el entrenamiento de la red tan profunda es lo que llamaron skip connections que implica que la señal con la que se alimenta una capa también se agregue a una capa que se encuentre más adelante. Resumiendo, las tres principales aportaciones de este modelo son:\n\nEmpleo de conexiones de acceso directo.\nDesarrollo y repetición de los bloques residuales.\nModelos muy profundos (152 capas) Aunque se encuentran otros modelos que también son muy populares con 34, 50 y 101 capas.\n\n\nUna buena parte de los modelos comentados se incluyen en la librería de Keras y se pueden encontrar en la siguiente dirección de internet: https://keras.io/api/applications/ Según los autores del programa Keras: “Las aplicaciones Keras son modelos de aprendizaje profundo que están disponibles junto con pesos preentrenados. Estos modelos se pueden usar para predicción, extracción de características y ajustes. Los pesos se descargan automáticamente cuando se crea una instancia de un modelo. Se almacenan en ~ / .keras / models /. Tras la creación de instancias, los modelos se construirán de acuerdo con el formato de datos de imagen establecido en su archivo de configuración de Keras en ~ / .keras / keras.json. Por ejemplo, si ha configurado image_data_format = channel_last, cualquier modelo cargado desde este repositorio se construirá de acuerdo con la convención de formato de datos TensorFlow,”Altura-Ancho-Profundidad”.\n\n\n\n\n\n\nFigure 1.21: Modelos preentrenados en Keras\n\n\n\n\n\n1.4.2.6 Ejemplos de Redes Convolucionales con keras\nRed Convolucional con imágenes importadas a memoria\nEn este ejemplo vamos a usar imágenes que vienen preparadas dentro de un array de datos que se carga directamente en memoria.\n\n# Importamos las librerías de keras/tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Importamos la librería de los datasets de keras y cogemos el de mnist\nfrom tensorflow.keras.datasets import mnist\n\n# Obtenemos los datos de entrenamiento y test\n# separados en las imagenes y las etiquetas de las mismas\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\n# Reestructuramos los datos de las imágenes para que se traten como imagen\ntrain_images = train_images.reshape((60000, 28, 28, 1))\n# Dividimos entre 255 para \"normalizar\" el dato y dejarlo entre 0 y 1\ntrain_images = train_images.astype(\"float32\") / 255\n# Reestructuramos los datos de las imágenes para que se traten como imagen\ntest_images = test_images.reshape((10000, 28, 28, 1))\n# Dividimos entre 255 para \"normalizar\" el dato y dejarlo entre 0 y 1\ntest_images = test_images.astype(\"float32\") / 255\n\n\n# Creamos el modelo\n# Capa de entrada formato 28x28 pixels y sólo un canal de color (escala de grises\ninputs = keras.Input(shape=(28, 28, 1))\n\n# Añadimos capa de convolución con 32 filtros de tamaño 3 y activación relu\nx = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n# Añadimos capa de pooling, tipo max y de tamaño 2\nx = layers.MaxPooling2D(pool_size=2)(x)\n# Añadimos capa de convolución con 64 filtros de tamaño 3 y activación relu\nx = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n# Añadimos capa de pooling, tipo max y de tamaño 2\nx = layers.MaxPooling2D(pool_size=2)(x)\n# Añadimos capa de convolución con 128 filtros de tamaño 3 y activación relu\nx = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n\n# Aplanamos los datos\nx = layers.Flatten()(x)\n\n# Ponemos una capa densamente conectada\nx = layers.Dense(512, activation=\"relu\")(x)\n\n# La salida la hacemos de tipo softmax con 10 neuronas (números de clases diferentes)\noutputs = layers.Dense(10, activation=\"softmax\")(x)\n\n# Construimos el modelo de la Red Neuronal Convolucional\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\n# Mostramos el Modelo creado\nmodel.summary()\n\n# Compilamos el modelo definiendo el optimizador, función de pérdida y métrica\n# RMSProp, sparse_categorical_crossentropy, accuracy\nmodel.compile(optimizer=\"rmsprop\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"])\n\n# Realizamos el entrenamiento\n# 5 épocos (iteraciones), con tamaño de batch de 64\nhistory = model.fit(train_images, train_labels, epochs=5, batch_size=64)\n\n\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\n# Evaluamos el modelo con los datos de test\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\nprint(f\"Test accuracy: {test_acc:.3f}\")\nRed Convolucional con imágenes importadas desde un directorio\nAhora vamos a ver un ejemplo en el que descargamos las imágenes y las desempaquetamos en un directorio.\n# Importamos las librerías de keras/tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport os\nimport PIL\nimport PIL.Image\nimport tensorflow as tf\nimport tensorflow.keras.datasets as tfds\n\nimport pathlib\ndataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\ndata_dir = tf.keras.utils.get_file(origin=dataset_url,\n                                   fname='flower_photos',\n                                   untar=True)\ndata_dir = pathlib.Path(data_dir)\n\nimage_count = len(list(data_dir.glob('*/*.jpg')))\nprint(image_count)\n\n\nbatch_size = 32\nimg_height = 100\nimg_width = 100\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n\nclass_names = train_ds.class_names\nprint(class_names)\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")\n\nnum_classes = 5\n\n\n# Creamos el modelo\n# Capa de entrada formato 180x180 pixels y 3 canales de color RGB\ninputs = keras.Input(shape=(img_width, img_height, 3))\n\n# Dividimos entre 255 para \"normalizar\" el dato y dejarlo entre 0 7 1\nx = layers.Rescaling(1./255)(inputs),\n# Añadimos capa de convolución con 32 filtros de tamaño 3 y activación relu\nx = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n# Añadimos capa de pooling, tipo max y de tamaño 2\nx = layers.MaxPooling2D(pool_size=2)(x)\n# Añadimos capa de convolución con 64 filtros de tamaño 3 y activación relu\nx = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n# Añadimos capa de pooling, tipo max y de tamaño 2\nx = layers.MaxPooling2D(pool_size=2)(x)\n# Añadimos capa de convolución con 128 filtros de tamaño 3 y activación relu\nx = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n\n# Aplanamos los datos\nx = layers.Flatten()(x)\n\n# Ponemos una capa densamente conectada\nx = layers.Dense(512, activation=\"relu\")(x)\n\n# La salida la hacemos de tipo softmax con 5 neuronas (números de clases diferentes)\noutputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n\n# Construimos el modelo de la Red Neuronal Convolucional\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(optimizer=\"rmsprop\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"])\n\n\nmodel.summary()\n\n\nhistory=model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=3\n)\n\n\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n\n\n\n\n1.4.3 Clasificación de textos\nLas redes convolucionales son actualmente utilizadas para diferentes propósitos: tratamiento de imágenes (visión por computador, extracción de características, segmentación, etc.), generación y clasificación de texto (o audio), predicción de series temporales, etc. En este caso, veremos en detalle un ejemplo de clasificación de texto.\nSe presenta a continuación una aplicación práctica de clasificación de texto multiclase a partir de redes Convolucionales de una dimensión. Para ello, se utiliza una bbdd referida a las reclamaciones de los usuarios ante una entidad bancaria en función del tipo de producto.\nEn primer lugar, se importan las librerías a utililizar y se le el fichero:\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\nEl fichero de trabajo contiene una serie de reclamaciones que no vienen acompañadas con su texto asociado. Se considera que lo más adecuado es excluir tales instancias del dataset de partida.\ndatos = pd.read_csv('C:/DEEP LEARNING/consumer_complaints.csv')\ndatos = datos[['product', 'consumer_complaint_narrative']] # variables de\ninterés\ndatos =\ndatos.dropna(subset=['product','consumer_complaint_narrative']).reset_index\n(drop=True) # registros con texto no informado son eliminados de la muestra\nprint('Tamaño de los datos:', datos.shape)\nTamaño de los datos: (66806, 2)\nsns.countplot(y='product', data=datos, order =\ndatos['product'].value_counts().index)\nplt.xlabel('Reclamaciones'), plt.ylabel('Producto')\nplt.show()\nComo puede verse, se parte de once tipos de productos diferentes; si bien, para varios de ellos el número de reclamaciones no es considerado significativo por el área legal de la entidad. Por ello, y en base a la similitud de los productos, se agrupan las cuatro categorías con un menor número reclamaciones en\n\nPrepaid card: se incluye en la categoría de “Credit card”\nPayday loan: se incluye en la categoría “Bank account or service”\nMoney transfers y Other financial service: forman un grupo conjunto denominado “Money transfers and Other financial service”\n\n# agrupaciones\ndatos['product'] = np.where(datos['product']=='Payday loan', 'Bank account\nor service', datos['product']) # préstamos\ndatos['product'] = np.where(datos['product']=='Prepaid card', 'Credit\ncard', datos['product']) # créditos\ntipo_producto = ['Money transfers','Other financial service'] # otros\nservicios financieros\ndatos['product'] = np.where(datos['product'].isin(tipo_producto), 'Money\ntransfers and Other', datos['product'])\nsns.countplot(y='product', data=datos, order =\ndatos['product'].value_counts().index)\nplt.xlabel('Reclamaciones'), plt.ylabel('Producto')\nplt.show()\nDe esta forma, el número de grupos ha sido distribuido de una forma más equitativa. A modo de ejemplo, se muestra una de las reclamaciones:\ndef plot_reclamaciones(df, elemento):\n    df = df.loc[elemento].to_list()\n    return df\nprint('Producto:', plot_reclamaciones(datos, 100)[0])\nprint('Reclamacion:', plot_reclamaciones(datos, 100)[1])\nComo puede verse, la reclamación 101 del dataset está asociada a un préstamo al consumo. Sin embargo, lo verdaderamente interesante del texto de ejemplo es la necesidad de realizar un preprocesado a los textos puesto que algunos símbolos, caracteres o, incluso palabras, no son relevantes para que la red sea capaz de interpretar el contenido del mismo. Por tanto, se lleva a cabo lo siguiente:\n\nConversión del texto a minúsculas\nExclusión del texto el contenido cifrado (XXXX)\nEliminación de caracteres extraños\nPara poder hacer este preprocesado de textos se hace uso del paquete re de Python.\n\ndef preprocesado(reclamacion):\n    reclamacion = reclamacion.lower() # texto en minúsculas\n    reclamacion = reclamacion.replace('x','') # cambio X por espacio\n    reclamacion = re.compile('[/(){}\\[\\]\\|@,;]').sub('', reclamacion) # símbolos extraños (1)\n    reclamacion = re.compile('[^0-9a-z #+_]').sub('', reclamacion) # símbolos extraños (2)\n    return reclamacion\ndatos['consumer_complaint_narrative'] = datos['consumer_complaint_narrative'].apply(preprocesado) # aplicación de la función\nSe presenta de nuevo el ejemplo anterior para ver el resultado del procesamiento de textos realizado.\nprint('Producto:', plot_reclamaciones(datos, 100)[0])\nprint('Reclamacion:', plot_reclamaciones(datos, 100)[1])\nseed=123\ntf.random.set_seed(seed)\nnp.random.seed(seed)\nX_texto = datos['consumer_complaint_narrative']\nY_label = pd.get_dummies(datos['product']).values # las categorías son\nconvertidas a variable dummy\nX_train_text, X_test_text, Y_train, Y_test =\ntrain_test_split(X_texto,Y_label, test_size = 0.2, random_state = seed)\nprint('Entrenamiento:', X_train_text.shape)\nprint('Test:', X_train_text.shape)\nAntes de definir la arquitectura de la red, se lleva a cabo la conversión del texto a variables numéricas que es el input que puede leer la red. Para ello, se realiza:\n\nLa vectorización del texto asociado a las reclamaciones.\nEl truncamiento y rellenado de las secuencias de entrada para igualar la longitud en la modelización.\n\nMAX_NB_WORDS = 25000 # frecuencia de palabras\nMAX_SEQUENCE_LENGTH = 200 # número de palabras en cada reclamacion\nEMBEDDING_DIM = 150 # dimensión del embedding\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS,\nfilters='!\"#$%&()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(X_train_text.values)\nword_index = tokenizer.word_index\nprint('Tokens:', len(word_index))\n\nX_train = tokenizer.texts_to_sequences(X_train_text.values)\nX_train = tf.keras.preprocessing.sequence.pad_sequences(X_train,\nmaxlen=MAX_SEQUENCE_LENGTH)\nprint('Datos de entrada:', X_train.shape)\nPor último, se crea la red neuronal siguiendo el método funcional. La red tiene las siguientes capas: - Entrada: de 200 neuronas pues corresponde con la longitud de las secuencias - Embedding: de dimensión 200 y toma como input el número máximo de palabras (25.000) - Convolucional: de 64 neuronas - MaxPooling: - Densa: de 32 neuronas y con función de activación “relu” - Salida: capa densa con 8 neuronas (número de categorías del target) y función de activación “softmax”\n# capa de entrada\ninputs = tf.keras.Input(shape=(X_train.shape[1],))\nembedding = tf.keras.layers.Embedding(input_dim=MAX_NB_WORDS,\noutput_dim=EMBEDDING_DIM)(inputs)\ncapa_conv = tf.keras.layers.Conv1D(filters=64,\nkernel_size=3,\npadding='valid',\nactivation='relu')(embedding)\nmax_pooling = tf.keras.layers.GlobalMaxPooling1D()(capa_conv)\ncapa_densa = tf.keras.layers.Dense(units=32,\nactivation='relu',\nkernel_regularizer=tf.keras.regularizers.l2(0.01))(max_pooling)\nout = tf.keras.layers.Dense(units=Y_train.shape[1],\nactivation='softmax')(capa_densa)\nmodelo = tf.keras.Model(inputs=inputs, outputs=out)\nEl summary nos muestra el número de parámetros por capa y el número de parámetros total. Puede verse que el alto número de parámetros viene, principalmente, por la capa de Embedding.\nmodelo.summary()\nLa métrica utilizada para evaluar el desempeño es el accuracy y, como es un problema de clasificación multiclase, como función de pérdida categorical_crossentropy. Por su parte, se emplea Adam para la utilización del algoritmo de propagación del error hacia atrás (parámetros por defecto).\nmodelo.compile(loss='categorical_crossentropy', optimizer='adam',\nmetrics=['accuracy'])\n&lt;&lt;&lt; Para el proceso de entrenamiento de la red destacar:\n\nUn máximo de 10 épocas y actualización de los pesos cada 128 muestras\nReserva del 20% del dataset para ser usado como validación\nUso de parada temprana para recoger el mejor modelo posible en el proceso iterativo\n\nepochs = 10\nbatch_size = 128\nhistory = modelo.fit(X_train, Y_train,\nepochs=epochs,\nbatch_size=batch_size,\nvalidation_split=0.2,\n\ncallbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',\nUna vez realizado el entrenamiento, se visualiza el proceso para conocer su convergencia\n# construcción de un data.frame\ndf_train=pd.DataFrame(history.history)\ndf_train['epochs']=history.epoch\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.plot(df_train['epochs'], df_train['loss'], label='train_loss')\nax1.plot(df_train['epochs'], df_train['val_loss'], label='val_loss')\nax2.plot(df_train['epochs'], df_train['accuracy'], label='train_acc')\nax2.plot(df_train['epochs'], df_train['val_accuracy'], label='val_acc')\nax1.legend()\nax2.legend()\nplt.show()\nFinalmente, se estima la bondad de ajuste con la muestra de test. Para ello, como esta muestra hace referencia a la puesta en producción del modelo, es necesario crear las secuencias de este nuevo dataset en función de la tokenización del modelo.\nX_test = tokenizer.texts_to_sequences(X_test_text)\nX_test = tf.keras.preprocessing.sequence.pad_sequences(X_test,\nmaxlen=MAX_SEQUENCE_LENGTH)\nY ahora ya sí, se realizan las predicciones y se evaluar el performance del modelo creado en la muestra de test.\n# uso de argmax para pasar de probabilidad a estimación final\nY_test_pred = np.argmax(modelo.predict(X_test), axis=1) # predicción de la\netiqueta\nY_test_label = np.argmax(Y_test, axis=1) # obtención de las etiquetas sin\ndummy\nprint('accuracy - test:', np.round(accuracy_score(Y_test_label,\nY_test_pred),5))\nsns.heatmap(confusion_matrix(Y_test_pred, Y_test_label), annot = True,\nfmt='.0f') # matriz de confusión\nplt.title('Matriz de confusión - test')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "capitulo1.html#redes-recurrentes",
    "href": "capitulo1.html#redes-recurrentes",
    "title": "1  Deep Learning",
    "section": "1.5 Redes Recurrentes",
    "text": "1.5 Redes Recurrentes",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "capitulo1.html#redes-recurrentes-elman-jordan-lstm-y-gru",
    "href": "capitulo1.html#redes-recurrentes-elman-jordan-lstm-y-gru",
    "title": "1  Deep Learning",
    "section": "1.6 Redes recurrentes: Elman, Jordan, LSTM y GRU",
    "text": "1.6 Redes recurrentes: Elman, Jordan, LSTM y GRU\n\n1.6.1 Forecasting en series de tiempo\n\n\n1.6.2 Clasificación y generación de textos",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "capitulo1.html#autoencoders",
    "href": "capitulo1.html#autoencoders",
    "title": "1  Deep Learning",
    "section": "1.7 Autoencoders",
    "text": "1.7 Autoencoders\n\n1.7.1 Bases del Autoencoder\nLos Autoencoders (AE) son uno de los tipos de redes neuronales que caen dentro del ámbito del Deep Learning, en la que nos encontramos con un modelo de aprendizaje no supervisado. Ya se empezó a hablar de AE en la década de los 80 (Bourlard and Kamp 1988), aunque es en estos últimos años donde más se está trabajando con ellos.\nLa arquitectura de un AE es una Red Neuronal Artificial (ANN por sus siglas en inglés) que se encuentra dividida en dos partes, encoder y decoder (Charte et al. 2018), (Goodfellow, Bengio, and Courville 2016). El encoder va a ser la parte de la ANN que va codificar o comprimir los datos de entrada, y el decoder será el encargado de regenerar de nuevo los datos en la salida. Esta estructura de codificación y decodificación le llevará a tener una estructura simétrica. El AE es entrenado para ser capaz de reconstruir los datos de entrada en la capa de salida de la ANN, implementando una serie de restricciones (la reducción de elementos en las capas ocultas del encoder) que van a evitar que simplemente se copie la entrada en la salida.\nSi recordamos la estructura de una ANN clásica o también llamada Red Neuronal Densamente Conectada (ya que cada neurona conecta con todas las de la siguiente capa) nos encontramos en que en esta arquitectura, generalmente, el número de neuronas por capa se va reduciendo hasta llegar a la capa de salida que debería ser normalmente un número (si estamos en un problema regresión), un dato binario (si es un problema de clasificación).\nFigura nº 86: Red Neuronal Clasica\nYa se empezó a hablar de AE en la década de los 80 (Bourlard and Kamp 1988), aunque es en estos últimos años donde más se está trabajando con ellos.\nLa arquitectura de un AE es una Red Neuronal Artificial (ANN por sus siglas en inglés) que se encuentra dividida en dos partes, encoder y decoder (Charte et al. 2018), (Goodfellow, Bengio, and Courville 2016). El encoder va a ser la parte de la ANN que va codificar o comprimir los datos de entrada, y el decoder será el encargado de regenerar de nuevo los datos en la salida. Esta estructura de codificación y decodificación le llevará a tener una estructura simétrica. El AE es entrenado para ser capaz de reconstruir los datos de entrada en la capa de salida de la ANN, implementando una serie de restricciones (la reducción de elementos en las capas ocultas del encoder) que van a evitar que simplemente se copie la entrada en la salida.\nSi recordamos la estructura de una ANN clásica o también llamada Red Neuronal Densamente Conectada (ya que cada neurona conecta con todas las de la siguiente capa) nos encontramos en que en esta arquitectura, generalmente, el número de neuronas por capa se va reduciendo hasta llegar a la capa de salida que debería ser normalmente un número (si estamos en un problema regresión), un dato binario (si es un problema de clasificación).\n\n\n\n\n\n\nFigure 1.22: Red Neuronal Clasica\n\n\n\nSi pensamos en una estructura básica de AE en la que tenemos una capa de entrada, una capa oculta y una capa de salida, ésta sería su representación:\n\n\n\n\n\n\nFigure 1.23: Autoencoder Básico\n\n\n\nDonde los valores de son los datos de entrada y los datos son la reconstrucción de los mismos después de pasar por la capa oculta que tiene sólo dos dimensiones. El objetivo del entrenamiento de un AE será que estos valores de sean lo más parecidos posibles a los .\nSegún (Charte et al. 2018) los AE se puden clasificar según el tipo de arquitectura de red en:\n\nIncompleto simple\nIncompleto profundo\nExtra dimensionado simple\nExtra dimensionado profundo\n\n\n\n\n\n\n\nFigure 1.24: Tipos Arquitectura Autoencoders\n\n\n\nCuando hablamos de Incompleto nos referimos a que tenemos una reducción de dimensiones que permite llegar a conseguir una “compresión” de los datos iniciales como técnica para que aprenda los patrones internos. En el caso de Extra dimensionado es cuando subimos de dimensión para conseguir que aprenda esos patrones. En este último caso sería necesario aplicar técnicas de regularización para evitar que haya un sobreajuste en el aprendizaje.\nCuando hablamos de Simple estamos haciendo referencia a que hay una única capa oculta, y en el caso de Profundo es que contamos con más de una capa oculta.\nNormalmente se trabaja con las arquitecturas de tipo Incompleto profundo, sobre todo cuando se está trabajando con tipos de datos que son imágenes. Aunque también podríamos encontrar una combinación de Incompleto con Extra dimensionado profundo cuando trabajamos con tipos de datos que no son imágenes y así crecer en la primera o segunda capa oculta, para luego reducir. Esto nos permitiría por ejemplo adaptarnos a estructuras de AE en las que trabajemos con número de neuronas en una capa que sean potencia de 2, y poder construir arquitecturas dinámicas en función del tamaño de los datos, adaptándolos a un tamaño prefijado.\nA continuación, vemos un gráfico de una estructura mixta Extra dimensionado - Incompleto profundo.\n\n\n\n\n\n\nFigure 1.25: Autoencoder Mixto (Incompleto y Extra dimensionado)\n\n\n\n\n\n1.7.2 Idea intuitiva del uso de Autoencoders\nSi un AE trata de reproducir los datos de entrada mediante un encoder y decoder, ¿que nos puede aportar si ya tenemos los datos de entrada?\nYa hemos comentado que la red neuronal de un AE es simétrica y está formada por un encoder y un decoder, además cuando trabajamos con los AE que son incompletos, se está produciendo una reducción del tamaño de los datos en la fase de codificación y de nuevo una regeneración a partir de esos datos más pequeños al original. Ya tenemos uno de los conceptos más importantes de los AE que es la reducción de dimensiones de los datos de entrada. Estas nuevas variables que se generan una vez pasado el encoder se les suele llamar el espacio latente.\nEste concepto de reducción de dimensiones en el mundo de la minería de datos lo podemos asimiliar rápidamente a técnicas como el Análisis de Componentes Principales (PCA), que nos permite trabajar con un número más reducido de dimensiones que las originales. Igualmente, esa reducción de los datos y la capacidad de poder reconstruir el original podemos asociarlo al concepto de compresión de datos, de forma que con el encoder podemos comprimir los datos y con el decoder los podemos descomprimir. En este caso habría que tener en cuenta que sería una técnica de compresión de datos con pérdida de información (JPG también es un formato de compresión con pérdida de compresión). Es decir, con los datos codificados y el AE (pesos de la red neuronal), seríamos capaces de volver a regenerar los datos originales.\nOtra de las ideas alrededor de los AE es que, si nosotros tenemos un conjunto de datos de la misma naturaleza y los entrenamos con nuestro AE, somos capaces de construir una red neuronal (pesos en la red neuronal) que es capaz de reproducir esos datos a través del AE. Que ocurre si nosotros metemos un dato que no era de la misma naturaleza que los que entrenaron el AE, lo que tendremos entonces es que al recrear los datos originales no va a ser posible que se parezca a los datos de entrada. De forma que el error que vamos a tener va a ser mucho mayor por no ser datos de la misma naturaleza. Esto nos puede llevar a construir un AE que permita detectar anomalías, es decir, que seamos capaces de detectar cuando un dato es una anomalía porque realmente el AE no consigue tener un error lo bastante pequeño.\nSegún lo visto de forma intuitiva vamos a tener el encoder \\(f(X)\\) que será el encargado de codificar los datos de entrada y luego tendremos el decoder \\(g(H)\\) que será el encargado de realizar la decodificación y conseguir acercarnos al dato original . Es decir intentamos conseguir \\(g(f(X))=\\hat{Y} \\approx X\\) . Si suponemos un Simple Autoencoder en el que tenemos una única capa oculta, con una función de activación intermedia y una función de activación de salida y los parámetros y represetan los parámetros de la red neuronal en cada capa, tendríamos la siguiente expresión:\n\\[\n\\begin{aligned}\n& f(X)=\\delta^{(1)}\\left(W^{(1)} * X+b^{(1)}\\right) \\\\\n& g(H)=\\delta^{(2)}\\left(W^{(2)} * H+b^{(2)}\\right)\n\\end{aligned}\n\\]\ndonde \\(f(g(H))=\\delta^{(2)}\\left(W^{(2)} *\\left(\\delta^{(1)}\\left(W^{(1)} * H+b^{(1)}\\right)+b^{(2)}\\right)\\right)=\\hat{Y} \\approx X\\)\nAsí tendremos que \\(g(f(X))=\\hat{Y}\\) donde \\(\\hat{Y}\\) será la reconstrucción de \\(X\\).\nUna vez tenemos la idea intuitiva de para qué nos puede ayudar un AE, recopilamos algunos de los principales usos sobre los que actualmente se está trabajando.\n\n\n1.7.3 Casos de uso\nReducción de dimensiones / Compresión de datos\nEn la idea intuitiva de los AE ya hemos visto claro que se pueden usar para la reducción de dimensiones de los datos de entrada. Si estamos ante unos datos de entrada de tipo estructurado estamos en un caso de reducción de dimensiones clásico, en el que queremos disminuir el número de variables con las que trabajar.\nMuchas veces este tipo de trabajo se hace mediante el PCA (Análisis de Componente Principales, por sus siglas en inglés), sabiendo que lo que se realiza es una transformación lineal de los datos, ya que conseguimos unas nuevas variables que son una combinación lineal de las mismas. En el caso de los AE conseguimos mediante las funciones de activación no lineales (simgmoide, ReLu, tanh, etc) combinaciones no lineales de las variables originales para reducir las dimensiones. También existen versiones de PCA no lineales llamadas Kernel PCA que mediante las técnicas de kernel son capaces de construir relaciones no lineales.\nEn esta línea estamos viendo que cuando el encoder ha actuado, tenemos unos nuevos datos más reducidos y que somos capaces de practicamente volver a reproducir teniendo el decoder. Podríamos pensar en este tipo de técnica para simplemente comprimir información. Hay que tener en cuenta que este tipo de técnicas no se pueden aplicar a cualquier dato que queramos comprimir, ya que debemos haber entrenado al AE con unos datos de entrenamiento que ha sido capaz de obtener ciertos patrones de ellos, y por eso es capaz luego de reproducirlos.\nBúsqueda de imágenes\nCuando pensamos en un buscador de imágenes nos podemos hacer a la idea que el buscar al igual que con el texto nos va a mostrar entradas que seán imágenes parecidas a la que estamos buscando.\nSi construimos un autoencoder, el encoder nos va a dar unas variables con información para poder recrear de nuevo la imagen. Lo que parece claro es que si hay muy poca distancia entre estas variables y otras la reconstrucción de la imagen será muy parecida.\nAsí nosotros podemos entrenar el AE con nuestro conjunto de imágenes, una vez tenemos el AE pasamos el encoder a todas las imágenes y las tenemos todas en ese nuevo espacio de variables.\nCuando queremos buscar una imagen, le pasamos el autoencoder, y ya buscamos las más cercanas a nuestra imagen en el espacio de variables generado por el encoder.\nDetección de Anomalías\nCuando estamos ante un problema de clasificación y tenemos un conjunto de datos que está muy desbalanceado, es decir, tenemos una clase mayoritaria que es mucho más grande que la minoritaria (posiblemente del orden de más del 95%), muchas veces es complicado conseguir un conjunto de datos balanceado que sea realmente bueno para hacer las predicciones.\nCuando estamos en estos entornos tan desbalanceados muchas veces se dice que estamos ante un sistema para detectar anomalías. Un AE nos puede ayudar a detectar estas anomalías de la siguiente forma:\n\nTomamos todos los datos de entrenamiento de la clase mayoritaria (o normales) y construimos un AE para ser capaces de reproducirlos. Al ser todos estos datos de la misma naturaleza conseguiremos entrenar el AE con un error muy pequeño.\nAhora tomamos los datos de la clase minoritaria (o a nomalías) y los pasamos a través del AE obteniendo unos errores de reconstrucción.\nDefinimos el umbral de error que nos separará los datos normales de las anomalías, ya que el AE sólo está entrenado con los normales y conseguirá un error más alto con las anomalías al reconstruirlas.\nCogemos los datos de test y los vamos pasando por el AE, si el error es menor del umbral, entonces será de la clase mayoritaria. Si el error es mayor que el umbral, entonces estaremos ante una anomalía.\n\nEliminación de ruido\nOtra de las formas de uso de los autoencoders en tratamiento de imágenes es para eliminar ruido de las mismas, es decir poder quitar manchas de las imágenes. La forma de hacer esto es la siguiente:\n\nPartimos de un conjunto de datos de entrenamiento (imágenes) a las que le metemos ruido, por ejemplo, modificando los valores de cada pixel usando una distribución normal, de forma que obtenemos unos datos de entrenamiento con ruido.\nConstruimos el AE de forma que los datos de entrada son los que tienen ruido, pero los de salida vamos a forzar que sean los originales. De forma que intentamos que aprendan a reconstruirse como los que no tienen ruido.\nUna vez que tenemos el AE y le pasamos datos de test con ruido, seremos capaces de reconstruirlos sin el ruido.\n\nModelos generativos\nCuando hablamos de modelos generativos, nos referimos a AE que son capaces de generar cosas nuevas a las que existían. De forma que mediante técnicas como los Variational Autoencoders, los Adversarial Autoencoders seremos capaces de generar nuevas imágenes que no teníamos inicialmente. Es decir, podríamos pensar en poder tener un AE que sea capaz de reconstruir imágenes de caras, pero que además con toda la información aprendida fuera capaz de generar nuevas caras que realmente no existen.\n\n\n1.7.4 Diseño del modelo de AE\nTransformación de datos\nCuando se trabaja con redes neuronales y en particular con AEs, necesitamos representar los valores de las variables de entrada en forma numérica. En una red neuronal todos los datos son siempre numéricos. Esto significa que todas aquellas variables que sean categóricas necesitamos convertirlas en numéricas. Además es muy conveniente normalizar los datos para poder trabajar con valores entre 0 y 1, que van a ayudar a que sea más fácil que se pueda converger a la solución.\nComo ya sabemos normalmente nos encontramos que en una red neuronal las variables de salida son:\n\nun número (regresión)\nuna serie de números (regresión múltiple)\nun dato binario (clasificación binaria)\nun número que representa una categoría (clasifiación múltiple)\n\nEn el caso de los AE puede que tengamos una gran parte de las veces valores de series de números, ya que necesitamos volver a representar los datos de entrada. Esto significa que tendremos que conseguir en la capa de salida esos datos numéricos que teníamos inicialmente, como si se tuviera una regresión múltiple.\nArquitectura de red\nComo ya se ha comentado en las redes neuronales, algunos de los hiperparámetros más importantes en un AE son los relacionados con la arquitectura de la red neuronal.\nPara la construcción de un AE vamos a elegir una topología simétrica del encoder y el decoder.\nDurante el diseño del AE necesitaremos ir probando y adaptando todos estos hiperparámetros de la ANN para conseguir que sea lo más eficiente posible:\n\nNúmero de capas ocultas y neuronas en cada una\nFunción de coste y pérdida\nOptimizador\nFunción de activación en capas ocultas\nFunción de activación en salida\n\nNúmero de capas ocultas y neuronas en cada una\nLa selección del número de capas ocultas y la cantidad de neuronas en cada una va a ser un procedimiento de prueba y error en el que se pueden probar muchas combinaciones. Es cierto que en el caso de trabajar con imágenes y CNN ya hay muchas arquitecturas definidas y probadas que consiguen muy buenos resultados. Por otro lado para tipos de datos estructurados será muy dependiente de esos datos, de forma que será necesario realizar diferentes pruebas para conseguir un buen resultado.\nFunción de coste y pérdida\nEn este caso no hay ninguna recomendación especial para las funciones de costes/pérdida y dependerá al igual que en las redes neuronales de la naturaleza de los datos de salida con los que vamos a trabajar.\nOptimizador\nSe recomienda usar el optimizador ADAM (Diederik P. Kingma 2017) que es el que mejores resultados ha dado en las pruebas según (Walia 2017), consiguiendo una convergencia más rápida que con el resto de optimizadores.\nFunción de activación en capas ocultas\nEn un AE las funciones de activación en las capas ocultas van a conseguir establecer las restricciones no lineales al pasar de una capa a la siguiente, normalmente se evita usar la función de activación lineal en las capas intermedias ya que queremos conseguir transformaciones no lineales.\nSe recomienda usar la función de activación ReLu en las capas ocultas, ya que parece ser que es la que mejores resultados da en la convergencia de la solución y además menor coste computacional tiene a la hora de realizar los cálculos.\nFunción de activación en salida\nEn la capa de salida tenemos que tener en cuenta cual es el tipo de datos final que queremos obtener, que en el caso de un AE es el mismo que el tipo de dato de entrada. Normalmente las funciones de activación que se usarán en la última capa seran:\n• Lineal con multiples unidades, para regresión de varios datos numéricos\n• Sigmoid para valores entre 0 y 1\n\n\n1.7.5 Ejemplos de Autoencoders\nUna vez entendido el funcionamiento de los AE, veamos algunos de los AE que se pueden construir para diversas tareas.\n\nSimple\nMulticapa o Profundo\nConvolucional\nDenoising\n\nEn la descripción de los tipos de AE vamos a pensar en python y el framework keras con el backend Tensorflow. Todo el código se proporciona aparte. Usaremos como dataset a MINIST, que contiene 60.000/10.000 (entrenamiento/validación) imágenes de los números del 0 al 9, escritos a mano. Cada imagen tiene un tamaño de 28x28 = 784 pixels, en escala de grises, con lo que para cada pixel tendremos un valor entre 0 y 255 para definir cuál es su intensidad de gris.\nAutoencoder Simple\nVamos a describir como construir un Autoencoder Simple usando una red neuronal densamente conectada en lugar de usar una red neuronal convolucional, para que sea más sencillo comprender el ejemplo.\nEs decir, vamos a tratar los datos de entrada como si fueran unos datos numéricos que nos da cualquier variable, que queremos reproducir y no vamos a utilizar ninguna de las técnicas asociadas a las redes convolucionales. Hay que recordar que las redes convolucionales permiten mediante un tratamiento de las imágenes (convolución, pooling, etc) conseguir mejores resultados que si lo hiciéramos directamente con redes densamente conectadas.\nEn este caso tendremos una capa de entrada con 784 neuronas (correspondientes a los pixels de cada imagen), una capa intermedia de 32 neuronas, y una capa de salida de nuevo de las 784 neuronas para poder volver a obtener de nuevo los datos originales.\n\nimport keras\nfrom keras import layers\n\n# This is the size of our encoded representations\nencoding_dim = 32  # 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats\n\n# This is our input image\ninput_img = keras.Input(shape=(784,))\n# \"encoded\" is the encoded representation of the input\nencoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = layers.Dense(784, activation='sigmoid')(encoded)\n\n# This model maps an input to its reconstruction\nautoencoder = keras.Model(input_img, decoded)\n\n# This model maps an input to its encoded representation\nencoder = keras.Model(input_img, encoded)\n\n# This is our encoded (32-dimensional) input\nencoded_input = keras.Input(shape=(encoding_dim,))\n# Retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n# Create the decoder model\ndecoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\nfrom keras.datasets import mnist\nimport numpy as np\n(x_train, _), (x_test, _) = mnist.load_data()\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nprint(x_train.shape)\nprint(x_test.shape)\n\nautoencoder.fit(x_train, x_train,\n                epochs=5,\n                batch_size=256,\n                shuffle=True,\n                validation_data=(x_test, x_test))\n\n# Encode and decode some digits\n# Note that we take them from the *test* set\nencoded_imgs = encoder.predict(x_test)\ndecoded_imgs = decoder.predict(encoded_imgs)\n\n# Use Matplotlib (don't ask)\nimport matplotlib.pyplot as plt\n\nn = 10  # How many digits we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # Display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()\n\nAutoencoder Multicapa o profundo\nVamos a pasar ahora a una versión del autoencoder donde habilitamos más capas ocultas y hacemos que el descenso del número de neuronas sea más gradual hasta llegar a nuestro valor deseado, para luego volver a reconstruirlo.\nEn este caso seguimos con redes densamente conectadas y aplicamos varias capas intermedias reduciendo el número de neuronas en cada una hasta llegar a la capa donde acaba el encoder para volver a ir creciendo en las sucesivas capas hasta llegar a la de salida.\n\nimport keras\nfrom keras import layers\n\n# This is the size of our encoded representations\nencoding_dim = 32  # 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats\n\n# This is our input image\ninput_img = keras.Input(shape=(784,))\nencoded = layers.Dense(128, activation='relu')(input_img)\nencoded = layers.Dense(64, activation='relu')(encoded)\nencoded = layers.Dense(32, activation='relu')(encoded)\n\ndecoded = layers.Dense(64, activation='relu')(encoded)\ndecoded = layers.Dense(128, activation='relu')(decoded)\ndecoded = layers.Dense(784, activation='sigmoid')(decoded)\n\n# This model maps an input to its reconstruction\nautoencoder = keras.Model(input_img, decoded)\n\n# This model maps an input to its encoded representation\n\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\nfrom keras.datasets import mnist\nimport numpy as np\n(x_train, _), (x_test, _) = mnist.load_data()\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nprint(x_train.shape)\nprint(x_test.shape)\n\nautoencoder.fit(x_train, x_train,\n                epochs=5,\n                batch_size=256,\n                shuffle=True,\n                validation_data=(x_test, x_test))\n\n# Encode and decode some digits\n# Note that we take them from the *test* set\n\ndecoded_imgs = autoencoder.predict(x_test)\nprint(decoded_imgs.shape)\n# Use Matplotlib (don't ask)\nimport matplotlib.pyplot as plt\n\nn = 10  # How many digits we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # Display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()\nAutoencoder Convolucional\nEn nuestro ejemplo al estar trabajando con imágenes podemos pasar a trabajar con Redes Convolucionales (CNN) de forma que en lugar de usar las capas densamente conectadas que hemos usado hasta ahora, vamos a pasar a usar las capacidades de las redes convolucionales.\nAl trabajar con redes convolucionales necesitaremos trabajar con capas de convolución o pooling para llegar a la capa donde acaba el encoder para volver a ir creciendo aplicando operaciones de convolución y upsampling (contrario al pooling).\nEn nuestro ejemplo vamos a tener los siguientes elementos.\n\nimport keras\nfrom keras import layers\n\ninput_img = keras.Input(shape=(28, 28, 1))\n\nx = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n\n# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(16, (3, 3), activation='relu')(x)\nx = layers.UpSampling2D((2, 2))(x)\ndecoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = keras.Model(input_img, decoded)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\nfrom keras.datasets import mnist\nimport numpy as np\n\n(x_train, _), (x_test, _) = mnist.load_data()\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\nx_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n\n\nautoencoder.fit(x_train, x_train,\n                epochs=5,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test, x_test))\n\ndecoded_imgs = autoencoder.predict(x_test)\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(2, n, i)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()\n\nAutoencoder Denoising\nVaamos a usar ahora un autoencoder para hacer limpieza en imagen, es decir, conseguir a partir de una imagen que tiene ruido otra imagen sin ese ruido. Entrenaremos al autoencoder para que limpie “ruido” que hay en la imagen y lo reconstruya sin ello. El ruido lo vamos a generar mediante una distribución normal y modificaremos el valor de los pixels de las imágenes.\nUsaremos estas imágenes con ruido para que sea capaz de reconstruir la imagen original sin ruido con el AE. Para realizar este proceso lo que haremos será:\n• Crear nuevas imágenes con ruido\n• Entrenar el autoencoder con estas nuevas imágenes\n• Calcular el error de reconstrucción respecto a las imágenes originales\nAl estar trabajando con imágenes vamos a partir del Autoencoder de Convolución para poder aplicar el denosing.\nimport keras\nfrom keras import layers\n\ninput_img = keras.Input(shape=(28, 28, 1))\n\nx = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n\n# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(16, (3, 3), activation='relu')(x)\nx = layers.UpSampling2D((2, 2))(x)\ndecoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = keras.Model(input_img, decoded)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\nfrom keras.datasets import mnist\nimport numpy as np\n\n(x_train, _), (x_test, _) = mnist.load_data()\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\nx_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n\nnoise_factor = 0.5\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n\n\nn = 10\nplt.figure(figsize=(20, 2))\nfor i in range(1, n + 1):\n    ax = plt.subplot(1, n, i)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()\n\n\nautoencoder.fit(x_train_noisy, x_train,\n                epochs=5,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test_noisy, x_test))\n\n\ndecoded_imgs = autoencoder.predict(x_test)\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(2, n, i)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "capitulo1.html#arquitecturas-preentrenadas",
    "href": "capitulo1.html#arquitecturas-preentrenadas",
    "title": "1  Deep Learning",
    "section": "1.8 Arquitecturas preentrenadas",
    "text": "1.8 Arquitecturas preentrenadas\nLas arquitecturas pre-entrenadas en Deep Learning son modelos de redes neuronales que han sido previamente entrenados en grandes conjuntos de datos para realizar tareas específicas como clasificación de imágenes, generación de texto o reconocimiento de voz. Durante el entrenamiento, estos modelos aprenden patrones complejos de los datos, lo que les permite realizar tareas relacionadas con alta precisión y generalización.\nEl funcionamiento de las arquitecturas pre-entrenadas se basa en el concepto de transferencia de aprendizaje. La transferencia de aprendizaje consiste en aprovechar el conocimiento aprendido en una tarea para aplicarlo a otra tarea diferente; es decir, se utiliza como base para realizar tareas específicas con mayor facilidad.\nLas arquitecturas pre-entrenadas suelen ser diseñadas por investigadores y equipos de desarrollo en instituciones académicas, laboratorios de investigación y empresas de tecnología. Estos expertos desarrollan las arquitecturas, definen el proceso de entrenamiento y seleccionan los conjuntos de datos adecuados para cada tarea. Algunas de las organizaciones e instituciones que lideran el diseño de arquitecturas pre-entrenadas son:\n\nGoogle AI: ha desarrollado arquitecturas pre-entrenadas como BERT y Vision Transformers, las cuales han tenido un gran impacto en el procesamiento del lenguaje natural y la visión artificial, respectivamente. Además, en los últimos años, han publicado Gema y Gemini, que permiten el aprendizaje automático en general, ofreciendo flexibilidad, escalabilidad y alto rendimiento en diversas tareas\nFacebook AI Research: ha contribuido con arquitecturas pre-entrenadas como FAIRSEQ y Detectron2, que han impulsado el rendimiento en tareas de procesamiento del lenguaje natural y detección de objetos, respectivamente. Además, también ha desarrollado Llama, Llama2 y Llama3, modelos que han destacado por su versatilidad y precisión en aplicaciones de reconocimiento de voz y procesamiento de texto.\nOpenAI: conocido por sus arquitecturas pre-entrenadas líderes como GPT-3 y Whisper, ha lanzado recientemente GPT-4. Este modelo promete mejorar la comprensión contextual y la generación de texto, lo que podría tener un gran impacto en aplicaciones de procesamiento del lenguaje natural. GPT-4 representa un avance significativo en la investigación de inteligencia artificial y abre nuevas posibilidades para sistemas más avanzados.\n\nEstos modelos son entrenados en grandes clústeres de servidores con hardware especializado, como GPUs y TPUs, utilizando conjuntos de datos masivos y técnicas de optimización avanzadas. A día de hoy se han convertido en un elemento capital para los desarrolladores de aprendizaje automático puesto que nos permite aprovechar modelos entrenados previamente que, en condiciones normales, sería casi imposible realizar por centros no especializados. Así, podríamos destacar las siguientes ventajas:\n\nAhorro de tiempo y recursos: al aprovechar el conocimiento pre-entrenado, nos permiten a los desarrolladores ahorrar tiempo y recursos computacionales en comparación con entrenar un modelo desde cero\nMayor rendimiento: suelen ofrecer un rendimiento superior a los modelos entrenados desde cero, especialmente para tareas complejas\nFacilidad de uso: las librerías y entornos facilitan el acceso y la utilización de arquitecturas pre-entrenadas, incluso para usuarios con poca experiencia en Deep Learning\n\nComo se ha dicho, cada vez más se emplean este tipo de arquitecturas para realizar tareas que, con otro tipo de diseños de aprendizaje automático serían más complejas y costosas de abordar. En este aspecto, destacar que a la hora de elegir qué tipo de arquitectura pre-entrenada es adecuada para un caso de uso es necesario tener en cuenta la tarea específica a abordar, el conjunto de datos con los que ha sido entrenada y la similaridad con los nuestros propios así como si disponemos de ciertas limitaciones en cuanto a recursos computacionales.\nPor último, destacar que existen diversas librerías y entornos que facilitan el trabajo con arquitecturas pre-entrenadas. Algunas de las opciones más interesantes son:\n- TensorFlow Hub: es un repositorio de módulos pre-entrenados para TensorFlow, que incluye una amplia gama de arquitecturas pre-entrenadas para diversas tareas. - Hugging Face Hub: es una plataforma similar a TensorFlow Hub, pero que ofrece soporte para múltiples frameworks de Deep Learning, incluyendo TensorFlow, PyTorch y JAX.\n\n1.8.1 Paquetes específicos en Python\n\n1.8.1.1 Transformers\nTransformers es una biblioteca de código abierto en Python desarrollada por Hugging Face que proporciona un conjunto de herramientas para trabajar con modelos de lenguaje basados en redes neuronales transformadoras.\nEsta librería se caracteriza por su gran variedad de modelos pre-entrenados y capacidad de hacer fine-tuning de modelos. Así, entre sus principales ventajas podemos citar: - La facilidad de uso - La flexibilidad y la escalabilidad - La comunidad activa de desarrolladores\nFinalmente, indicamos proporcionamos una serie de recursos adicionales:\n\nSitio web de Transformers: https://huggingface.co/docs/transformers/en/index\nDocumentación de Transformers: https://huggingface.co/docs\nTutoriales de Transformers: https://www.youtube.com/watch?v=QEaBAZQCtwE\n\n\n\n\n1.8.2 Tensorflow-Hub\nTensorflow-Hub alberga una gran colección de módulos de aprendizaje automático pre-entrenados y reutilizables, creados por Google y la comunidad de TensorFlow.\nEsta librería se caracteriza por disponer de una gran cantidad de modelos pre-entrenados para diferentes tareas relacionadas con el aprendizaje profundo. Así, entre sus principales ventajas podemos citar: - El acceso a modelos pre-entrenados de última generación - La flexibilidad y personalización - La facilidad de\nFinalmente, indicamos proporcionamos una serie de recursos adicionales:\n\nSitio web de TensorFlow Hub: https://www.tensorflow.org/hub\nDocumentación de TensorFlow Hub: https://www.tensorflow.org/hub\nTutoriales de TensorFlow Hub: https://www.tensorflow.org/hub/tutorials\n\n\n\n1.8.3 Arquitecturas Zero-shot\nLas arquitecturas Zero-Shot, también conocidas como modelos de Aprendizaje por Analogía son capaces de realizar tareas de clasificación sin necesidad de entrenamiento específico para cada categoría. En su lugar, estas arquitecturas aprenden representaciones vectoriales de conceptos a partir de datos no etiquetados, lo que les permite generalizar a nuevas categorías sin haberlas visto nunca antes.\n\n\n1.8.3.1 Uso en imágenes\nEn el ámbito de la visión artificial, las arquitecturas Zero-Shot para imágenes han demostrado ser particularmente útiles para tareas como la clasificación de imágenes de escenas naturales, la identificación de animales y la detección de objetos. Entre los modelos más destacados en esta área se encuentran: - ImageNet-pretrained CLIP: basado en la arquitectura CLIP (Contrastive Language-Image Pre-training), este modelo utiliza representaciones de imágenes y texto para realizar clasificación Zero-Shot de imágenes con gran precisión - ResNet-pretrained ZSL: Este modelo combina la arquitectura ResNet, conocida por su rendimiento en tareas de clasificación de imágenes, con un enfoque Zero-Shot basado en la distancia entre representaciones\nMATERIAL COMPLEMENTARIO - NOTEBOOK: ejemplo en Zero-shot: clasificación animales\n\n\n1.8.3.2 Uso en texto\nEn el procesamiento del lenguaje natural, las arquitecturas Zero-Shot para texto han ganado popularidad para tareas como la clasificación de documentos, la extracción de información y la categorización de textos. Algunos modelos representativos en este campo son:\n\nBERT-pretrained ZSL: basado en la arquitectura BERT (Bidirectional Encoder Representations from Transformers), este modelo utiliza representaciones contextuales de palabras para realizar clasificación Zero-Shot de texto con gran precisión\nSiamese Networks with Word Embeddings: este enfoque utiliza redes siamesas, un tipo de red neuronal que aprende a comparar pares de entradas, para clasificar texto Zero-Shot utilizando representaciones de palabras pre-entrenadas.\n\n\n\n\n1.8.4 Detección de objetos\nLa detección de objetos es una tarea fundamental en el ámbito de la visión artificial, que consiste en identificar y localizar objetos dentro de una imagen o video. Las arquitecturas pre-entrenadas para la detección de objetos han revolucionado este campo, ofreciendo modelos de alta precisión y eficiencia. Entre los modelos más utilizados se encuentran:\n\nFaster R-CNN: Este modelo combina la arquitectura de redes convolucionales profundas (CNN) con una región de propuesta de regiones (RPN) para detectar y localizar objetos con gran precisión\nYOLOv5: Este modelo destaca por su velocidad y eficiencia, utilizando una arquitectura basada en convoluciones y bloques de atención para detectar objetos en tiempo real\n\nMATERIAL COMPLEMENTARIO - VIDEOTUTORIAL: ejemplo en Arquitecturas Tensorflow-Hub\n\n\n1.8.5 Conversión de voz a texto\nLa conversión de voz a texto, también llamado como Speech-to-Text es una tarea crucial para la interacción hombre-máquina. Las arquitecturas pre-entrenadas para este tipo de casos han impulsado el desarrollo de sistemas de reconocimiento de voz de alta precisión, capaces de transcribir audio en texto con gran fluidez. Uno de los principales modelos para esta tarea es Whisper el cual fue desarrollado por OpenAI. Este modelo se basa en la arquitectura Transformer y utiliza aprendizaje supervisado y multitarea para lograr una precisión y robustez excepcionales en una amplia gama de condiciones acústicas.\nMATERIAL COMPLEMENTARIO - VIDEOTUTORIAL: ejemplo en Speech2Text",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "capitulo1.html#aprendizaje-por-refuerzo",
    "href": "capitulo1.html#aprendizaje-por-refuerzo",
    "title": "1  Deep Learning",
    "section": "1.9 Aprendizaje por Refuerzo",
    "text": "1.9 Aprendizaje por Refuerzo\n\n1.9.1 Introducción\nHasta ahora hemos visto como el Deep Learning se usa para el aprendizaje supervisado y el aprendizaje no supervisado, pero vamos a dar un paso más, en el que veremos como usar Deep Learning en otro tipo de aprendizaje llamado aprendizaje por refuerzo.\nEl Aprendizaje por Refuerzo (RL por sus siglas en ingles, Reinforcement Learning) trata de conseguir que el sistema aprenda mediante recompensa/castigo, en función de si los pasos que da son buenos o malos. De esta manera, cuanta mayor recompensa se tenga es que nuestro sistema se ha acercado a la solución buena. Se trata de aprender mediante la interacción y la retroalimentación de lo que ocurra.\nPartiremos de dos elementos clave agente (es el que aprende y toma decisiones), y el entorno (donde el agente aprende y decide que acciones tomar). Tendremos que el agente podrá realizar acciones que normalmente provocarán un cambio de estado y a la vez se tendrá una recompensa (positiva o negativa) en función de la acción tomada en el entorno en ese momento.\nEs decir, nos encontraremos un agente que realizará una acción \\(a_t\\) en el tiempo \\(t\\), esta acción afectará al entorno que estará en un estado \\(S_t\\) y mediante esta acción cambiará a un estado \\(S_{t+1}\\) y además dará una recompensa \\(r_{t+1}\\) en función de los malo o bueno que haya sido este paso.\nEl agente volverá a examinar el nuevo estado del entorno \\(S_{t+1}\\) y la nueva recompensa recibida \\(r_{t+1}\\) y volverá a tomar la decisión de realizar una nueva acción \\(a_{t+1}\\).\n\n\n\n\n\n\nFigure 1.26: Esquema Aprendizaje por Refuerzo - Fuente: Propia\n\n\n\n\n\n1.9.2 Formalismo Matemático\nEl formalismo matemático para el Aprendizaje por Refuerzo está basado en los Procesos de Decisión de Markov (MDP por sus siglas en ingles). (CS229 Lecture notes).\n\n1.9.2.1 Propiedad de Markov\nSi tenemos una secuencia de estados \\(s_1, s_2, ..., s_t\\) y tenemos la probabilidad de pasar a otro estado \\(s_{t+1}\\), diremos que se cumple la Propiedad de Markov si el futuro es independiente del pasado y sólo se ve afectado por el presente, es decir:\n\\[\n\\mathbb P[S_{t+1}| S_t] = \\mathbb P[ S_{t+1}| S_t, s_{t-1}, ... S_2, S_1]\n\\]\nTendremos una Matriz de Probabilidades de Transición a una matriz con las probabilidades de todos los posibles cambios de estado que se puedan producir, \\[\\mathcal P_{ss'}=\\mathbb P[S_{t+1}=s'|S_t = s]\\]\n\n\n1.9.2.2 Proceso de Markov\nAsí llamaremos Proceso de Markov a un proceso aleatorio sin memmoria, es decir, una secuencia de estados \\(S_1, S_2, …\\) con la propiedad de Markov.\nUn Proceso de Markov está formado por una dupla \\(&lt;\\mathcal S,\\mathcal P&gt;\\),:\n\n\\(\\mathcal S\\) conjunto finito de Estados\n\\(\\mathcal P\\) matriz de probabilidades de transición\n\n\n\n1.9.2.3 Proceso de Recompensa de Markov\nLLamaremos Proceso de Recompensa de Markov (MRP, por sus siglas en ingles) a una cuádrupla \\(&lt;\\mathcal S,\\mathcal P,\\mathcal R,\\gamma&gt;\\), formada por:\n\n\\(\\mathcal S\\) conjunto finito de Estados\n\\(\\mathcal P\\) matriz de probabilidades de transición\n\\(\\mathcal R\\) Función de recompensa definida como: \\(\\mathcal R_s=E[R_{t+1}|S_t=s]\\), donde \\(R_{t+1}\\)es la recompensa obtenida de pasar al estado \\(S_{t+1}\\) desde el estado \\(S_t\\)\n\\(\\gamma\\) Factor de descuento, con \\(\\gamma \\in [0,1]\\)\n\nEn este contexto llamaremos Saldo (\\(G_t\\)) a la suma de todas las recompensas conseguidas a partir del estado \\(s_t\\) con el factor de descuento aplicado.\n\\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}\n\\]\nEl hecho de usar \\(\\gamma\\) (factor descuento), nos permite dar grandes recompensas lo antes posible, y no dar tanto valor a futuras recompensas lejanas. También puede haber otras interpretaciones por ejemplo a nivel económico, si la recompensa está basado en un dato monetario real, tendría sentido que el dinero a futuro tendría menos valor. También nos permite asegurar que este valor de \\(G_t\\) es finito ya que produce que la serie sea convergente.\nCuando los valores del factor descuento se acercan a 0 podríamos decir que nos fijamos sólo en los valores más cercanos de la recompensa. En cambio cuando los valores se acercan a 1 entonces les daremos más peso a los valores más lejanos de la recompensa.\nUna vez definido el Saldo podemos definir la Función Valor de Estado como la función que nos da el Saldo Esperado comenzando por el estado \\(s\\). Es decir:\n\\[\nV(s) = \\mathbb E[G_t|S_t=s]\n\\]\nEsta función nos dice cómo de bueno es partir de este estado y continuar.\n\n\n1.9.2.4 Proceso de Decisión de Markov\nUn Proceso de Decisión de Markov (MDP por sus siglas en inglés) es un tupla \\(&lt;\\mathcal S,\\mathcal A,\\mathcal P,\\mathcal R, \\gamma &gt;\\) donde:\n\n\\(\\mathcal S\\) es el conjunto de posibles estados.\n\\(\\mathcal A\\) es el conjunto de posibles acciones.\n\\(\\mathcal P\\) son las probabilidades de transición de un estado a otro en función de la acción realizada. Por cada estado y acción hay una distribución de probabilidad para pasar a otro estado.\n\\[\n\\mathcal P_{ss'}^a=\\mathbb P[S_{t+1}=s'|S_t=s,A_t=a]\\]\n\\(\\gamma\\) es el conocido como factor de descuento y tendrá un valor entre \\([0,1)]\\) y nos proporciona cuanto descontamos en las recompensas a futuro.\n\\(\\mathcal R\\) es la Función de recompensa definida como: \\(\\mathcal R_s^a=E[R\\_{t+1}|S_t=s, A_t=a]\\), donde \\(R\\_{t+1}\\)es la recompensa obtenida de pasar al estado \\(S_{t+1}\\) desde el estado\n\nAdemás tenemos que este proceso estocástico cumple la propiedad de Markov que dice que el futuro es independiente del pasado dado el presente. En términos de nuestro problema, podría decir que pasar de un estado \\(s_t\\) al siguiente \\(s_{t+1}\\) sólo depende de \\(s_t\\) y no de los anteriores estados \\[\n\\mathbb P(s_{t+1}|s_t)= \\mathbb P(s_{t+1}|s_1,s_2,...,s_t)\n\\]\nVeamos cual es la dinámica de un MDP:\n\nEmpezamos con un estado \\(s_0 \\in \\mathcal S\\)\nElegimos una acción \\(a_0 \\in \\mathcal A\\) (la política será la que la elija)\nObtenemos una recompensa \\(R_1 = R(s_0) = R(s_0, a_0)\\)\nElegimos una acción \\(a_1 \\in \\mathcal A\\)(la política será la que la elija)\nSe transiciona aleatoriamente a un estado \\(s_1\\) en un función del valor de \\(P_{s_0s_1}^{a_1}\\)\nObtenemos una recompensa \\(R_2 = R(s_1) = R(s_1, a_1)\\)\nSe transiciona a aleatoriamente a un estado \\(s_12\\) en un función del valor de \\(P_{s_1s_2}^{a_2}\\)\n…\nRepetimos de forma iterativa este proceso\n\nLa meta en RL es elegir las acciones adecuadas en el tiempo para maximizar: \\[\\mathbb E[G_t] =\\mathbb E[R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\gamma^3 R(s_3) + ...]\\] Que es conocido como la hipotesis de la recompensa.\nVamos a introducir el término de política como una función \\(\\pi : \\mathcal S \\rightarrow \\mathcal A\\) que mapea los estados a las acciones. Es decir, es la que decide que acción hay que ejecutar en función de cual es el estado en el que estamos. Una política podría ser determinística o estocástica. \\[\na = \\pi(s) \\\\\na = \\pi (a|s)=\\mathbb P[A=a|S=s]\n\\]\nUna política define cual va a ser el comportamiento de un agente. En un MDP las políticas dependen del estado actual, y no de la historia de los estados pasados.\nDiremos que estamos ejecutando una política \\(\\pi\\) si cuando estamos en un estado \\(s\\) aplicamos la acción \\(a=\\pi(s)\\)\nDefiniremos:\n\\[\n\\mathcal P_{s,s`}^\\pi = \\sum_{a \\in \\mathcal A}\\pi(a|s)\\mathcal P_{s,s`}^a \\\\\n\\mathcal R_{s}^\\pi = \\sum_{a \\in \\mathcal A}\\pi(a|s)\\mathcal R_{s}^a\n\\]\nTambién definiremos la Función Valor de Estado para una política \\(\\pi\\) a la función que nos predice la recompensa a futuro (el saldo esperado): \\[V^{\\pi}(s)=\\mathbb E_\\pi[G_t|S_t=s]=\n\\mathbb E_\\pi[R(s_t) + \\gamma R(s_{t+1}) + \\gamma^2 R(s_{t+2}) + \\gamma^3 R(s_{t+3}) + ...|s_t=s]\\] Es decir, la esperanza de la suma de las recompensas con factor descuento suponiendo el comienzo en \\(s_t=s\\) y tomando las acciones bajo la política \\(\\pi\\). Nos permite decir cómo de buenos o malos son los estados.\nAñadiremos el concepto de la Función Valor de Acción, también llamada Función de Calidad (por eso se usa la \\(Q\\) (Quality), para una política \\(\\pi\\) a la función que nos predice la recompensa a futuro (el saldo esperado), suponiendo que se se parte de una acción \\(a\\). \\[\nQ^\\pi(s,a)=\\mathbb E_\\pi[G_t|S_t=s,A_t=a]\\\\\n=\\mathbb E_\\pi[R(s_t) + \\gamma R(s_{t+1}) + \\gamma^2 R(s_{t+2}) + \\gamma^3 R(s_{t+3}) + ...|s_t=s,A_t=a]\n\\] La función de Valor de Estado puede ser descompuesta en la recompensa inmediata y el resto de la recompensa: \\[\nV^\\pi(s) = \\mathbb E_\\pi[R_{t+1}+\\gamma V^\\pi(S_{t+1)}|S_t=s]\n\\] y del mismo modo se puede descomponer la función Valor de Acción: \\[\nQ^\\pi(s,a) = \\mathbb E_\\pi[R_{t+1}+\\gamma Q^\\pi(S_{t+1}, A_{t+1})|S_t=s,A_t=a]\n\\] Luego tenemos \\[\nV^\\pi(s) = \\sum_{a\\in A}\\pi(a|s)Q^\\pi(s,a)\n\\]\ny \\[\nQ^\\pi(s,a) = R_s^a+\\gamma \\sum_{s'\\in S} P_{ss'}^{a}V^\\pi(s')\n\\]\nLlegando a\n\\[\nV^\\pi(s) = \\sum_{a\\in A}\\pi(a|s)(R_s^a+\\gamma \\sum_{s'\\in S} P_{ss'}^{a}V^\\pi(s'))\n\\]\ny\n\\[\nQ^\\pi(s,a) = R_s^a+\\gamma \\sum_{s'\\in S} P_{ss'}^{a}\\sum_{a \\in \\mathcal A} \\pi (a|s)Q^\\pi(s',a)\n\\]\nDada una política \\(\\pi\\) su función valor de estado asociada \\(V^{\\pi}(s)\\) cumple la Ecuación de Bellman: \\[V^{\\pi}(s)=R_s+ + \\gamma\\sum_{s'\\in S}P_{s,\\pi(s)}(s')V^{\\pi}(s')\\] Lo que nos dice que la función valor está separada en dos términos:\n\nLa recompensa inmediata \\(R(s)\\)\nLa suma de recompensas a futuro con el factor de descuento.\n\nIgualmente su función valor de acción asociada \\(Q^\\pi(s,a)\\)cumple la Ecuación de Bellman:\n\\[\nQ^\\pi(s,a) = R_s^a+\\gamma \\sum_{s'\\in S} P_{ss'}^{a}\\sum_{a \\in \\mathcal A} \\pi (a|s)Q^\\pi(s',a)\n\\]\nLas Ecuaciones de Bellman permiten garantizar una solución óptima del problema de forma que dada una política óptima (\\(\\pi^*\\)), además se cumple:\n\\[\nV^{\\pi^*}(s)=V^*(s)=max_\\pi V^\\pi(s)\\\\\nQ^{\\pi^*}(s,a)=Q^*(s,a)=max_\\pi Q^\\pi(s,a)\n\\]\nEs decir, que las funciones de valor de estado y de acción óptimas son las mismas que se general con la política óptima.\nComo la meta del RL es encontrar una política óptima \\(\\pi^*\\) la cual maximize el valor del saldo esperado total (desde el inicio) \\(G_0=\\sum_{t=0}^\\infty\\), es decir, podríamos definir la política óptima como:\n\\[\\begin{equation}\n\\pi^*(a|s)= \\left\\lbrace\n\\begin{array}{ll}\n1 \\text{ si } a=\\mathop{\\mathrm{argmax}}\\limits_{a \\in \\mathcal A} Q^* (s,a) \\\\\n0 \\text{ si cualqier otro caso}\n\\end{array}\n\\right.\n\\end{equation}\\]\nLuego si conocemos \\(Q^*(s,a)\\) inmediatamente tenemos una política óptima.\n\n\n1.9.2.5 Resolución de las Ecuaciones de Bellman\nLas ecuaciones de Bellman pueden ser usadas para resolver de forma eficiente \\(V^\\pi\\), especialmente en un MDP de un número finito de estado, escribiendo una ecuación \\(V^\\pi (s)\\) por cada estado.\nLa mayoría de los algoritmos de RL usan las Ecuaciones de Bellman para resolver el problema. La forma básica de resolverlo es usando progración dinámica (PD por sus siglas en inglés), aunque nos encontramos con muchos problemas para resolverla cuando el número de acciones/estados aumenta. También se usan otras técnicas como los métodos de montecarlo (MMC, por sus siglas en inglés) o los métodos de diferencia temporal (TD, por sus siglas en ingles).\nPasemos a ver una clasificación de los tipos de algoritmos para resolver los problemas de RL.\n\n\n\n1.9.3 Taxonomía de Algoritmos\nDesde OpenAI (https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#citations-below) obtenemos la siguiente taxonomía de algoritmos de RL que nos servirá como guía para entender como clasificar los algoritmos:\n\n\n\n\n\n\nFigure 1.27: Modelos Reinforcement Learning\n\n\n\nLa primera gran separación se hace sobre si los algoritmos siguen un modelo definido (model-baed) o no (modelo-free).\nModel-free\nPor otro lado los model-free usan la experiencia para aprender o una o ambas de dos cantidades más simples (valores estado/acción o políticas).\nLas aproximaciones de estos algorimtos son de tres tipos:\n\nPolicy Optimization\n\nEl agente aprende directamente la función política que mapea el estado a una acción. Nos podemos encontrar con dos tipos de políticas, las políticas deterministicas (no hay incertidumbre en el mapeo) y las políticas estocásticas (tenemos una distribución de probabilidad en las acciones). En este último caso diremos que tenemos un Proceso de Decisión de Markov Parcialmente Observable (POMDP, por sus siglas en ingles).\n\nQ-Learning\n\nEn este caso el agente aprende una función valor de acción \\(Q(s,a)\\) que nos dirá cómo de bueno es tomar una acción dependiendo del estado.\n\nHíbridos\n\nEstos métodos combinan la fortaleza de los dos métodos anteriores, aprendiendo tanto la función política como la función valor de acción.\nModel-based\nLos algoritmos model-based usan la experiencia para construir un modelo interno de transiciones y resultados inmediatos en el entorno. Las acciones son elegidas mediante búsqueda o planificación en este modelo construido.\nLas aproximaciones de estos algorimtos son de dos tipos:\n\nAprender el Modelo\n\nPara aprender el modelo se ejecuta una política base,\n\nAprender dado el Modelo\n\nNos centraremos en los algoritmos de tipo Model-Free que son los más utilizados ya que no requieren del modelo. Si se quieren profundizar en los diferentes algoritmos, se puede consultar las documentación en: https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#links-to-algorithms-in-taxonomy.\nVamos a ver 2 de los algoritmos de tipo Model-free que nos van a permitir el ver el paso de un algoritmo sin Deep Learning y otro en el que se aplica Deep Learning para obtener el objetivo final de tener un agente capaz de aprender por sí solo a realizar las tareas específicas que se tengan que realizar.\n\n\n1.9.4 Q-Learning (value)\nQ-Learning es un método basado en valor y que usa el sistema TD (actualización su función valor en cada paso) para el entrenamiento y su función de valor de estado.\nEl nombre de Q viende de Quality (calidad), por que nos da la calidad de la acción en un determinado estado. Lo que tenemos es que vamos a tener una función de valor de acción (Q-función) que nos da un valor numérico de cómo de buena es a partir de un estado s y una acción a.\nEn este caso tenemos que internamente nuestra Q-función (\\(Q(s,a)\\)) es una Q-tabla, de forma que cada fila corresponde a un estado, y cada columna a una de las posibles acciones.\n\n\n\nQ-Learning - Fuente: https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python\n\n\nEs decir, esta tabla va a contener la información de recompensa total esperada para cada valor de estado y acción. Cuando nosotros realizamos el entrenamiento de la Q-función, nosotros conseguimos una función que optimice esta Q-tabla.\nSi nosotros tenemos una Q-función óptima (\\(Q^*(s,a)\\)), entonces podremos obtener la política óptima a partir de ella:\n\\[\n\\pi^*(s) = \\mathop{\\mathrm{argmax}}\\limits_{a \\in \\mathcal A}Q^*(s,a)\n\\] Veamos cuales serían los pasos que deberíamos dar:\nInicializamos nuestra Q-Tabla con valores a 0. Conforme avanece nuestro entrenamiento estos valores irán cambiando en función de los datos que se obtengan al porbar a realizar acciones y obtener las recompensas correspondientes.\nEl siguiente elemento que necesitamos es una política de entrenamiento (función que nos permita elegir que acción tomar en función del estado en el que estemos), en este caso nuestra política estará basada en los valores de la Q-tabla, es lo que llamaremos explotación (explotamos la información que tenemos cogiendo la acción con mejor valor Q) o elegiremos otra acción, es lo que llamaremos exploración (exploramos nuevos caminos cogiendo una acción de forma aleatoria).\nEsto es lo que se llama una política \\(\\epsilon\\)-greedy, ya que se usa un parámetro \\(\\epsilon\\), valor entre 0 y 1, que nos permite decidir si elegimos explorar o si queremos explotar los datos que ya tenemos.\nXXXXX Imagen del gráfico epsilon (epsilon respecto al número de epsisodios)\nTendremos que:\n\ncon probabilidad 1-\\(\\epsilon\\) nosotros haremos explotación y\ncon probabilidad \\(\\epsilon\\) nosotros haremos exploración.\n\nEs decir, inicialmente le damos valor 1 a \\(\\epsilon\\) de forma que empezaremos haciendo exploración e iremos bajando este valor de epsilon conforme avance el entrenamiento para que cada vez usemos más la explotación.\nLa idea base es que al principio del entrenamiento, lo prioritario es explorar, es decir, seleccionar una acción al azar y obtener su recompensa, ya que nuestra Q-Tabla está inicializada a 0. Conforme avance el entrenamiento nos tendremos que ir fiando más de los datos que ya tenemos y tendrá que primar la explotación de nuestros datos de la Q-Tabla. Para hacer ésto de una forma efectiva, usaremos un parámetro decay_epsion que conforme avancemos en entrenamiento se encargará de ir reduciendo el valor de \\(\\epsilon\\) para conseguir este efecto.\nUna vez que tenemos nuestros elementos base, pasaremos al entrenamiento, de forma que para todos los episodios (iteraciones de partidas) que definamos haremos lo siguiente:\n\nPartimos de un estado inicial, y obtenemos una acción a partir de nuestra política de entrenamiento\nActualizmos \\(\\epsilon\\) con el nuevo valor en este episodio\nIteramos para un número máximo de pasos dentro de este episodio\nObtenemos el nuevo estado, así como la recompensa obtenida\nActulizamos el valor de la Q-Tabla correspondiente según la fórmula basada en los métodos de TD (Diferencias temporales) \\[\nQ(s,a) = Q(s,a) + \\alpha(R(s,a)+\\gamma argmax_aQ(s',a) - Q(s,a))\\\\\n\\text{donde }s\\text{ es el estado actual y }s'\\text{ es el nuevo estado}\n\\]\nVerificamos si se ha llegado al final del juego para salir de este espisodio si es el caso\nCambiamos el estado como el nuevo estado\n\nUna vez acabemos nuestro entrenamiento, obtendremos nuestra política óptima como: \\[\n\\pi^*(s) = \\mathop{\\mathrm{argmax}}\\limits_{a \\in \\mathcal A}Q^*(s,a)\n\\]\nPseudo código Q-Learning\n\n\n\n\n\n\nFigure 1.28: Algoritmo Q-Learning - Fuente: Propia\n\n\n\n\n\n1.9.5 DQN (Deep Q-Learning)\nhttps://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\nHemos visto el algoritmo de Q-Learning en el que usábamos una Q-Tabla, es decir una tabla donde guardábamos todos los valores de la función \\(Q(s,a)\\) y que entrenando el agente, éramos capaces de conseguir aproximar a la función Q óptima, con lo cual teníamos una Política Óptima.\nEste tipo de algoritmos son válidos cuando nos encontramos con un número “limitado” de estados y acciones, de forma que la tabla es relativamente manejable y somos capaces de entrenarla. Si nos encontramos ante un problema en el que tenemos miles o cientos de miles de estados no va a ser efectivo construir una tabla y entrenarla para todas las posibles combinaciones etado-acción. Para abordar este tipo de problemas, la mejor solución es buscar un aproximador de la función \\(Q(s,a)\\), que nos permita obtener la mejor solución sin necesidad de entrenar todas las posibles combinaciones.\nPara realizar este trabajo una de las posibles opciones es usar redes neuronales como función aproximadora y que nos abrirá la posibilidad de trabajar con problemas en los que existan grandes cantidades de estados/acciones.\nFue el equipo de Deepmind en 2013 (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), en su artítulo “Human-level control through deep reinforcementlearning”, los primeros que decidieron atacar los problemas de alta dimensionalidad de estados/acciones mediante el uso de Redes Neuronales Profundas. La forma de probar su código fue mediante la implementación de agentes que fueran capaces de aprender a jugar a los clásicos juegos de Atari 2600. De forma que el agente, recibiendo la información de entrada de los pixels que hay en cada momento en pantalla y el marcador del juego, eran capaces de sobrepasar el rendimiento de algoritmos actuales que hacían ese trabajo. En estte caso usaron la misma red neuronal, con la misma arquitectura e hiperparámetros para los 49 juegos con los que se probaron.\n\n\n\n\n\n\nFigure 1.29: Deep Q-Learning - Fuente: https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\n\n\n\nCon nuestro algoritmo de Q-Learning teníamos una función \\(Q(s,a)\\) que implementábamos con un tabla y nos daba para cada estado y cada acción cual era el valor de Q (Quality) de la recompensa esperada. Ahora, con Deep Q-Learning nos encontramos que vamos a tener una red neuronal que será la encargada de para cada estado obtener el valor de Q para cada posible acción.\nDQN (Deep Q-Network) Arquitectura\nPara poder implementar nuestro trabajo con redes neuronales nos vamos a encontrar con el problema de entrenar la red neuronal (obtener los pesos) que permitan alcancar nuestra función Q-Óptima que nos daría la Política Òptima que es lo que realmente buscamos.\nBásicamente para realizar el trabajo usaremos 2 redes neuronales que tendrán la misma arquitectura de forma que el entrenamiento sea estable.\n\nDQN que será la red de predicción, y que será la que entrenaremos para minimizar el valor del error \\((R+\\gamma argmax_{a'}Q(s',a',w')-Q(s,a,w))^2\\)\nDQN_Target que será la red que calculará \\(R+\\gamma argmax_a'Q(s',a',w')\\)\n\n\n\n\n\n\n\nFigure 1.30: Arquitectura Redes DQN - Fuente: https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\n\n\n\nExperience Replay\nEl mecanismo del Experience Replay nos va a permitir entrenar nuestra red DQN con minibatchs que vamos a extraer de forma aleatoria de la memoria en la que vamos a ir guardando los resultados que vamos obteniendo &lt;s,a,r,s’&gt;.\nÉsto nos va a permitir por un lado entrenar nuestra red de predicción y además va a servirnos para evitar correlaciones de secuencias consecutivas que pudieran producir un sesgo en nuestros resultados. De esta manera, al elegir al azar los elementos que vamos a usar para entrenar la red, no tendrán ninguna relación con los datos consecutivos que se van produciendo en los pasos de los episodios.\nAlgoritmo Deep Q-Learning\n\nObtenemos los datos de entrada, que es el estado.\nSeleccionamos la acción usando nuestra política de entrenamiento epsilon-greedy\nEjecutamos la acción y obtenemos el siguiente estado así como la recompensa obtenida\nAlmacenamos en memoria &lt;s,a,r,s’&gt;\nSi tenemos bastantes elementos en la memoria\n\nHacemos un minibatch aleatorio y enteramos la red siendo \\(R+\\gamma argmax_{a'}Q(s',a',w')\\) el target de la red y \\(Q(s,a,w)\\) el valor predicho.\nLa función de pérdida será la de Diferencia de Cuadrados \\(L = (R+\\gamma argmax_a'Q(s',a',w')-Q(s,a,w))^2\\)\n\nDespués de cada C iteraciones, copiaremos los pesos de la red DQN a la DQN_Target\nRepetiremos estos pasos durante M episodios\n\nPseudo-código Deep Q-Learning\n\n\n\n\n\n\nFigure 1.31: Algoritmo Deep Q-Learning - Fuente: Propia\n\n\n\nVariantes de Deep Q-Learning\n\nDouble Deep Q Network (DDQN) – 2015\nDeep Recurrent Q Network (DRQN) – 2015\nDueling Q Network – 2015\nPersistent Advantage Learning (PAL) – 2015\nBootstrapped Deep Q Network – 2016\nNormalized Advantage Functions (NAF) = Continuous DQN – 2016\nN-Step Q Learning – 2016\nNoisy Deep Q Network (NoisyNet DQN) – 2017\nDeep Q Learning for Demonstration (DqfD) – 2017\nCategorical Deep Q Network = Distributed Deep Q Network = C51 – 2017\n\nRainbow – 2017\n\nQuantile Regression Deep Q Network (QR-DQN) – 2017\nImplicit Quantile Network – 2018\n\n\n\n1.9.6 Listado Algoritmos\n1. Model-Free\nValue-based\nQ-learning = SARSA max – 1992\nState Action Reward State-Action (SARSA)– 1994\nDeep Q Network (DQN) – 2013\nDouble Deep Q Network (DDQN) – 2015\nDeep Recurrent Q Network (DRQN) – 2015\nDueling Q Network – 2015\nPersistent Advantage Learning (PAL) – 2015\nBootstrapped Deep Q Network – 2016\nNormalized Advantage Functions (NAF) = Continuous DQN – 2016\nN-Step Q Learning – 2016\nNoisy Deep Q Network (NoisyNet DQN) – 2017\nDeep Q Learning for Demonstration (DqfD) – 2017\nCategorical Deep Q Network = Distributed Deep Q Network = C51 – 2017\n\nRainbow – 2017\n\nQuantile Regression Deep Q Network (QR-DQN) – 2017\nImplicit Quantile Network– 2018\nMixed Monte Carlo (MMC) – 2017\nNeural Episodic Control (NEC) – 2017\nPolicy-based\nCross-Entropy Method (CEM)– 1999\nPolicy Gradient\n\nREINFORCE = Vanilla Policy Gradient(VPG)- 1992\nPolicy gradient softmax\nNatural Policy Gradient (Optimisation) (NPG) / (NPO) – 2002\nTruncated Natural Policy Gradient (TNPG) – 2016\n\nActor-Critic\nAdvantage Actor Critic (A2C) – 2016\nAsynchronous Advantage Actor-Critic (A3C)  – 2016\nGeneralized Advantage Estimation (GAE) – 2015\nTrust Region Policy Optimization (TRPO) – 2015\nDeterministic Policy Gradient (DPG) – 2014\nDeep Deterministic Policy Gradients (DDPG)  – 2015\n\nDistributed Distributional Deterministic Policy Gradients (D4PG) – 2018\nTwin Delayed Deep Deterministic Policy Gradient (TD3) – 2018\n\nActor-Critic with Experience Replay (ACER) – 2016\nActor Critic using Kronecker-Factored Trust Region (ACKTR) – 2017\nProximal Policy Optimization (PPO) – 2017\n\nDistributed PPO (DPPO) – 2017\nClipped PPO (CPPO)  – 2017\nDecentralized Distributed PPO (DD-PPO)– 2019\n\nSoft Actor-Critic (SAC)  – 2018\nGeneral Agents\n\nCovariance Matrix Adaptation Evolution Strategy (CMA-ES)– 1996\nEpisodic Reward-Weighted Regression (ERWR) – 2009\nRelative Entropy Policy Search (REPS)– 2010\nDirect Future Prediction (DFP) – 2016\n\nImitation Learning Agents\nBehavioral Cloning (BC)\nDataset Aggregation (Dagger) (i.e. query the expert) – 2011\nAdversarial Reinforcement Learning\n\nGenerative Adversarial Imitation Learning (GAIL) – 2016\nAdverserial Inverse Reinforcement Learning (AIRL)– 2017\n\nConditional Imitation Learning – 2017\nSoft Q-Imitation Learning (SQIL) – 2019\nHierarchical Reinforcement Learning Agents\n\nHierarchical Actor Critic (HAC) – 2017\n\nMemory Types\n\nPrioritized Experience Replay (PER) – 2015\nHindsight Experience Replay (HER) – 2017\n\nExploration Techniques\n\nE-Greedy\nBoltzmann\nOrnstein–Uhlenbeck process\nNormal Noise\nTruncated Normal Noise\nBootstrapped Deep Q Network \nUCB Exploration via Q-Ensembles (UCB) \nNoisy Networks for Exploration \nIntrinsic Curiosity Module (ICM) – 2017\n\nMeta Learning\n\nModel-agnostic meta-learning (MAML)– 2017\nImproving Generalization in Meta Reinforcement Learning using Learned Objectives (MetaGenRLis) – 2020\n\n2. Model-Based\nDyna-Style Algorithms / Model-based data generation\n\nDynamic Programming (DP) = DYNA-Q – 1990\nEmbed to Control (E2C)– 2015\nModel-Ensemble Trust-Region Policy Optimization (ME-TRPO) – 2018\nStochastic Lower Bound Optimization (SLBO) – 2018\nModel-Based Meta-Policy-Optimzation (MB-MPO) (meta learning) – 2018\nStochastic Ensemble Value Expansion (STEVE) – 2018\nModel-based Value Expansion (MVE) – 2018\nSimulated Policy Learning (SimPLe) – 2019\nModel Based Policy Optimization (MBPO) – 2019\n\nPolicy Search with Backpropagation through Time / Analytic gradient computation\n\nDifferential Dynamic Programming (DDP) – 1970\nLinear Dynamical Systems and Quadratic Cost (LQR) – 1989\nIterative Linear Quadratic Regulator (ILQR) – 2004\nProbabilistic Inference for Learning Control (PILCO) – 2011\nIterative Linear Quadratic-Gaussian (iLQG) – 2012\nApproximate iterative LQR with Gaussian Processes (AGP-iLQR) – 2014\nGuided Policy Search (GPS) – 2013\nStochastic Value Gradients (SVG) – 2015\nPolicy search with Gaussian Process – 2019\n\nShooting Algorithms / sampling-based planning\nRandom Shooting (RS) – 2017\nCross-Entropy Method (CEM)– 2013\n\nDeep Planning Network (DPN)-2018\nProbabilistic Ensembles with Trajectory Sampling (PETS-RS and PETS-CEM) – 2018\nVisual Foresight – 2016\n\nModel Predictive Path Integral (MPPI) – 2015\n\nPlanning with Deep Dynamics Models (PDDM) – 2019\n\nMonte-Carlo Tree Search (MCTS) – 2006\n\nAlphaZero – 2017",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "capitulo2.html",
    "href": "capitulo2.html",
    "title": "2  Modelos Gráficos Probabilísticos y Análisis Causal",
    "section": "",
    "text": "2.1 Redes Bayesianas\nAl contrario de la Estadística tradicional, el aprendizaje bajo la Estadística Bayesiana tiene un enfoque probabilístico. Así, el razonamiento bayesiano supone que:\nComo veremos más adelante, los modelos bayesianos son muy utilizados en todo tipo de investigaciones debido a que proporcionan muy buenos resultados tanto para problemas descriptivos como predictivos:\nAntes de entrar en detalle en la estructura de los métodos bayesianos definir algunos conceptos:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelos Gráficos Probabilísticos y Análisis Causal</span>"
    ]
  },
  {
    "objectID": "capitulo2.html#redes-bayesianas",
    "href": "capitulo2.html#redes-bayesianas",
    "title": "2  Modelos Gráficos Probabilísticos y Análisis Causal",
    "section": "",
    "text": "Las hipótesis están gobernadas por una distribución de probabilidad\nLas decisiones son tomadas de forma “óptima” a partir de las observaciones y dichas probabilidades En este proceso de aprendizaje, las instancias de entrenamiento pueden modificar la probabilidad de una hipótesis, de forma que su planteamiento es mucho menos restrictivo que las técnicas tradicionales (cumplimiento de hipótesis más deterministas). Por tanto, el conocimiento a priori es combinado con las observaciones de los datos con el fin de mejorar el eficiencia de las estimaciones.\n\n\n\nMétodo descriptivo: permite descubrir las relaciones de dependencia/independencia entre las diferentes variables\nMétodo predictivo: son utilizadas como métodos de clasificación. Entre las características de este tipo de técnicas se pueden citar:\nPermite realizar inferencias sobre los datos, lo que conlleva a inducir modelos probabilísticos\nFacilitar la interpretación de otros métodos en términos probabilísticos\nSe necesita conocer un elevado número de probabilidades\nElevado coste computacional al realizar la actualización de las probabilidades\n\n\n\nArco: es un par ordenado (X, Y). En la representación gráfica, un arco (X,Y) viene dado por una flecha desde X hasta Y.\nGrafo dirigido: es un par G = (N, A) donde N es un conjunto de nodos y A un conjunto de arcos definidos sobre los nodos.\nGrafo no dirigido. Es un par G = (N,A) donde N es un conjunto de nodos y A un conjunto de arcos no orientados (es decir, pares noordenados (X,Y)) definidos sobre los nodos. Ciclo: es un camino no dirigido que empieza y termina en el mismo nodo X.\nGrafo acíclico: es un grafo que no contiene ciclos.\nPadre. X es un padre de Y si y sólo si existe un arco X -&gt; Y. Se dice también que Y es hijo de X. Al conjunto de los padres de X se representa como pa(X), y al de los hijos de X por S(X).\nAntepasado o ascendiente. X es un antepasado o ascendiente de Z si y sólo si existe un camino dirigido de X a Z.\nDescendiente. Z es un descendiente de X si y sólo si X es un antepasado de Z. Al conjunto de los descendientes de X lo denotaremos por de(X). - Variable proposicional es una variable aleatoria que toma un conjunto exhaustivo y excluyente de valores. La denotaremos con letras mayúsculas, por ejemplo X, y a un valor cualquiera de la variable con la misma letra en minúscula, x.\nDos variables X e Y son independientes si se tiene que P(X/Y) = P(X). De esta definición se tiene una caracterización de la independencia que se puede utilizar como definición alternativa: X e Y son independientes sí y sólo sí P(X,Y) = P(X)·P(Y).\nDos variables X e Y son independientes dado una tercera variable Z si se tiene que P(X/Y,Z) = P(X/Y). De esta definición se tiene una caracterización de la independencia que se puede utilizar como definición alternativa: X e Y son independientes dado Z sí y sólo sí P(X,Y/Z) = P(X/Z)·P(Y/Z). También se dice que Z separa condicionalmente a X e Y.\n\n\n2.1.1 Modelo Naive Bayes: Hipótesis Map y Teorema de Bayes\nLa inferencia bayesiana es el eje central de los métodos bayesianos. Bajo ella, las hipótesis son expresadas a partir de distribuciones de probabilidad formuladas según los datos observados, \\(p(\\theta)\\), donde \\(\\theta\\) son magnitudes desconocidas. La función verosimilitud, \\(p(y/\\theta)\\), contiene la información disponible en los datos en relación a los parámetros y es ésta la que se usa para actualizar la distribución a priori, \\(p(\\theta)\\)). Finalmente, para llevar a cabo dicha actualización se emplea el Teorema de Bayes.\nPara entender el del Teorema de Bayes es necesario definir los siguientes conceptos:\n\nP(h) es la probabilidad a priori de la hipótesis h. Esta probabilidad contiene la información de que dicha hipótesis sea cierta\nP(D) es la probabilidad a priori de D. Esta es la probabilidad de observar los datos D (sin tener en cuenta la hipótesis que ha de ser cumplida)\nP(h/D) es la probabilidad a posteriori de D, es decir, es la probabilidad de que la hipótesis h una vez los datos D son observados.\nP(D/h) es la probabilidad a posteriori de D, es decir, es la probabilidad de que los datos D sean observados una vez la hipótesis h sea correcta.\n\nSabiendo que la probabilidad conjunta de un evento dado el otro es proporcional a la probabilidad conjunta de ambos ponderada por la probabilidad del evento condicionante, se tiene:\n\\[\nP(h \\cap D) = P(h) \\cdot P(D \\mid h)\n\\]\n\\[\nP(h \\cap D) = P(D) \\cdot P(h \\mid D)\n\\]\nIgualando ambas ecuaciones y manipulando los términos se llega el Teorema de Bayes:\n\\[\nP(h \\mid D) = \\frac{P(h) \\cdot P(D \\mid h)}{P(D)}\n\\]\nDe forma que la probabilidad a posteriori se puede determinar a partir de la probabilidad a priori y un factor de corrección.\nPara una mejora interpretación del Teorema de Bayes se muestra un ejemplo: &gt;En la sala de Pediatría de un determinado hospital el 60% de los pacientes son niñas. De los niños, se conoce que el 35% tienen menos de 24 meses, mientras que para las niñas el 20% son menores de 24 meses. Un médico selecciona una criatura al azar. Si la criatura tiene menos de 24, ¿cuál es la probabilidad de que sea niña? La tabla siguiente muestra la información que se deduce del enunciado:\n\nLa tabla siguiente muestra la información que se deduce del enunciado:\n\n\n\n\nProbabilidad\nValor\n\n\n\n\nP(niño)\n0.40\n\n\nP(niña)\n0.60\n\n\nP(&lt;24m / niño)\n0.35\n\n\nP(&lt;24m / niña)\n0.20\n\n\n\n\nSe obtiene la probabilidad total de que la criatura tenga menos de 24 meses\n\n\\[\nP(&lt;24m) = P(\\text{niño}) \\cdot P(&lt;24m \\mid \\text{niño}) + P(\\text{niña}) \\cdot P(&lt;24m \\mid \\text{niña}) = 0.4 \\cdot 0.35 + 0.6 \\cdot 0.2 = 0.26\n\\]\n\nAplicando el teorema de Bayes:\n\n\\[\nP(\\text{niña} \\mid &lt;24m) = \\frac{P(\\text{niña}) \\cdot P(&lt;24m \\mid \\text{niña})}{P(&lt;24m)} = \\frac{0.6 \\cdot 0.2}{0.26} = 0.46\n\\]\nPor tanto, se tiene un 46% de posibilidades de que el médico haya seleccionado a una niña.\nA partir de la probabilidad a posteriori obtenida mediante la aplicación del Teorema de Bayes, se está en disposición de maximizar tal expresión; es decir, obtener la hipótesis más probable conocida como hipótesis MAP (o máximo a posteriori):\n\\[\nh_{MAP} = \\arg\\max_h P(h \\mid D) = \\arg\\max_h [P(h) \\cdot P(D \\mid h)]\n\\]\nDonde se ha tenido en cuenta que P(D) toma el mismo valor en todas las hipótesis.\n\nSupongamos que estamos tratando de predecir si un estudiante aprueba un examen basándonos en dos características: horas de estudio y nivel de preparación. Nuestras hipótesis son:\n\n\n\n\\(H_1\\): el estudiante aprueba el examen\n\n\n\n\n\\(H_2\\): el estudiante no aprueba el examen\n\n\n\nTenemos los siguientes datos:\n\n\n\n\\(H_1 = 0.7\\): probabilidad de que el estudiante apruebe el examen\n\\(H_2 = 0.3\\): probabilidad de que el estudiante NO apruebe el examen\n\\(P(E\\mid H_1) = 0.8\\): probabilidad de que el estudiante estudie suficiente si aprueba\n\\(P(E\\mid H_2) = 0.8\\): probabilidad de que el estudiante estudie suficiente si NO aprueba\n\n\n\nAhora supongamos que un estudiante estudia durante 4 horas y está muy bien preparado. Queremos calcular las probabilidades a posteriori de que el estudiante apruebe o no apruebe el examen, y determinar la hipótesis MAP.\n\n\n\nCalculamos la probabilidad marginal de observar las evidencias \\(E\\):\n\n\n\\[\nP(E) = P(E \\mid H_1) \\times P(H_1) + P(E \\mid H_2) \\times P(H_2)\n\\]\n\\[\nP(E) = (0.8 \\times 0.7) + (0.3 \\times 0.3) = 0.56 + 0.09 = 0.65\n\\]\n\n\nCalculamos la probabilidad a posteriori de que el estudiante apruebe el examen (\\(H_1\\)) dado que las evidencias \\(E\\) se observan:\n\n\n\\[\nP(H_1 \\mid E) = \\frac{P(E \\mid H_1) \\times P(H_1)}{P(E)}\n\\]\n\\[\nP(H_1 \\mid E) = \\frac{0.8 \\times 0.7}{0.65} = \\frac{0.56}{0.65} \\approx 0.861\n\\]\n\n\nCalculamos la probabilidad a posteriori de que el estudiante no apruebe el examen (\\(H_2\\)) dado que las evidencias \\(E\\) se observan:\n\n\n\\[\nP(H_2 \\mid E) = \\frac{P(E \\mid H_2) \\times P(H_2)}{P(E)}\n\\]\n\\[\nP(H_2 \\mid E) = \\frac{0.3 \\times 0.3}{0.65} = \\frac{0.09}{0.65} \\approx 0.138\n\\]\n\nPor tanto, la hipótesis más probable es que el estudiante apruebe el examen dado que ha estudiado durante 4 horas y está bien preparado.\n\n\nNota: Dado que \\(P(E)\\) es constante para ambas hipótesis, se podría haber comparado directamente \\(P(H_1 \\mid E)\\) y \\(P(H_2 \\mid E)\\) para determinar la hipótesis MAP.\n\nEl uso de la hipótesis MAP puede ser aplicado para resolver problemas de clasificación.\nComo sabemos, en dichas investigaciones se tiene una variable independiente conocida como clase o target y un conjunto de variables predictoras o atributos. Así, el Teorema de Bayes se puede reescribir como:\n\\[\nP(C \\mid (A_1, A_2, \\ldots, A_N)) = \\frac{P(C) \\cdot P((A_1, A_2, \\ldots, A_N) \\mid C)}{P(A_1, A_2, \\ldots, A_N)}\n\\]\nDonde C denota el target o clase y \\(A_i\\) el conjunto de variables explicativas.\nHaciendo máxima la probabilidad de C dado los atributos se tiene:\n\\[\nc_{MAP} = \\underset{c \\in \\Delta}{\\arg\\max} \\ P(C \\mid (A_1, A_2, \\ldots, A_N)) = P(C) \\cdot P((A_1, A_2, \\ldots, A_N) \\mid C)\n\\]\nsiendo \\(\\Delta\\) el conjunto de valores que puede tomar la variable objetivo (target del problema).\nComo puede verse, el enfoque planteado es bastante sencillo pero también muy costoso desde el punto de vista computacional ya que es necesario conocer las distribuciones de probabilidad de las variables implicadas en la investigación.\n\n\n2.1.2 Modelo Naive-Bayes\nEl clasificador Naïve-Bayes es una versión simplificada del proceso de modelización anterior. Este método supone que todos los atributos son independientes conocido el valor de la variable clase de forma que la función de probabilidad conjunta queda como:\n\n\n\\[P(C \\mid (A_1, A_2, \\ldots, A_N)) = P(C) \\cdot \\prod_{i=1}^{N} P(A_i \\mid C)\\]\n\n\nComo es de esperar, el supuesto que subyace este clasificador no es muy realista; si bien, alcanza muy buenos resultados por lo que su uso está muy extendido en la comunidad de científico de datos.\n\n\n\n\n\n\nFigure 2.1: Naive bayes\n\n\n\nComo en el caso anterior, se obtiene la hipótesis que maximiza la probabilidad del valor de la clase.\n\n\n\\[c_{MAP} = \\underset{c \\in \\Delta}{\\arg\\max} \\left( P(C) \\cdot \\prod_{i=1}^{N} P(A_i \\mid C) \\right)\\]\n\n\nEl clasificador Naïve-Bayes puede emplearse tanto con variables explicativas discretas como numéricas.\nCuando las variables explicativas son discretas, la probabilidad condicional es obtenida a partir de la frecuencia de los datos muestrales; de forma que ésta se define como el número de casos favorables entre el número de casos posibles. Matemáticamente, se tiene:\n\n\n\\[P(x_i \\mid \\text{pa}(x_i)) = \\frac{n(x_i, \\text{pa}(x_i))}{n(\\text{pa}(x_i))}\\]\n\n\nDonde \\(n(x_i, Pa(x_i ))\\) denota el número de registros de la muestra en el que la variable \\(X_i\\) toma el valor \\(x_i\\) y \\(pa(x_i )\\) los padres de \\(X_i\\). Notar que el padre de cada variable explicativa es la variable independiente, la cual se ha denominado target o clase.\nEn el caso en que el tamaño de la muestra de trabajo sea pequeño, el uso de las frecuencias puede ocasionar estimaciones poco fiables por lo que se emplean estimadores basados en suavizados. Uno de los más empleados es el estimador de Laplace en el que la probabilidad viene expresada por el número de casos favorables + 1 dividida por el de casos totales más el número de alternativas.\n\n\n\\[P(x_i \\mid \\text{Pa}(x_i)) = \\frac{n(x_i, \\text{pa}(x_i)) + 1}{n(\\text{pa}(x_i)) + \\alpha}\\]\n\n\nPor su parte, si se dispone de variables numéricas el estimador Naïve-Bayes supone que dichas variables siguen una distribución normal donde la media y la desviación típica son estimadas a partir de los datos de la muestra. Sin embargo, en la mayor parte de las ocasionales, las variables continuas no suelen seguir una distribución de probabilidad normal es posible que las estimaciones sean poco eficientes por lo que se recomienda transformar dichas variables en cualitativas (por ejemplo: empleando los intervalos que se obtienen al tomar los cuantiles de su distribución).\nimport os\nimport numpy as np\nimport pandas as pd\n\ndatos = pd.read_csv(\"../datos/credit_g.csv\")\n\ndatos.info()\n# Pasamos las variables a categóricas\ndatos['checking_status'] = datos['checking_status'].astype('category')\ndatos['credit_history'] = datos['credit_history'].astype('category')\ndatos['purpose'] = datos['purpose'].astype('category')\ndatos['savings_status'] = datos['savings_status'].astype('category')\ndatos['employment'] = datos['employment'].astype('category')\ndatos['personal_status'] = datos['personal_status'].astype('category')\ndatos['other_parties'] = datos['other_parties'].astype('category')\ndatos['property_magnitude'] = datos['property_magnitude'].astype('category')\ndatos['other_payment_plans'] = datos['other_payment_plans'].astype('category')\ndatos['housing'] = datos['housing'].astype('category')\ndatos['job'] = datos['job'].astype('category')\ndatos['property_magnitude'] = datos['property_magnitude'].astype('category')\ndatos['own_telephone'] = datos['own_telephone'].astype('category')\ndatos['foreign_worker'] = datos['foreign_worker'].astype('category')\ndatos['class'] = datos['class'].astype('category')\n# La variable class es una variable reservada en diferentes módulos de Python -&gt; reemplazar por por target\ndatos.rename(columns={'class': 'target'}, inplace=True)\ndatos['target']=np.where(datos['target']=='good', 0, 1) # cambio en la codificación por sencillez en el preprocesado\n# Definición de la muestra de trabajo\ndatos_entrada = datos.drop('target', axis=1) # Datos de entrada\ndatos_entrada = pd.get_dummies(datos_entrada, drop_first=True, dtype=int) #conversión a variables dummy\n\ntarget = datos[\"target\"] # muestra del target\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV\n\n# Partición de la muestra\n\ntest_size = 0.3 # muestra para el test \nseed = 222 # semilla\n\nX_train, X_test, y_train, y_test = train_test_split(\n    datos_entrada, target, test_size=test_size, random_state=seed, stratify=target\n)\n\n# Estandarización de la muestra\nesc = StandardScaler().fit(X_train) # valores media y std de los datos de train\n\n# aplicación a los datos de train y test\nX_train_esc = esc.transform(X_train)\nX_test_esc = esc.transform(X_test)\n\n# Validación cruczada\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=seed)\n\n2.1.2.1 Bernoulli Naive Bayes\nfrom sklearn.naive_bayes import BernoulliNB \nbernoulli_nb=BernoulliNB(force_alpha=False)\n\ngrid=[{'alpha': list(np.arange(0.05, 1, 0.1)), 'binarize': [0.3, 0.1, 0.0]}]\n# Definición del modelo con hiperparámetros\ngs_bernoulli_nb = GridSearchCV(\n    estimator=bernoulli_nb, param_grid=grid, scoring='accuracy', cv=cv, n_jobs=1, return_train_score=False\n)\ngs_bernoulli_nb = gs_bernoulli_nb.fit(X_train, y_train)\n\nprint(f'Naive-Bayes (Bernoulli) (parámetros): {gs_bernoulli_nb.best_params_}') # parámetros del modelo final\n\nbernoulli_nb = gs_bernoulli_nb.best_estimator_ # modelo final\n# Resultados importantes de estos algoritmos (acceso dentro del objeto del modelo)\nprint(bernoulli_nb.class_log_prior_)  # logaritmo de la probabilidad de cada clase\nprint(bernoulli_nb.class_log_prior_)  # logaritmo de la probabilidad de cada clase\nbernoulli_nb.feature_log_prob_ # logaritmo de la probabilidad de la variable dada la clase (P(Xi|Y)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix\n\nimport warnings\n# Suprimir todas las advertencias\nwarnings.simplefilter(\"ignore\")\n\n\n# Predicciones muestra entrenamiento y test\n\npreds_train = bernoulli_nb.predict(X_train)\npreds_test = bernoulli_nb.predict(X_test)\n\n# Cálculo métricas bondad de ajuste \nprint('Accuracy')\nprint('------------------------------')\nprint(f'Entrenamiento (cv): {round(gs_bernoulli_nb.best_score_,5)}')\naccuracy_test = accuracy_score(y_test, preds_test)\nprint(f'Test: {round(accuracy_test,5)}')\n\n# AUC - test y curva roc (final\ny_pred_test = bernoulli_nb.predict_proba(X_test)\nfp_rate_test, tp_rate_test, thresholds = roc_curve(y_test, y_pred_test[:,1])\nauc_test = auc(fp_rate_test, tp_rate_test)\n\n# Bondad de ajuste: matriz de confusión y curva roc para los datos de test\n\nf, axes = plt.subplots(1, 2, figsize=(10,5))\n\nsns.heatmap(confusion_matrix(preds_test, y_test), annot = True, cmap = plt.cm.Reds, fmt='.0f', ax=axes[0]) # matriz de confusión\nsns.lineplot(x=fp_rate_test, y=tp_rate_test, color='skyblue', label='AUC = %0.2f' % auc_test, ax=axes[1]) # curva roc\n\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n2.1.2.2 Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB  \n\ngaussian_nb = GaussianNB()\ngrid=[{'var_smoothing': list(np.arange(0,0.1, 0.02))}]\n\n# Definición del modelo con hiperparámetros\ngs_gaussian_nb=GridSearchCV(\n    estimator=gaussian_nb, param_grid=grid, scoring='accuracy', cv=cv, n_jobs=1, return_train_score=False\n)\n\ngs_gaussian_nb = gs_gaussian_nb.fit(X_train, y_train)\nprint('Naive-Bayes (Bernoulli) (parámetros):', gs_gaussian_nb.best_params_) \n\n#parámetros del modelo final\ngaussian_nb = gs_gaussian_nb.best_estimator_ #modelo final\n\n# predicciones muestra entrenamiento y test\n\npreds_train = gaussian_nb.predict(X_train)\npreds_test = gaussian_nb.predict(X_test)\n\n# Cálculo métricas bondad de ajuste \n\nprint('Accuracy')\nprint('------------------------------')\nprint(f'Entrenamiento (cv):, {round(gs_gaussian_nb.best_score_,5)}')\naccuracy_test = accuracy_score(y_test, preds_test)\nprint('Test:', round(accuracy_test,5))\n\n#AUC - test y curva roc (final)\n\ny_pred_test = gaussian_nb.predict_proba(X_test)\nfp_rate_test, tp_rate_test, thresholds = roc_curve(y_test, y_pred_test[:,1])\nauc_test = auc(fp_rate_test, tp_rate_test)\n\n# Bondad de ajuste: matriz de confusión y curva roc para los datos de test\n\nf, axes = plt.subplots(1, 2, figsize=(10,5))\n\nsns.heatmap(confusion_matrix(preds_test, y_test), annot = True, cmap = plt.cm.Reds, fmt='.0f', ax=axes[0]) # matriz de confusión\nsns.lineplot(x=fp_rate_test, y=tp_rate_test, color='skyblue', label='AUC = %0.2f' % auc_test, ax=axes[1]) # curva roc\n\nplt.legend(loc=\"lower right\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelos Gráficos Probabilísticos y Análisis Causal</span>"
    ]
  },
  {
    "objectID": "capitulo2.html#modelos-bayesianos",
    "href": "capitulo2.html#modelos-bayesianos",
    "title": "2  Modelos Gráficos Probabilísticos y Análisis Causal",
    "section": "2.2 Modelos Bayesianos",
    "text": "2.2 Modelos Bayesianos\nLas redes bayesianas son métodos estadísticos que representan la incertidumbre a través de las relaciones de independencia condicional que se establecen entre ellas. Por tanto, permiten modelar un fenómeno a partir de dichas relaciones y hacer inferencia.\nEste tipo de métodos son una representación gráfica de dependencias para razonamiento probabilístico, en las que los nodos representan variables aleatorias y los arcos las relaciones de dependencia directa entre las variables.\n\n\n\n\n\n\nFigure 2.2: Topologia Bayesiana\n\n\n\nLa ventaja de las redes bayesianas frente a otros métodos es la posibilidad de codificar las dependencias/independencias relevantes considerando no sólo las dependencias marginales sino también las dependencias condicionales entre un conjunto de variables.\nEn definitiva, las redes bayesianas modelan las relaciones entre las variables tanto de forma cualitativa como cuantitativa. La fuerza de dichas relaciones viene dada en las distribuciones de probabilidad como una medida de la creencia que tenemos sobre esas relaciones en el modelo.\n\n2.2.1 Formulación general\nUna red bayesiana queda especificada formalmente por una dupla B=(G,Θ) donde G es un grafo dirigido acíclico (DAG, por las siglas en inglés) y Θ es el conjunto de distribuciones de probabilidad. Definimos un grafo como un par G = (V, E), donde V es un conjunto finito de vértices, nodos o variables, y E es un subconjunto del producto cartesiano VxV de pares ordenados de nodos que llamamos enlaces o aristas. Por tanto, puede decirse que las redes bayesianas representan el conocimiento cualitativo del modelo mediante el grafo dirigido acíclico.\n\nSupongamos una red bayesiana que contine un padre A y 3 hijos (B, C y D), siendo C también padre de B. El DAG que definido sería:\n\nimport bnlearn as bn\nimport matplotlib.pyplot as plt\n\nedges = [('A', 'B'), ('A', 'C'), ('A', 'D'), ('C', 'B')]\nDAG = bn.make_DAG(edges, methodtype=\"bayes\")\n\nbn.plot(DAG, interactive=False)\nplt.show()\n\n# print(DAG[\"adjmat\"])  # podemos ver el dag en formato tabla (no visual cuando existen muchos nodos)\nEl grafo define un modelo probabilístico mediante el producto de varias funciones de probabilidad condicionada:\n\n\n\\[P(x_1, \\ldots, x_n) = \\prod_{i=1}^{N} P(x_i \\mid \\text{pa}(x_i))\\]\n\n\nCon \\(pa(x_i)\\) las variables inmediatamente predecesoras de la variable \\(X_i\\). En este sentido, los valores de probabilidades \\(P(x_i⁄pa(x_i ))\\) son “almacenados” en el nodo que precede a la variable \\(X_i\\).\nEs importante resaltar que de no existir la expresión anterior, la red debiese ser descrita a partir de la probabilidad conjunta, lo que obligaría a trabajar con un número de parámetros mucho más elevado (creciente de forma exponencial en el número de nodos).\n\n\n2.2.2 Independencia condicional e inferencia de la red\nComo se ha comentado anteriormente, una variable X es condicionalmente independiente de otra variable Y dada una tercera Z si, el hecho de que se tenga conocimiento Z, hace que Y no tenga influencia en X.\n\n\n\\[P(X|Y,Z)=P(X|Z)\\]\n\n\nPor tanto, la hipótesis de independencia condicional establece que cada nodo debe ser independiente de los otros nodos de la red (salvo sus descendientes) dados sus padres. Dicho de otro modo, si se conocen los padres de una variable, ésta se vuelve independiente del resto de sus predecesores.\n\nVeamos un ejemplo para facilitar la comprensión de la independencia condicional.\n\n\n\n\n\n\n\nFigure 2.3: Topologia Bayesiana\n\n\n\n\nPartiendo de la red bayesiana de la imagen anterior, la probabilidad conjunta se define como:\n\n\\[\\begin{align}\nP(X_1, X_2, \\ldots, X_9) &= P(X_1) \\cdot P(X_2) \\cdot P(X_3 \\mid X_2, X_1) \\cdot P(X_4 \\mid X_3, X_2, X_1) \\\\\n&\\quad \\cdot P(X_5 \\mid X_4, X_3, X_2, X_1) \\cdot P(X_6 \\mid X_5, X_4, X_3, X_2, X_1) \\\\\n&\\quad \\cdot P(X_7 \\mid X_6, X_5, X_4, X_3, X_2, X_1) \\\\\n&\\quad \\cdot P(X_8 \\mid X_7, X_6, X_5, X_4, X_3, X_2, X_1) \\\\\n&\\quad \\cdot P(X_9 \\mid X_8, X_7, X_6, X_5, X_4, X_3, X_2, X_1)\n\\end{align}\\]\n\nEn cambio, como las probabilidades condicionales solo dependen de sus padres (teorema anterior), la probabilidad conjunta toma la siguiente forma:\n\n\\[\\begin{align}\nP(X_1, X_2, \\ldots, X_9) &= P(X_1) \\cdot P(X_2) \\cdot P(X_3 \\mid X_2) \\cdot P(X_4 \\mid X_2, X_1) \\\\\n&\\quad \\cdot P(X_5 \\mid X_4) \\cdot P(X_6 \\mid X_4) \\cdot P(X_7 \\mid X_4) \\\\\n&\\quad \\cdot P(X_8 \\mid X_3) \\cdot P(X_9 \\mid X_3)\n\\end{align}\\]\nPor tanto, *la propiedad de independencia de las redes bayesianas hace que se reduzca en gran medida los cálculos**.\nEn una red bayesiana, se conoce como inferencia probabilística a la propagación del conocimiento a través de la misma una vez se tienen nuevos datos. Este proceso se lleva a cabo actualizando las probabilidades a posteriori en toda la estructura de la red mediante el Teorema de Bayes.\nComo es de imaginar, el proceso de inferencia es muy costoso computacionalmente de forma que, dependiendo de las necesidades, se emplean algoritmos exactos o aproximados:\n\nExactos: cuando puede calcularse la inferencia de forma exacta. El coste computacional necesario para la actualización de las probabilidades es viable\nAproximados: se usan técnicas de muestreo que permita calcular de forma aproximada la inferencia. Usado cuando no es viable obtener la propagación exacta en un tiempo razonable\n\n\n\n2.2.3 Aprendizaje de las redes bayesianas\nComo se ha visto, para determinar una red bayesiana es necesario especificar su estructura gráfica y una función de probabilidad conjunta. Dicho proceso es bastante laborioso debido a que, en muchos casos, se desconoce ambas especificaciones. Para paliar esta circunstancia, se han desarrollado diferentes métodos de aprendizaje. Así, el proceso de aprendizaje de una red bayesiana puede dividirse en dos estapas:\n\nEstructural (o dimensión cualitativa): búsqueda en el espacio de posibles redes\nParamétrico (o dimensión cuantitativa): aprende la distribución de probabilidad a partir de los datos, dada la red\n\nEl aprendizaje paramétrico consiste en hallar los parámetros asociados a la estructura de la red. Estos parámetros están constituidos por las probabilidades de los nodos raíz y las probabilidades condicionales de las demás variables dados sus padres. Las probabilidades previas se corresponden con las marginales de los nodos raíz y las condicionales se obtienen de las distribuciones de cada nodo con sus padres.\nEn el aprendizaje estructural es donde se establecen las relaciones de dependencia que existen entre las variables del conjunto de datos para obtener el mejor grafo que represente estas relaciones. Este problema se hace prácticamente intratable desde el punto de vista computacional cuando el número de variables es grande. Por ello, suelen emplearse algoritmos de búsqueda para aprender la estructura de la red.\nA continuación, se presentan algunos algoritmos de búsqueda para establecer la estructura de una red bayesiana.\nAlgoritmo K2\nEl algoritmo K2 es considerado el predecesor de otros algoritmos de búsqueda más sofisticados. basado en búsqueda y optimización de una métrica bayesiana es considerado como el predecesor y fuente de inspiración para las generaciones posteriores. El proceso de búsqueda de este algoritmo está dividido en las siguientes etapas: - Ordenación de los nodos (variables de entrada) de forma que los posibles padres de una variable aparezcan siempre antes de ella para evitar la generación de ciclos. Esta restricción provoca que el algoritmo busque los padres posibles entre las variables predecesoras (ventaja computacional) - Partiendo de este orden establecido, se calcula la ganancia que se produce en la medida al introducir una variable como padre\nFinalmente, el proceso se repite para cada nodo mientras el incremento de calidad supere un cierto umbral preestablecido.\nAlgoritmo B\nEste algoritmo elimina la dependencia de la ordenación previa de los nodos de forma que su coste de computación es superior al algoritmo K2. complejidad computacional es mayor. Como en el caso anterior, el proceso es iniciado con padres vacíos con padres vacíos y en cada etapa se añade aquel enlace que maximice el incremento de calidad eliminando aquellos que producen ciclos. El proceso es detenido cuando una vez la inclusión de un arco no represente ninguna ganancia.\nAlgoritmo Hill Climbing\nEl algoritmo Hill Climbing (HC) es un procedimiento de búsqueda que parte de una solución inicial y, a partir de ésta, mediante técnicas heurística se calcula el nuevo valor utilizando todas las soluciones vecinas a la solución actual, seleccionando el vecino que mejor solución presenta. Por tanto, este algoritmo finaliza cuando no existe ningún vecino que pueda mejorar la solución vecina.\nUna variante muy útil y muy empleada consiste en considerar todos los posibles movimientos a partir del estado actual y elegir el mejor de ellos como nuevo estado. A este método se le denomina ascensión por la máxima pendiente o búsqueda del gradiente.\n\nVamos a mostrar un ejemplo de aprendizaje de la estructura en python:\n\nimport pandas as pd\n\ndatos = pd.read_csv(\"../datos/bayesian_data.csv\", sep=\";\", index_col=\"Unnamed: 0\")\ndatos = datos.rename(columns={'class': 'target'})  # target con 4 categorías\n\n\n# Modelo de estructura\nstructure_model = bn.structure_learning.fit(datos, methodtype='tan', root_node=\"doors\", class_node=\"target\") # uso de hill-climbing\n\n# nota: en este caso no estamos definiendo un padre para obtener la estructura bayesian\nstructure_model[\"adjmat\"]\nimport matplotlib.pyplot as plt\nbn.plot(model)\nplt.show()\n\nTanto del cuadro como del grafo, podemos ver que:\n\n\n\ntarget es padre de: safety, lug_boot y person\ntarget es hijo de: buying y maint\n\n\n\n\n2.2.4 Clasificadores\nComo determinar la estructura de la red bayesiana es una tarea realmente compleja, la mayor parte de los modelos de clasificación basados en redes bayesianas suelen ser modificaciones del clasificador Naïve-Bayes.\nA día de hoy, existen muchos clasificadores de forma que se exponen brevemente tres de los más utilizados.\nTan: Tree Augmented Naïve Bayes\nEn el modelo TAN todos los atributos tienen como padre a otro atributo como mucho, además de la clase en sí, de forma que cada atributo obtiene un arco aumentado apuntando a él. \nBan: Naïve Bayes aumentado\nEn este modelo se incorporan nuevos arcos entre todas las variables con la limitación de que no formen ciclos. Destacar la relevancia de este clasificador ya que su estructura es capaz de representar cualquier forma de red bayesiana. \nAODE: Average One-Dependence Estimators\nAl igual que el algoritmo TAN, cada variable tiene como padre a la variable clase y como máximo a otro atributo. Sin embargo, la principal diferencia respecto al modelo anterior tiene lugar en la forma de obtener la predicción definitiva del modelo. Dicha predicción consiste en: - El algoritmo establece posibles estructuras de red compatibles con el problema y, en función de ésta, hace una predicción de la clase - La predicción final se obtiene como la media ponderada de las predicciones anteriores \nUna vez visto la parte teórica entramos en detalle a nivel práctico.\nstructure_tan_model = bn.structure_learning.fit(\n    datos,\n    methodtype='tan',\n    root_node=\"doors\", # hay que tener en cuenta algún hijo que no tenga más padre que el target\n    class_node=\"target\"  # en el modelo tan hay que tener una clase/padre)\n) \nparameter_model = bn.parameter_learning.fit(structure_tan_model, datos, methodtype='bayes', verbose=0) \nstructure_tan_model[\"model_edges\"] # bordes y nodos. También podría pintarse como en el caso anterior\n\nObención de las probabilidades condicionadas\n\n# Probabilidades condicionadas\n\nCPDs = bn.print_CPD(parameter_model, verbose=0)  # esto es un diccionario de dataframes (clave cada columna del df\n- Para doors:\nCPDs[\"doors\"][CPDs[\"doors\"][\"target\"] == 0]\n- Para maint (y primera clase del target):\nCPDs[\"maint\"][CPDs[\"maint\"][\"target\"] == 0]\nObtención de las Predicciones sobre la muestra\nfeats = list(datos.columns)\nfeats.remove(\"target\")\n\n# dado las evidencias de dos variables, calculamos la probabilidad de la clase\nquery = bn.inference.fit(parameter_model, variables=[\"target\"], evidence={'doors':2, 'lug_boot': 'small'}, verbose=0)\n\nquery.df\nPor último, presentamos un ejemplo de uso de clasificador bayesiano empleando la librería pyAgrum. Esta librería es que es un contenedor de Python para la biblioteca aGrUM de C++. Proporciona una interfaz de alto nivel a la parte de aGrUM que permite crear, modelar, aprender, usar, calcular e integrar redes bayesianas y otros modelos gráficos probabilísticos como las redes de Markov o los modelos relacionales probabilísticos.\nLa librería se integra adecuadamente con scikit-learn por lo que se recomienda su uso para desarrollar clasificadores bayesianos.\nimport os\n\nimport pandas as pd\nimport numpy as np\n\nimport pyAgrum.skbn as skbn\nimport pyAgrum.lib.notebook as gnb\n\ndatos = pd.read_csv(\"../datos/credit_g.csv\")\n\ndatos.info()\n# Pasamos las variables a categóricas\ndatos['checking_status'] = datos['checking_status'].astype('category')\ndatos['credit_history'] = datos['credit_history'].astype('category')\ndatos['purpose'] = datos['purpose'].astype('category')\ndatos['savings_status'] = datos['savings_status'].astype('category')\ndatos['employment'] = datos['employment'].astype('category')\ndatos['personal_status'] = datos['personal_status'].astype('category')\ndatos['other_parties'] = datos['other_parties'].astype('category')\ndatos['property_magnitude'] = datos['property_magnitude'].astype('category')\ndatos['other_payment_plans'] = datos['other_payment_plans'].astype('category')\ndatos['housing'] = datos['housing'].astype('category')\ndatos['job'] = datos['job'].astype('category')\ndatos['property_magnitude'] = datos['property_magnitude'].astype('category')\ndatos['own_telephone'] = datos['own_telephone'].astype('category')\ndatos['foreign_worker'] = datos['foreign_worker'].astype('category')\ndatos['class'] = datos['class'].astype('category')\n\n# La variable class es una variable reservada en diferentes módulos de Python -&gt; reemplazar por por target\ndatos.rename(columns={'class': 'target'}, inplace=True)\ndatos['target']=np.where(datos['target']=='good', 0, 1) # cambio en la codificación por sencillez en el preprocesado\n\n# Definición de la muestra de trabajo\ndatos_entrada = datos.drop('target', axis=1) # Datos de entrada\ndatos_entrada = pd.get_dummies(datos_entrada, drop_first=True, dtype=int) #conversión a variables dummy\n\ntarget = datos[\"target\"] # muestra del target\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV\n\n# Partición de la muestra\n\ntest_size = 0.3 # muestra para el test \nseed = 222 # semilla\n\nX_train, X_test, y_train, y_test = train_test_split(\n    datos_entrada, target, test_size=test_size, random_state=seed, stratify=target\n)\n\n# Estandarización de la muestra\nesc = StandardScaler().fit(X_train) # valores media y std de los datos de train\n\n# aplicación a los datos de train y test\nX_train_esc = esc.transform(X_train)\nX_test_esc = esc.transform(X_test)\n# Creación del clasificador TAN en python\nbayesian_network = skbn.BNClassifier(\n    learningMethod='TAN',\n    prior='Smoothing',\n    scoringType='BIC',\n    priorWeight=0.5,\n    discretizationStrategy='quantile',\n    usePR=True,\n    significant_digit = 6\n)\n\nbayesian_network.fit(X_train, y_train) # ajuste del modelo\nfrom sklearn.metrics import accuracy_score\n\n# predicciones para la muestra de train y test\n\ntrain_probs = bn.predict_proba(X_train)  \ntest_probs = bn.predict_proba(X_test)\n\n# predict-proba proporciona las probabilidades\n\ndef preds_ones(probs, threshold = 0.5):\n    return np.where(probs[:, 0] &gt; threshold, 0, 1)\n\ny_train_pred = preds_ones(train_probs)\ny_test_pred = preds_ones(tests_probs)\n\nprint(f'Accuracy (train) {round(accuracy_score(y_train, y_train_pred),2)}')\nprint(f'Accuracy (test) {round(accuracy_score(y_test, y_test_pred), 2)}')",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelos Gráficos Probabilísticos y Análisis Causal</span>"
    ]
  },
  {
    "objectID": "capitulo2.html#modelos-ocultos-de-markov",
    "href": "capitulo2.html#modelos-ocultos-de-markov",
    "title": "2  Modelos Gráficos Probabilísticos y Análisis Causal",
    "section": "2.3 Modelos Ocultos de Markov",
    "text": "2.3 Modelos Ocultos de Markov\n\n2.3.1 Cadenas de Markov\nUna cadena de Markov es un sistema matemático que experimenta transiciones de un estado a otro de acuerdo con un conjunto dado de reglas probabilísticas. La siguiente imagen presenta una representación gráfica de una cadena de Markov.\n\n\n\n\n\n\nFigure 2.4: Cadena Markow\n\n\n\nComo puede verse, una cadena de Markov puede ser planteada como un gráfico dirigido en el que los nodos son los estados y los arcos contienen la probabilidad de pasar de un estado a otro.\nLas cadenas de Markov son procesos estocásticos pero se diferencian en que carecen de memoria. Así, en un proceso de Markov la probabilidad del siguiente estado del sistema depende solamente del estado actual del sistema y no de ningún estado anterior.\n\n\\[P(x_i│x_0 … x_{i-1})= P(x_i│x_{i-1})\\]\n\nLa expresión anterior se conoce como propiedad de Markov.\nEs importante destacar que una cadena de Markov puede ser vista como una red bayesiana en la que cada nodo tiene una tabla de probabilidad correspondiente a \\(P(x_t│x_{t-1})\\) y es a misma para todos los nodos salvo para el instante inicial.\n\n\n\n\n\n\nFigure 2.5: Cadena Markov\n\n\n\nEn toda cadena de Markov es necesario definir una matriz de transición, T, la cual contiene la información sobre la probabilidad de transición entre los diferentes estados del sistema. Como hecho relevante, cada fila de la matriz debe ser un vector de probabilidad y la suma de todos sus términos debe ser igual a la unidad.\nAsimismo, las matrices de transición tienen la propiedad de que el producto de las matrices posteriores puede describir las probabilidades de transición a lo largo de un intervalo de tiempo. Esta característica permite modelar la probabilidad de estar en un determinado estado después de n pasos como:\n\n\\[p^n= p^0* T^n\\]\n\nVeamos un ejemplo con el que facilitar la comprensión del funcionamiento de una cadena de Markov.\n\nUn grupo farmacéutico ha sacado al mercado tres pomadas hace pocas semanas. Con el fin de conocer su acogida así como el comportamiento futuro de los potenciales clientes ante las tres variantes del producto ha realizado un estudio de mercado. De dicho estudio se conocen las probabilidades de cambio de un tipo de pomada a otra.\n\n\nLa matriz de transición para T es:\n\n\n\n\n\\[\nT = \\begin{pmatrix}\n0.80 & 0.10 & 0.10 \\\\\n0.03 & 0.95 & 0.02 \\\\\n0.20 & 0.05 & 0.75 \\\\\n\\end{pmatrix}\n\\]\n\n\n\n\nSabiendo que actualmente, la participación en el mercado de las tres pomadas es:\n\n\n\n\n\\[\np = \\begin{pmatrix}\n0.30 \\\\\n0.45 \\\\\n0.25 \\\\\n\\end{pmatrix}\n\\]\n\n\n\n\n¿Cuáles serán las participaciones de mercado de cada marca en dos meses más?\n\n\nLa matriz de transición para \\(T^2\\) es:\n\n\n\n\n\\[\nT^2 = \\begin{pmatrix}\n0.663 & 0.180 & 0.155 \\\\\n0.057 & 0.907 & 0.037 \\\\\n0.312 & 0.105 & 0.584 \\\\\n\\end{pmatrix}\n\\]\n\n\n\n\nDe forma que usando la fórmula anterior, se tiene:\n\n\n\n\n\\[\np^2 = p^0 \\cdot T^2 = \\begin{pmatrix}\n0.30 & 0.45 & 0.25\n\\end{pmatrix} \\begin{pmatrix}\n0.663 & 0.180 & 0.155 \\\\\n0.057 & 0.907 & 0.037 \\\\\n0.312 & 0.105 & 0.584 \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n0.302 & 0.488 & 0.209\n\\end{pmatrix}\n\\]\n\n\n\n\nEn vista de los resultados, la cuota de mercado de cada tipo de pomada variará en los dos meses siguientes en: - Pomada 1: de un 30% a 30,2% (estable) - Pomada 2: de un 45% a un 48,8% (leve aumento) - Pomada 3: de un 25% a un 20,9% (ligera caída)\n\n\n\n2.3.2 Cadena de Markov absorvente\nUna cadena de Markov absorbente es una cadena de Markov en la que para algunos estados una vez ingresados, no es posible salir. Sin embargo, este es solo uno de los requisitos previos para que una cadena de Markov sea una cadena de Markov absorbente. Para que sea una cadena de Markov absorbente, todos los demás estados transitorios deben poder alcanzar el estado absorbente con una probabilidad de 1.\nCon el fin de ayudar al entendimiento del comportamiento de una cadena de Markov arbsorvente, se plantea una simulación en python sobre la calidad creditia de n individuos y su comportamiento durante un año (12 pagos).\nSuponiendo un modelo de impago bancario con los siguientes tres estados: - Pago al día - Pago con retraso - Impago (estado absorbente)\nAsí, la matriz de transición para esta cadena de Markov es:\n\n\n\\[\nT = \\begin{pmatrix}\n0.8 & 0.1 & 0.0 \\\\\n0.2 & 0.4 & 0.4 \\\\\n0.0 & 0.0 & 1.0 \\\\\n\\end{pmatrix}\n\\]\n\n\nEsto significa que hay un 80% de probabilidad de que un individuo que paga al día continúe pagando al día, un 20% de probabilidad de que pase a un estado de pago con retraso, y un 0% de probabilidad de que entre en estado de impago (para pasar a impago debe pasar previamente por pago con retraso). Además, hay un 20% de probabilidad de que un individuo en estado de pago con retraso vuelva al estado de pago al día, un 40% de probabilidad de que permanezca en estado de pago con retraso y un 20% de probabilidad de que entre en estado de impago. Por último, el estado de impago es absorbente, lo que significa que una vez que un individuo entra en estado de impago, permanece allí indefinidamente.\nimport numpy as np\n\nnp.random.seed(123)\n\n# Matriz de transición completa\ntransition_matrix = np.array([[0.8, 0.2, 0.0],  # De pago al día a pago con retraso o impago\n                              [0.45, 0.4, 0.15],  # De pago con retraso a pago al día o impago\n                              [0.0, 0.0, 1.0]]) # De impago a impago (estado de absorción)\n\n# Muestra de individuos + número de pagos\nn_samples = 10\nn_pagos = 12\n\ny = np.zeros(n_samples, dtype=int)  # Todos los individuos comienzan en estado de pago al día\n\nmuestra_dict = {} # Diccionario para recoger los pagos de cada muestra\nfor i in range(n_samples):\n    # Generar transiciones de estado basadas en la matriz de transición completa\n    current_state = 0  # Estado inicial: pago al día\n    pagos_muestra_list = [] # Obtener secuencia en cada mes de pago\n    for _ in range(n_pagos):  # Realizar los 12 pagos\n        if current_state == 0:  # Si estamos en el estado de pago al día\n            # solo nos quedamos con las posibles transiciones (no es posible ir al impago sin tener retraso en pago)\n            next_state = np.random.choice([0, 1], p=transition_matrix[current_state][0:2])\n        elif current_state == 1:  # Si estamos en el estado de pago con retraso\n            # una vez estamos en retraso pago podemos volver a regular pagos (pago al día) o ir a impoago\n            next_state = np.random.choice([0, 1, 2], p=transition_matrix[current_state])\n        else:  # Si estamos en el estado de impago\n            y[i] = 1  # estado absorbente\n            break\n        current_state = next_state\n        pagos_muestra_list.append(current_state)\n        muestra_dict[f\"Individuo_{i}\"] = pagos_muestra_list\nEn el diccionario muestra_dict se ha guardado el comportamiento de cada individuo a lo largo de los 12 pagos posteriores al punto inicial.\nmuestra_dict\nComo puede verse, la mayor parte de individuos no llegan al estado de impago y esto es consecuencia de las probabilidades existentes en la matriz de transición de partida.\nLa secuencia de pagos del Individuo_5 hace que sea de interés focalizarse en él para detallar el impacto que tienen las cadenas de markov. Como puede verse, al inicio de pago se empieza a retrasar hasta volver a regularizar sus pagos a mediados del segundo trimestre. Tras esta regularización, meses después vuelve a caer de estado.\nLas cadenas de Markov absorbentes tienen algunas propiedades específicas que las diferencian de las cadenas de Markov más simples. La más destacada es la referida a la forma en que la matriz de transición puede ser escrita. Sea una cadena con t estados transitorios y r estados absorbentes, la matriz de transición T puede escribirse en su forma canónica como:\n\n\n\\[\nT = \\begin{pmatrix}\nQ & R \\\\\n0 & I_t \\\\\n\\end{pmatrix}\n\\]\n\n\nDonde Q es una matriz de txt, R es una matriz de txr, 0 es una matriz de ceros de rxt e It es la matriz identidad de txt.\nEn particular, la descomposición de la matriz de transición en la matriz fundamental permite ciertos cálculos, como el número esperado de pasos hasta la absorción de cada estado. La matriz fundamental N se calcula de la siguiente manera:\n\n\n\\[\nN= (I_t-Q)^{-1}\n\\]\n\n\nSiendo I_t es la matriz identidad de txt. Así, para obtener el número esperado de pasos se calcula como:\n\n\n\\[\nn= N*1\n\\]\n\n\nDonde 1 denota un vector columna de valor uno y longitud igual al número estados transitorios.\nPor último, la probabilidad de que un estado transitorio sea absorbido es calculada como:\n\n\n\\[\np_{trans \\rightarrow abs}= N * R\n\\]\n\n\nVeamos un ejemplo de Cadena de Markov absorbente con el que podamos ver en detalle estos cálculos matriciales:\n\nImaginemos un cliente en un casino. Por cada apuesta gana 1€ con probabilidad de 0.3 o pierde 1€ con probabilidad de 0.7. Sabiendo que la apuesta ha sido iniciada con 2 € y que el cliente se retirará se retirará si pierde todo el dinero o bien lo duplica. Se pide:\n\n\nCuestión 1: Escribir la matriz de transición de una cadena de Markov\nCuestión 2: Determinar el promedio de apuestas hasta que el juego termina\nCuestión 3: Determinar la probabilidad de terminar el juego con 4€ o de marcharse de vacío\n\n\nCuestión 1: Del enunciado se conoce que se tienen 5 posibles estados (0, 1, 2, 3, 4) siendo los estados 0 y 4 absorbentes (pierde todo o duplica la apuesta, respectivamente). Teniendo en cuenta los posibles movimientos y las probabilidades asociadas se tiene:\n\n\n\\[\nT = \\begin{pmatrix}\nt_{00} & t_{01} & t_{02} & t_{03} & t_{04} \\\\\nt_{10} & t_{11} & t_{12} & t_{13} & t_{14} \\\\\nt_{20} & t_{21} & t_{22} & t_{23} & t_{24} \\\\\nt_{30} & t_{31} & t_{32} & t_{33} & t_{34} \\\\\nt_{40} & t_{41} & t_{42} & t_{43} & t_{44} \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0.7 & 0 & 0.3 & 0 & 0 \\\\\n0 & 0.7 & 0 & 0.3 & 0 \\\\\n0 & 0 & 0.7 & 0 & 0.3 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n\\end{pmatrix}\n\\]\n\n\n\n\nCuestión 2: Se escribe la matriz T en su forma canónica. Notar que para ello es necesario reorganizar los estados (ahora, los estados absorbentes están en las últimas filas de la matriz T).\n\n\n\\[\nT =\n\\begin{pmatrix}\nQ & R \\\\\n0 & I_t \\\\\n\\end{pmatrix}\n= \\begin{pmatrix}\nt_{11} & t_{12} & t_{13} & t_{10} & t_{14} \\\\\nt_{21} & t_{22} & t_{23} & t_{20} & t_{24} \\\\\nt_{31} & t_{32} & t_{33} & t_{30} & t_{34} \\\\\nt_{01} & t_{02} & t_{03} & t_{00} & t_{04} \\\\\nt_{41} & t_{42} & t_{43} & t_{40} & t_{44} \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n0 & 0.3 & 0 & 0.7 & 0 \\\\\n0.7 & 0 & 0.3 & 0 & 0 \\\\\n0 & 0.7 & 0 & 0 & 0.3 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n\\end{pmatrix}\n\\]\n\n\n\n\nDe forma que Q y R son:\n\n\n\\[ Q = \\begin{pmatrix} 0.0 & 0.3 & 0.0 \\\\ 0.7 & 0.0 & 0.3 \\\\ 0.0 & 0.7 & 0.0 \\end{pmatrix}  \\] \\[ R = \\begin{pmatrix} 0.7 & 0.0 \\\\ 0.0 & 0.0 \\\\ 0.0 & 0.3 \\end{pmatrix} \\]\n\n\n\n\nEl número de apuestas hasta terminar el juego es:\n\n\n\n\n\\[\nN= (I_t-Q)^{-1} * 1 = {\\begin{pmatrix} 0.0 & 0.3 & 0.0 \\\\ 0.7 & 0.0 & 0.3 \\\\ 0.0 & 0.7 & 0.0 \\end{pmatrix}}^{-1} *\n\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} =\n\\begin{pmatrix} 1.362 & 0.517 & 0.155 \\\\ 1.207 & 1.724 & 0.517 \\\\ 0.845 &  1.207 & 1.362 \\end{pmatrix} *\n\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2.034 \\\\ 3.448 \\\\ 3.414 \\end{pmatrix}\n\\]\n\n\n\n\nTeniendo en cuenta que el cliente empezó su apuesta con 2€, el número de apuestas esperadas hasta que el juego acabe son 3.448€.\n\n\nCuestión 3: En este caso, se sabe que la probabilidad de llegar a un estado absorbente desde uno transitorio sigue la siguiente expresión:\n\n\n\\[\np_{trans \\rightarrow abs}= N * R = (I_t-Q)^{-1} * R =\n\\begin{pmatrix} 1.362 & 0.517 & 0.155 \\\\ 1.207 & 1.724 & 0.517 \\\\ 0.845 &  1.207 & 1.362 \\end{pmatrix} *\n\\begin{pmatrix} 0.7 & 0\\\\ 0 & 0 \\\\ 0 & 0.3 \\end{pmatrix} = \\begin{pmatrix} 0.953 & 0.046 \\\\ 0.845 & 0.155\\\\ 0.591 & 0.409 \\end{pmatrix}\n\\]\n\n\n\n\nAsí, la probabilidad de que el cliente acabe con 4€ es de 15.5%. Por su parte, se tiene un 84.5% de posibilidades de que se vaya de vacío.\n\n\n\n2.3.3 Modelos Ocultos de Markov\nLos Modelos Ocultos de Markov, HMMs (por sus siglas en inglés) son una extensión de las cadenas de Markov y sirven para tratar tanto eventos observables (presentes en la cadena de entrada) como eventos ocultos que consideramos causales del modelo probabilístico. Los Modelos Ocultos de Markov son utilizados cuando se conocen las evidencias sobre un sistema pero no los estados tienen lugar de forma que buscan establecer la relación existente entre los estados visibles y los ocultos. Algunos ejemplos de uso de este tipo de modelos:\n\nSeparación de secuencias de nucleótidos por sus características biológicas (exón-intrón)\nRelacionar proteínas con sus funcionalidades\nLocalización de genes en las células eucariotas\nReconocimiento del habla\nEtiquetado de texto y traducción automática\n\nEn un HMM, para cada instante de tiempo o posición t en una secuencia se tiene:\n\nUna variable aleatoria \\(X_t\\), con posibles estados \\(s_1, … ,s_n\\) (no observables directamente)\nOtra variable aleatoria \\(E_t\\), con posibles estados \\(v_1, … ,v_m\\) (observaciones)\n\nPara un buen funcionamiento de este tipo de modelos se asume dos propiedades:\n\nPropiedad de Markov: en cada posición, el estado solo depende del estado en la posición inmediatamente anterior: \\(P(X_t│Y, X_{t-1}) = P(X_t│X_{t-1})\\)\nIndpendencia de las observaciones: en cada posición, la observación solo depende del estado en esa posición: \\(P(E_t│Y, X_t )=P(E_t│X_t)\\)\n\nDe forma análoga a las cadenas de Markov, un HMM también puede ser expresado según una red bayesiana:\n\n\n\n\n\n\nFigure 2.6: HMM\n\n\n\nAsí, cada nodo \\(X_t\\) la misma tabla de probabilidad correspondiente a \\(P(X_t│X_{t-1})\\) salvo en el instante anterior. Por el contrario, cada nodo \\(E_t\\) tiene una única tabla de probabilidad correspondiente a \\(P(E_t│X_t)\\).\nAdemás de los estados ocultos y observables comentados anteriormente, un Modelo Oculto de Markov consta también de otros elementos que son citados a continuación:\n\nRespecto a los estados ocultos:\n\nLa matriz de probabilidades entre los estados, A, denominada matriz de transición. Así, \\(a_{ij}=P(X_t=s_j│X_{t-1}=s_i)\\) es la probabilidad de pasar del estado si al estado \\(s_j\\) Es importante destacar que el modelo probabilístico que describe la manera de transitar entre una posición y la siguiente no cambia a lo largo de la secuencia.\nEl vector de probabilidades a priori de cada estado, \\(\\pi\\), con \\(\\pi_i=P(x_1=s_i)\\) - Respecto a las observaciones: - La matriz de probabilidades de los observables, B, conocida como matriz de observación. Así, \\(b_{ij}=P(E_t=v_j│X_t=s_i)\\) es la probabilidad de observar \\(v_j\\) cuando el estado es \\(s_i\\)\n\n\nEs importante destacar que el modelo probabilístico que describe la emisión de la observación en cada estado no cambia a lo largo de la secuencia.\nPor tanto, un HMM está formado por la combinación de dos tipos de modelos: - El transicional el cual responde a los estados ocultos - El modelo de evidencias que tiene en cuenta la información disponible de las observaciones\nUn ejemplo básico sobre el uso de Modelos Ocultos de Markov en bioinformática se plantea a continuación. En este ejemplo, se parte de una secuencia de ADN ficticia (observaciones) y se hace uso de un HHM para predecir la probabilidad de los estados ocultos (“codificación de genes” y “regiones no codificantes”) en la secuencia de ADN.\nimport numpy as np\nfrom hmmlearn import hmm\n\nnp.random.seed(444)\n\ndna_sequence = \"TCGAATCGAAGTATCGGCATTGGCTCGAGCGATCGATGCTAGCA\"\nstates = [\"Gene\", \"Non-Gene\"]\n\n# Conversión de la secuencia de ADN a números para que el modelo HMM pueda procesarla\n# Por ejemplo, A=0, C=1, G=2, T=3\ndna_encoded = np.array([[0 if base == \"A\" else 1 if base == \"C\" else 2 if base == \"G\" else 3 for base in dna_sequence]]).T\n# Definir y entrenar el modelo\nmodel = hmm.CategoricalHMM(n_components=2, n_iter=100) # las componentes son los estados\nmodel.fit(dna_encoded) \nmodel.predict_proba(dna_encoded)[0:20] # probabilidades de decodificación\n# Decodificar los estados ocultos (genes vs no genes) utilizando el modelo entrenado\ndecoded_states = model.predict(dna_encoded) # predict asume un threshold de 0.5\n\n# Decodificar los estados ocultos a sus etiquetas originales\ndecoded_states_labels = [states[state] for state in decoded_states]\n\nprint(f\"Secuencia de ADN: {dna_sequence}\")\nprint(f\"Estados ocultos predichos: {decoded_states_labels}\")\nDado una secuencia de observaciones \\(o_1 o_2 … o_t\\), mediante un Modelo Oculto de Markov se pueden responder a distintos tipos de problemas como: - Filtrado: permite conocer la probabilidad de que \\(X_t=q\\) - Explicación más verosímil: también conocida como decodificación, permite conocer la secuencia de estados más probable.\nA continuación, se presenta un ejemplo para explicar en detalle el proceso de obtención del filtrado y de la explicación más verosímil en un Modelo Oculto de Markov.\nSuponga un trabajador en una plataforma de petróleo que no tiene contacto con el exterior en todo un año. Debido a su profesión, desconoce la situación meteorológica de cada día (si llueve o no), pero todas las mañanas siempre ve llegar al gerente a su oficina. El gerente unos días viene con paraguas y otros no. Imagine entonces que un sistema formado por dos estados ocultos (lluvia, no lluvia) y dos observaciones (paraguas, no paraguas) es utilizado para pronosticar el tiempo por el trabajador. La siguiente imagen muestra la estructura de un Modelo Oculto de Markov en formato de red.\n\n\n\n\n\n\nFigure 2.7: Ejemplo HHM\n\n\n\nEl ejemplo es detallado tanto siguiendo los cálculos “manualmente” como a partir de una implementación en python.\nLos vectors de información a priori como las matrices de probabilidad entre estados y las matrices de probablidad de observables se obtienen directamente del enunciado:\nimport numpy as np\n\n# Definir parámetros del modelo HMM como listas y diccionarios\n\nstates = ('lluvia', 'no_lluvia')\nobservations = ('paraguas', 'no_paraguas')\n\nstart_probability = {'lluvia': 0.5, 'no_lluvia': 0.5} # Vector de información a priori\n\n# Matrices de probabilidad entre estados \ntransition_probability = {\n    'lluvia': {'lluvia': 0.7, 'no_lluvia': 0.3},\n    'no_lluvia': {'lluvia': 0.3, 'no_lluvia': 0.7},\n}\n\n# Matriz de probabilidad de observables \nemission_probability = {\n    'lluvia': {'paraguas': 0.9, 'no_paraguas': 0.1},\n    'no_lluvia': {'paraguas': 0.2, 'no_paraguas': 0.8},\n}\n\n2.3.3.1 Filtrado\n\n2.3.3.1.1 Implementación del Algoritmo Forward\nSe define la función para calcular la probabilidad conjunta de una secuencia de observaciones y estados usando el algoritmo de avance (forward).\ndef forward(obs, states, start_p, trans_p, emit_p):\n    alpha = np.zeros((len(obs), len(states)))\n\n    # Inicializar primer paso\n    for i, state in enumerate(states):\n        alpha[0][i] = start_p[state] * emit_p[state][obs[0]]\n\n    # Recorrer el resto de la secuencia de observaciones\n    for t in range(1, len(obs)):\n        for i, current_state in enumerate(states):\n            alpha[t][i] = sum(alpha[t-1][j] * trans_p[states[j]][current_state] * emit_p[current_state][obs[t]] for j in range(len(states)))\n\n    return alpha\n# Secuencia de observaciones y estados de los tres primeros días\nobservations_sequence = ['paraguas', 'paraguas', 'no_paraguas']\n\n# Calcula la probabilidad conjunta de la secuencia de observaciones y estados usando el algoritmo de avance\nalpha = forward(observations_sequence, states, start_probability, transition_probability, emission_probability)\nalpha\n# Suma de las probabilidades en el último paso para obtener la probabilidad total de la secuencia de observaciones\nprobability_sequence = np.sum(alpha[-1])\nalpha[-1] / probability_sequence # Probabilidad normalizada en el último paso (día 3)\nAsí, la probabilidad de que el día 3 sea lluvia es del 19%\n\n\n\n2.3.3.2 Explicación más verosimil\n\n2.3.3.2.1 Implementación del algoritmo Viterbi\nFunción para calcular la secuencia de estados más probable utilizando el algoritmo Viterbi\ndef viterbi(obs, states, start_p, trans_p, emit_p):\n    V = [{}]\n    path = {}\n\n    # Inicializar primer paso\n    for state in states:\n        V[0][state] = start_p[state] * emit_p[state][obs[0]]\n        path[state] = [state]\n\n    # Recorrer el resto de la secuencia de observaciones\n    for t in range(1, len(obs)):\n        V.append({})\n        new_path = {}\n\n        for current_state in states:\n            (prob, state) = max(\n                (V[t - 1][previous_state] * trans_p[previous_state][current_state] * emit_p[current_state][obs[t]], previous_state)\n                for previous_state in states\n            )\n            V[t][current_state] = prob\n            new_path[current_state] = path[state] + [current_state]\n\n        path = new_path\n\n    # Encontrar el estado final con la mayor probabilidad\n    (prob, state) = max((V[len(obs) - 1][final_state], final_state) for final_state in states)\n\n    return (prob, path[state])\nSe aplica la función y se obtiene tanto la secuencia de estados ocultosmás probable como la probabilidad de ésta.\nprob, path = viterbi(observations_sequence, states, start_probability, transition_probability, emission_probability)\nprint(f\"Secuencia de estados ocultos más probable: {path}\")\nprint(f\"Probabilidad de la secuencia más probable: {prob}\")\n\n\n\n2.3.3.3 Aplicación de un HMM: Post-tagging\nEl post-tagging es una tarea fundamental en el procesamiento del lenguaje natural (NLP por sus siglas en inglés) que consiste en asignar etiquetas gramaticales a cada palabra en una oración después de haber sido segmentada en palabras individuales. Esta tarea es crucial para comprender el significado y la estructura de las oraciones, ya que las etiquetas gramaticales proporcionan información sobre la función sintáctica de cada palabra.\nEn el contexto del post-tagging, los estados del HMM representan las etiquetas gramaticales de las palabras, las transiciones representan la dependencia entre las etiquetas gramaticales de las palabras consecutivas y las emisiones representan la probabilidad de que una palabra dada se observe en un estado determinado.\nPara realizar el post-tagging con un HMM, se sigue el siguiente procedimiento:\n\nEntrenamiento del modelo: se entrena con un conjunto de datos de oraciones etiquetadas, aprendiendo las probabilidades de transición y emisión\nPredicción de etiquetas: para una nueva oración sin etiquetar, el modelo predice la secuencia de etiquetas gramaticales más probable para la oración, utilizando el algoritmo de Viterbi\n\nVentajas\n\nFlexibilidad: pueden modelar secuencias de palabras con diferentes patrones gramaticales\n\nInterpretabilidad: Los estados del HMM pueden interpretarse como diferentes tipos de palabras o estructuras gramaticales.\nRobustez: Los HMMs son robustos a errores de segmentación de palabras y a palabras desconocidas.\nLimitaciones\n\nDependencia de datos: el rendimiento del modelo depende de la calidad y cantidad de datos de entrenamiento disponibles\nAmbigüedad gramatical: pueden no ser capaces de resolver ambigüedades gramaticales en oraciones complejas\nNecesidad de preprocesamiento: requiere preprocesamiento previo de las oraciones, como la segmentación de palabras.\n\nimport warnings\n\nimport nltk\nimport numpy as np\nfrom hmmlearn import hmm\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom nltk.corpus import brown # corpus con etiquetado\n\n# Cargar las sentencias etiquetadas del corpus brown\ntagged_sentences = brown.tagged_sents(tagset='english')\n\n# Crear un diccionario de palabras y un diccionario de etiquetas\nword2idx = {}\ntag2idx = {}\n\n# Iterar sobre las sentencias etiquetadas para construir los diccionarios\nfor sentence in tagged_sentences:\n    for word, tag in sentence:\n        if word.lower() not in word2idx:\n            word2idx[word.lower()] = len(word2idx)\n        if tag not in tag2idx:\n            tag2idx[tag] = len(tag2idx)\n\n# Estos diccionarios serán útiles para convertir palabras y tags en índices numéricos que nuestro modelo HMM pueda entender.\n\n# Conjunto de entrenamiento\nwords_train = [] # Lista de palabras (en minúscualas por lower)  \ntags_train = [] # Lista de etiquetas\nfor sentence in tagged_sentences:\n    words, tags = zip(*sentence)\n    words_train.append([word.lower() for word in words])\n    tags_train.append(tags)\n# Creación y entrenamiento del modelo HMM\nmodel = hmm.MultinomialHMM(n_components=len(tag2idx), init_params=\"ste\") # estados ocultos como número de etiquetas\nmodel.fit(\n    X=np.array([word2idx[word] for words in words_train for word in words]).reshape(-1, 1),\n    lengths=[len(words) for words in words_train]\n) # El entrenamiento se hace converiendo a índices las palabras\n# Función para realizar post-tagging en una nueva sentencia en castellano\ndef post_tag(model, sentence, word2idx, tag2idx):\n    \n    # Convertir las palabras de la sentencia a índices\n    word_idxs = [word2idx[word.lower()] for word in sentence if word.lower() in word2idx]\n    \n    # Si no hay palabras conocidas, devolver None\n    if len(word_idxs) == 0:\n        return None\n    \n    # Realizar post-tagging utilizando el modelo HMM\n    predicted_tags = model.predict(np.array(word_idxs).reshape(-1, 1))\n    \n    # Convertir los índices de etiquetas a etiquetas POS\n    predicted_tags = [list(tag2idx.keys())[list(tag2idx.values()).index(tag)] for tag in predicted_tags]\n    \n    return list(zip(sentence, predicted_tags))\nsentence = \"I love Python\"\npredicted_tags = post_tag(model, sentence.split(), word2idx, tag2idx)\nprint(f\"Post-tagging de la oración: {predicted_tags}\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelos Gráficos Probabilísticos y Análisis Causal</span>"
    ]
  },
  {
    "objectID": "capitulo3.html",
    "href": "capitulo3.html",
    "title": "3  Algoritmos Genéticos",
    "section": "",
    "text": "3.1 Introducción\nLos algoritmos genéticos, o también llamados algoritmos evolutivos es un método bastante común en minería de datos. Se inspiran en el proceso natural de selección y evolución tal y como se describe por la teoría evolucionista de la selección natural postulada por Darwin [@darwin1859].\nLos principios sobre los que se asientan los algoritmos genéticos son:\nLos algoritmos genéticos se empezaron a estudiar sobre los años 60 a partir del trabajo de Fogel [@fogel1966] (donde los organismos eran máquinas de esados finitos), siguiendo con los trabajos de Rechenberg [@rechenberg1973] (se establecen estrategias de selección) y principalmente de Holland [@holland1975] (se estableció el nombre de Algoritmos Genéticos).\nLos algoritmos genéticos son adecuados para obtener buenas aproximaciones en problemas de búsqueda, aprendizaje y optimización [@marczyk2004].\nDe forma esquemática un algoritmo genético es una función matemática que tomando como entrada unos individuos iniciales (población origen) selecciona aquellos ejemplares (también llamados individuos o cromosomas) que recombinándose por algún método generarán como resultado la siguiente generación. Esta función se aplicará de forma iterativa hasta verificar alguna condición de parada, bien pueda ser un número máximo de iteraciones o bien la obtención de un individuo que cumpla unas restricciones iniciales.\nCondiciones para la aplicación de los Algoritmos Genéticos\nNo es posible la aplicación en toda clase de problemas de los algoritmos genéticos. Para que estos puedan aplicarse, los problemas deben cumplir las siguientes condiciones:\nHabitualmente, la segunda condición es la más complicada de conseguir, para ciertos problemas es trivial la función de fitness (por ejemplo, en el caso de la búsqueda del máximo de una función) no obstante, en la vida real a veces es muy complicada de obtener y, habitualmente, se realizan conjeturas evaluándose los algoritmos con varias funciones de fitness.\nVentajas e inconvenientes\nVentajas\nInconvenientes",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos Genéticos</span>"
    ]
  },
  {
    "objectID": "capitulo3.html#introducción",
    "href": "capitulo3.html#introducción",
    "title": "3  Algoritmos Genéticos",
    "section": "",
    "text": "Los individuos mejor adaptados al entorno son aquellos que tienen una probabilidad mayor de sobrevivir y, por ende, de reproducirse.\nLos descendientes heredan características de sus progenitores.\nDe forma esporádica y natural se producen mutaciones en el material genético de algunos individuos, provocando cambios permanentes.\n\n\n\n\n\n\n\nEl espacio de búsqueda [^Recordemos que cualquier método de Data Mining se puede asimilar como una búsqueda en el espacio solución, es decir, el espacio formado por todas las posibles soluciones de un problema] debe estar acotado, por tanto ser finito.\nEs necesario poseer una función de aptitud, que denominaremos fitness, que evalúe cada solución (individuo) indicándonos de forma cuantitativa cuán buena o mala es una solución concreta.\nLas soluciones deben ser codificables en un lenguaje comprensible para un ordenador, y si es posible de la forma más compacta y abreviada posible.\n\n\n\n\n\nNo necesitan ningún conocimiento particular del problema sobre el que trabajan, únicamente cada ejemplar debe representar una posible solución al problema.\nEs un algoritmo admisible, es decir, con un número de iteraciones suficiente son capaces de obtener la solución óptima en problemas de optimización.\nLos algoritmos genéticos son bastante robustos frente a falsas soluciones ya que al realizar una inspección del espacio solución de forma no lineal (por ejemplo, si quisiéramos obtener el máximo absoluto de una función) el algoritmo no recorre la función de forma consecutiva por lo que no se ve afectada por máximos locales.\nAltamente paralelizable, es decir, ya que el cálculo no es lineal podemos utilizar varias máquinas para ejecutar el programa y evaluar así un mayor número de casos.\nPueden ser incrustrables en muchos algoritmos de data mining para formar modelos híbridos. Por ejemplo para seleccionar el número óptimo de neuronas en un modelo de Perceptrón Multicapa.\n\n\n\nSu coste computacional puede llegar a ser muy elevado, si el espacio de trabajo es muy grande.\nEn el caso de que no se haga un correcto ajuste de los parámetros pueden llegar a caer en una situación de dominación en la que se produce un bucle infinito ya que unos individuos dominan sobre los demás impidiendo la evolución de la población y por tanto inhiben la diversidad biológica.\nPuede llegar a ser muy complicado encontrar una función de evaluación de cada uno de los individuos para seleccionar los mejores de los peores.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos Genéticos</span>"
    ]
  },
  {
    "objectID": "capitulo3.html#fundamentos-teóricos",
    "href": "capitulo3.html#fundamentos-teóricos",
    "title": "3  Algoritmos Genéticos",
    "section": "3.2 Fundamentos teóricos",
    "text": "3.2 Fundamentos teóricos\nA continuación, se explican los conceptos básicos de los algoritmos genéticos.\n\n3.2.1 Codificación de los datos\nCada individuo o cromosoma está formado por unos cuantos genes. Para nuestro caso vamos a establecer que los indiduos tienen un único cromosoma con una cierta cantidad de genes. Estos genes los consideramos como la cantidad mínima de información que se puede transferir. Los genes se pueden agrupar en características o rasgos que nos podrían ayudar en la resolución de ciertos problemas.\nEstos individuos con sus genes los tenemos que representar de forma que podamos codificar esa información.\n\n\n\n\n\n\nFigure 3.1: Representación de un cromosoma\n\n\n\nLos principales métodos de representación son:\n\nBinaria: Los individuos/cromosomas están representados por una serie de genes que son bits ( valores 0 ó 1).\nEntera: Los individuos/cromosomas están representados por una serie de genes que son números enteros.\nReal: Los individuos/cromosomas están representados por una serie de genes que son números reales en coma flotante.\nPermutacional: Los individuos/cromosomas están representados por una serie de genes que son permutaciones de un conjunto de elementos. Se usan en aquellos problemas en los que la secuencia u orden es importante.\nBasada en árboles: Los individuos/cromosomas están representados por una serie de genes que son estructuras jerárquicas.\n\n\n\n\n\n\n\nFigure 3.2: Diferentes representaciones\n\n\n\nEl primer paso para conseguir que un ordenador procese unos datos es conseguir representarlos de una forma apropiada. En primer término, para codificar los datos, es necesario separar las posibles configuraciones posibles del dominio del problema en un conjunto de estados finito.\nUna vez obtenida esta clasificación el objetivo es representar cada estado de forma unívoca con una cadena (compuesta en la mayoría de casos por unos y ceros).\nA pesar de que cada estado puede codificarse con alfabetos de diferente cardinalidad[^La longitud de las cadenas que representen los posibles estados no es necesario que sea fija, representaciones como la de Kitano para representar operaciones matemáticas son un ejemplo de esto], uno de los resultados fundamentales de la teoría de algoritmos genéticos es el Teorema del Esquema de Holland [@holland1975], que afirma que la codificación óptima es aquella en la que los algoritmos tienen un alfabeto de cardinalidad, es decir el uso del alfabeto binario.\nEl enunciado del Teorema del Esquema es el siguiente: Esquemas cortos, de bajo orden y aptitud superior al promedio reciben un incremento exponencial de representantes en generaciones subsecuentes de un Algoritmo Genético.\nUna de las ventajas de usar un alfabeto binario para la construcción de configuraciones de estados es la sencillez de los operadores utilizados para la modificación de estas. En el caso de que el alfabeto sea binario, los operadores se denominan, lógicamente, operadores binarios. Es importante destacar que variables que estén próximas en el espacio del problema deben preferiblemente estarlo en la codificación ya que la proximidad entre ellas condiciona un elemento determinante en la mutación y reproducibilidad de éstas. Es decir, dos estados que en nuestro espacio de estados del universo del problema que están consecutivos deberían estarlo en la representación de los datos, esto es útil para que cuando haya mutaciones los saltos se den entre estados consecutivos. En términos generales cumplir esta premisa mejora experimentalmente los resultados obtenidos con algoritmos genéticos.\nEn la práctica el factor que condiciona en mayor grado el fracaso o el éxito de la aplicación de algoritmos genéticos a un problema dado es una codificación acorde con los datos.\nOtra opción muy común es establecer a cada uno de los posibles casos un número natural y luego codificar ese número en binario natural, de esta forma minimizamos el problema que surge al concatenar múltiples variables independientes en el que su representación binaria diera lugar a numerosos huecos que produjeran soluciones no válidas.\n\n\n3.2.2 Algoritmo\nUn algoritmo genético implementado en pseudo código podría ser el siguiente:\n#| label: alg-genetico\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"//\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\n\\floatname{algorithm}{Algoritmo}\n\n\\begin{algorithm}\n\\caption{Quicksort}\n\\begin{algorithmic}\n\\Procedure{Quicksort}{$A, p, r$}\n  \\If{$p &lt; r$}\n    \\State $q = $ \\Call{Partition}{$A, p, r$}\n    \\State \\Call{Quicksort}{$A, p, q - 1$}\n    \\State \\Call{Quicksort}{$A, q + 1, r$}\n  \\EndIf\n\\EndProcedure\n\\Procedure{Partition}{$A, p, r$}\n \n\n      \\State $i = i + 1$\n      \\State exchange\n\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}\nUn posible diagrama de flujo que puede representar una posible implementación de algoritmos genéticos se muestra en la figura \\(\\ref{fig-esquema-genetico}\\) .\n\n\n\n\n\n\nFigure 3.3: Esquema de implementación de un algoritmo genético\n\n\n\nA continuación, en los siguientes apartados, se hará una descripción de las fases anteriormente expuestas:\nInicializar Población\nComo ya se ha explicado antes, el primer paso es inicializar la población origen. Habitualmente la inicialización se hace de forma aleatoria procurando una distribución homogénea en los casos iniciales de prueba. No obstante, si se tiene un conocimiento más profundo del problema es posible obtener mejores resultados inicializando la población de una forma apropiada a la clase de soluciones que se esperan obtener.\nEvaluar Población\nDurante cada iteración (generación) cada individuo/cromosoma se decodifica convirtiéndose en un grupo de parámetros del problema y se evalúa el problema con esos datos.\nPongamos por ejemplo que queremos evaluar el máximo de la función \\(f(x)=x²\\) en el intervalo \\([0,1]\\) y supongamos que construimos cada individuo con 6 dígitos \\((2^6=64)\\) , por lo que interpretando el número obtenido en binario natural y dividiéndolo entre 64 obtendremos el punto de la función que corresponde al individuo. Evaluando dicho punto en la función que queremos evaluar (\\(f(x)=x²\\)) obtenemos lo que en nuestro caso sería el fitness, en este caso cuanto mayor fitness tenga un individuo, mejor valorado está y más probable es que prospere su descendencia en el futuro. No en todas las implementaciones de algoritmos genéticos se realiza una fase de evaluación de la población tal y como aquí está descrita, en ciertas ocasiones se omite y no se genera ningún fitness asociado a cada estado evaluado. La fase de selección elige los individuos a reproducirse en la próxima generación, esta selección puede realizarse por muy distintos métodos.\nEn el algoritmo mostrado en pseudo código anteriormente el método de selección usado depende del fitness de cada individuo. A continuación, se describen los más comunes:\nSelección elitista: Se seleccionan los individuos con mayor fitness de cada generación. La mayoría de los algoritmos genéticos no aplican un elitismo puro, sino que en cada generación evalúan el fitness de cada uno de los individuos, en el caso de que los mejores de la anterior generación sean mejores que los de la actual éstos se copian sin recombinación a la siguiente generación.\nSelección proporcional a la aptitud: los individuos más aptos tienen más probabilidad de ser seleccionados, asignándoles una probabilidad de selección más alta. Una vez seleccionadas las probabilidades de selección a cada uno de los individuos se genera una nueva población teniendo en cuenta éstas.\nSelección por rueda de ruleta: Es un método conceptualmente similar al anterior. Se le asigna una probabilidad absoluta de aparición de cada individuo de acuerdo al fitness de forma que ocupe un tramo del intervalo total de probabilidad (de 0 a 1) de forma acorde a su fitness. Una vez completado el tramo total se generan números aleatorios de 0 a 1 de forma que se seleccionen los individuos que serán el caldo de cultivo de la siguiente generación.\nSelección por torneo: se eligen subgrupos de individuos de la población, y los miembros de cada subgrupo compiten entre ellos. Sólo se elige a un individuo de cada subgrupo para la reproducción.\nSelección por rango: a cada individuo de la población se le asigna un rango numérico basado en su fitness, y la selección se basa en este ranking, en lugar de las diferencias absolutas en el fitness. La ventaja de este método es que puede evitar que individuos muy aptos ganen dominancia al principio a expensas de los menos aptos, lo que reduciría la diversidad genética de la población y podría obstaculizar la búsqueda de una solución aceptable. Un ejemplo de esto podría ser que al intentar maximizar una función el algoritmo genético convergiera hacía un máximo local que posee un fitness mucho mejor que el de sus congéneres de población lo que haría que hubiera una dominancia clara con la consecuente desaparición de los individuos menos aptos (con peor fitness).\nSelección generacional: la descendencia de los individuos seleccionados en cada generación se convierte en la siguiente generación. No se conservan individuos entre las generaciones.\nSelección por estado estacionario: la descendencia de los individuos seleccionados en cada generación vuelve al acervo genético preexistente, reemplazando a algunos de los miembros menos aptos de la siguiente generación. Se conservan algunos individuos entre generaciones.\nBúsqueda del estado estacionario: Ordenamos todos los genes por su fitness en orden decreciente y eliminamos los últimos m genes, que se sustituyen por otros m descendientes de los demás. Este método tiende a estabilizarse y converger.\nSelección jerárquica: los individuos atraviesan múltiples rondas de selección en cada generación. Las evaluaciones de los primeros niveles son más rápidas y menos discriminatorias, mientras que los que sobreviven hasta niveles más altos son evaluados más rigurosamente. La ventaja de este método es que reduce el tiempo total de cálculo al utilizar una evaluación más rápida y menos selectiva para eliminar a la mayoría de los individuos que se muestran poco o nada prometedores, y sometiendo a una evaluación de aptitud más rigurosa y computacionalmente más costosa sólo a los que sobreviven a esta prueba inicial.\nRecombinación.\nRecombinación también llamada Cross-over o reproducción. La recombinación es el operador genético más utilizado y consiste en el intercambio de material genético entre dos individuos al azar (pueden ser incluso entre el mismo elemento). El material genético se intercambia entre bloques. Gracias a la presión selectiva[^ Presión Selectiva es la fuerza a la que se ven sometido naturalmente los genes con el paso del tiempo. Con el sucesivo paso de las generaciones los genes menos útiles estarán sometidos a una mayor presión selectiva produciéndose la paulatina desaparición de estos] irán predominando los mejores bloques génicos.\nExisten diversos tipos de cross-over:\nCross-over de 1 punto. Los cromosomas se cortan por 1 punto y se intercambian los dos bloques de genes.\nCross-over de n-puntos. Los cromosomas se cortan por n puntos y el resultado se intercambia.\nCross-over uniforme. Se genera un patrón aleatorio en binario, y en los elementos que haya un 1 se realiza intercambio genético.\nCross-over especializados. En ocasiones, el espacio de soluciones no es continuo y hay soluciones que a pesar de que sean factibles de producirse en el gen no lo son en la realidad, por lo que hay que incluir restricciones al realizar la recombinación que impidan la aparición de algunas combinaciones.\n\n\n\n\n\n\n\n\n\n\n\n(a) Cross-over 1 punto\n\n\n\n\n\n\n\n\n\n\n\n(b) Cross-over n puntos\n\n\n\n\n\n\n\n\n\n\n\n(c) Cross-over uniforme\n\n\n\n\n\n\n\nFigure 3.4: Cross-Over\n\n\n\nMutación.\nEste fenómeno, generalmente muy raro en la naturaleza, se modela de la siguiente forma: cuando se genera un hijo se examinan uno a uno los genes del mismo y se genera un coeficiente aleatorio para cada uno. En el caso de que algún coeficiente supere un cierto umbral se modifica dicho gen. Modificando el umbral podemos variar la probabilidad de la mutación. Las mutaciones son un mecanismo muy interesante por el cual es posible generar nuevos individuos con rasgos distintos a sus predecesores.\nLos tipos de mutación más conocidos son:\n\nMutación de gen: existe una única probabilidad de que se produzca una mutación de algún gen De producirse, el algoritmo toma aleatoriamente un gen, y lo invierte.\nMutación multigen: cada gen tiene una probabilidad de mutarse o no, que es calculada en cada pasada del operador de mutación multigen.\nMutación de intercambio: Se intercambia el contenido de dos genes aleatoriamente.\nMutación de barajado: existe una probabilidad de que se produzca una mutación. De producirse, toma dos genes aleatoriamente y baraja de forma aleatoria los genes, según hubiéramos escogido, comprendidos entre los dos.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Mutacion gen\n\n\n\n\n\n\n\n\n\n\n\n(b) Mutacion multigen\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Mutacion intercambio\n\n\n\n\n\n\n\n\n\n\n\n(d) Mutacion barajado\n\n\n\n\n\n\n\nFigure 3.5: Mutación\n\n\n\nEstos ejemplos de mutaciones han sido ilustradas usando una representación binaria de los datos, en caso de tener una representación entera al hacer la mutación no cambiaríamos de 0 a 1 o viceversa, sino que se elegiría al azar un entero de los posibles valores que tenemos para ese gen. En el caso de una representación real se podría pensar en al mutación de un gen como la selección de un número real entre unos valores dados mediante una distribución uniforme o incluso una distribución normal. Para profundizar en operadores sobre disintos tipos de representaciones puedes consultar [@ Eiben2015]\nCondición de finalización\nUna vez que se ha generado la nueva población se evalúa la misma y se selecciona a aquel individuo o aquellos que por su fitness se consideran los más aptos. Podemos tener definido un umbral del valor de fitness que queremos alcanzar o simplemente definir el número de iteraciones que queremos que se realicen.\n\n\n3.2.3 Otros Operadores\nLos operadores descritos anteriormente suelen ser operadores generalistas (aplicables y de hecho aplicados a todo tipo de problemas), sin embargo, para ciertos contextos suele ser más recomendable el uso de operadores específicos para realizar un recorrido por el espacio de solución más acorde a la solución buscada.\nModificadores de la longitud de los individuos. En ocasiones las soluciones no son una combinación de todas las variables de entrada, en estas ocasiones los individuos deberán tener una longitud variable[^En muchas ocasiones, se realizan estudios de minería de datos sobre todos los datos existentes, encontrándose en ellos variables espúreas, es decir, variables que no aportan nada de información para el problema evaluado]. Lógicamente, en este tipo de casos, es necesario modificar la longitud de los individuos, para ello haremos uso de los operadores añadir y quitar, que añadirán o quitarán a un individuo un trozo de su carga génica (es decir, un trozo de información).\n\n\n3.2.4 Parámetros necesarios al aplicar Algoritmos Genéticos\nCualquier algoritmo genético necesita ciertos parámetros que deben fijarse antes de cada ejecución, como:\nTamaño de la población: Determina el tamaño máximo de la población a obtener. En la práctica debe ser de un valor lo suficientemente grande para permitir diversidad de soluciones e intentar llegar a una buena solución, pero siendo un número que sea computable en un tiempo razonable.\nCondición de terminación: Es la condición de parada del algoritmo. Habitualmente es la convergencia de la solución (si es que la hay), un número prefijado de generaciones o una aproximación a la solución con un cierto margen de error.\nIndividuos que intervienen en la reproducción de cada generación: se especifica el porcentaje de individuos de la población total que formarán parte del acervo de padres de la siguiente generación. Esta proporción es denominada proporción de cruces.\nProbabilidad de ocurrencia de una mutación: En toda ejecución de un algoritmo genético hay que decidir con qué frecuencia se va a aplicar la mutación. Se debe de añadir algún parámetro adicional que indique con qué frecuencia se va a aplicar dentro de cada gen del cromosoma. La frecuencia de aplicación de cada operador estará en función del problema; teniendo en cuenta los efectos de cada operador, tendrá que aplicarse con cierta frecuencia o no. Generalmente, la mutación y otros operadores que generen diversidad se suelen aplicar con poca frecuencia; la recombinación se suele aplicar con frecuencia alta.\nCada implementación de algoritmo tendrá sus propios parámetros que permitirán personalizar la ejecución de nuestro problema concreto.\n\n\n\n\n\n\nRecordad\n\n\n\nLos algoritmos genéticos es uno de los enfoques más originales en data mining. Su sencillez, combinada con su flexibilidad les proporciona una robustez que les hace adecuados a infinidad de problemas. No obstante, su simplicidad y sobre todo independencia del problema hace que sean algoritmos poco específicos. Recorriendo este capítulo hemos visto los numerosos parámetros y métodos aplicables a los algoritmos genéticos que nos ayudan a realizar una adaptación de los algoritmos genéticos más concreta a un problema. En definitiva, la implementación de esquemas evolutivos tal y como se describen en biología podemos afirmar que funciona.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos Genéticos</span>"
    ]
  },
  {
    "objectID": "capitulo3.html#casos-de-uso",
    "href": "capitulo3.html#casos-de-uso",
    "title": "3  Algoritmos Genéticos",
    "section": "3.3 Casos de uso",
    "text": "3.3 Casos de uso\nPara los Algoritmos genéticos tenemos 3 grandes grupos de casos de uso:\n\nOptimización de Funciones\n\nPodemos buscar máximo o mínimos de funciones.\n\nOptimización Cominatorial\n\nTSP (Travel Salesman Problem) Problema del viajante\nVRP (Vechicule Routing Problem) Problema de rutas de vehículos\n\nOptimización Machine Learning\n\nHiperparámetros\nSelección de variables\nNetwork Architecture\n\n\nVamos a ver cómo se podrían abordar algunos de estos casos de uso:\n\n3.3.1 Selección de Variable\nEl objetivo del ejemplo es ver cómo podemos usar un algoritmo genético para hacer una selección de variables, quedándonos sólo con unas pocas.\nSupongamos que tenemos un dataset que es un problema de clasificación con 3 clases, cuenta con 1500 muestras y 14 variables explicativas.\nTendremos que, para el algoritmo genético, nuestro cromosoma o individuo será un vector de tamaño 14 (14 genes), que representa las 14 variables del dataset que hemos preparado.\nEn la imagen Figure 3.6 mostramosla población de 100 individuos en una iteración N.\n\n\n\n\n\n\nFigure 3.6: Población en iteración N\n\n\n\nFunción Fitness (Evaluación)\nCuando estamos trabajando con selección de variables, el objetivo es conseguir el conjunto de variables que mejor modelo construyan según nuestro dataset. En este caso, al ser un problema de clasificación, veremos cual es la combinación de variables que nos da menos errores al clasificar.\nNuestra función fitness deberá seguir estos pasos:\n- Recibe una variable que tiene el tamaño del numero de variables (el tamaño del cromosoma) que hay en el dataframe (en nuestro caso 14) de datos.\n\nLos valores son 1 si esa variable se va a usar y 0 si no se va a usar.\n\n- Se construye un modelo, en este caso usamos LDA (Análisis Discriminante Lineal) con las variables que tienen valor 1.\n- Calculamos el error que queremos minimizar (número de fallos)\n- Para este caso del LDA cogemos los valores $posterior que nos dan la probabilidad de cada clase para cada entrada de la muestra\n- Calculamos cual es el máximo y así le asignamos esa clase como su solución. También podríamos coger directamente el valor de $class con la clase dada como predicción.\n- Verificamos cuantos hemos fallado y lo dividimos por el número de muestras para ver el porcentaje de fallos\n- Devolvemos el porcentaje de fallos. El resultado de la ejecución del algoritmo evolutivo nos dará un objeto del que tendremos que obtener que variables son las que queremos usar.\nFigure 3.7\n\n\n\n\n\n\nFigure 3.7: Población Final\n\n\n\nUna vez que nuestro algoritmo pare, deberíamos tener la población que mejor se ha adaptado según el fitness que habíamos definido.\nEn nuestro caso estarán por ejemplo las 100 mejores combinaciones de variables, que dan el menor error al clasificar. De esta manera si para cada variable contamos cuantas veces ha salido en cada elemento de la población, sabremos cuantas veces se ha usado en las combinaciones de variables de esta última iteración (que es la mejor hasta ese momento). Con lo cual podremos saber cuales han sido las variables más usadas en la población final.\nEl objeto modelo_evolutivo, que nos devuelve rbga.bin, tiene una variable population de dimensiones tamaño_población x numero_variables , en nuestro caso de 100x14, que tiene la información de la población de la última iteración del algoritmo, que en un principio debería ser la mejor. Este population contiene para cada fila (elemento en la población), 0 o 1 en la posición que corresponde a cada variable.\nPara ver cuales son las variables que más se han usado tenemos que sumar por columnas y ese dato nos dará para cada columna (corresponde con una variable) la cantidad de veces que se ha usado en esta población. Una vez tenemos estos datos ya podemos quedarnos con el número de variables que deseemos cogiendo las que más alto valor tienen.\nFigure 3.8\nImagen \\(\\ref{fig-frecuencia_variables}\\)\n\n\n\n\n\n\nFigure 3.8: Frecuencia de las Variables\n\n\n\n\n\n3.3.2 Entrenamiento de Red Neuronal\nOtro de los casos de uso sobre los que se podría trabajar con un Algoritmo Genético es el entrenamiento de una Red Neuronal. La forma estandard de entrenar una Red Neuronal es lo que se denomina el Backpropagation, que mediante el uso de las estrategias del Descenso del Gradiente se consigue optimizar los parámetros de la Red Neuronal.\nCuando entrenamos una Red Neuronal, lo que conseguimos es obtener una serie de valores de los pesos de la Red Neuronal. Estos pesos combinados con las funciones de activación serán los que nos darán el resultado de salida a partir de los datos de entrada.\nSupongamos que tenemos una Red Neuronal con 500 parámetros distribuidos en las diferentes capas ocultas del mismo.\nTendremos que, para el algoritmo genético, nuestro cromosoma o individuo será un vector de tamaño 500 (500 genes), que representan los 500 valores de los pesos de la Red Neuronal. En este caso los valores del vector tendrán una representación real.\nA cada uno de estos posibles valores reales lo deberemos acotar en un rango de valores, que podría ser *(-2.0 , 2.0 ) de forma que cuando cuando se generen los valores aleatorios de una población, cada dato deberá partir de este rango.\nEn la imagen Figure 3.6 mostramos la población de 100 individuos en una iteración N.\n\n\n\n\n\n\nFigure 3.9: Población en iteración N\n\n\n\nFunción Fitness (Evaluación)\nCuando estamos trabajando con el Entrenamiento de una Red Neuronal, el objetivo es conseguir la red neuronal que mejor valor de la función de pérdida de la red neuronal tenga.\nNuestra función fitness deberá seguir estos pasos:\n- Recibe una variable que tiene el tamaño del numero de pesos en la red neuronal (el tamaño del cromosoma), en nuestro caso 500.\n\nLos valores serán un número real\n\n- Se cogen estos pesos y se asignan a la Red Neuronal.\n\nUna vez tenemos la red neuronal, pasamos nuestros datos de entrenamiento y evaluamos el modelo obteniendo el valor de la función de pérdida.\n\n- Devolvemos el valor de la función de pérdida.\nFigure 3.7\n\n\n\n\n\n\nFigure 3.10: Población Final\n\n\n\nUna vez que nuestro algoritmo pare, deberíamos tener la población que mejor se ha adaptado según el fitness que habíamos definido.\nSeleccionamos aquella que menor valor de función de pérdida nos haya dado.\n\n\n3.3.3 Arquitecura de RedNeuronal\nSiguiendo con las redes neuronales, otro de los casos de uso sobre los que se podría trabajar con un Algoritmo Genético es la Arquitectura de Red de la Red Neuronal. En el sentido de poder definir cuantos nodos por cada capa oculta podemos tener, para conseguir una buena Red Neuronal. En ese caso estamos pensando en una red neuronal usada para datasets clásicos de datos y no de imágenes.\nCuando entrenamos una Red Neuronal, lo que conseguimos es obtener una serie de valores de los pesos de la Red Neuronal. Estos pesos combinados con las funciones de activación serán los que nos darán el resultado de salida a partir de los datos de entrada.\nSupongamos que tenemos una Red Neuronal con 500 parámetros distribuidos en las diferentes capas ocultas del mismo.\nTendremos que, para el algoritmo genético, nuestro cromosoma o individuo será un vector de tamaño 500 (500 genes), que representan los 500 valores de los pesos de la Red Neuronal. En este caso los valores del vector tendrán una representación real.\nA cada uno de estos posibles valores reales lo deberemos acotar en un rango de valores, que podría ser *(-2.0 , 2.0 ) de forma que cuando cuando se generen los valores aleatorios de una población, cada dato deberá partir de este rango.\nEn la imagen Figure 3.6 mostramos la población de 100 individuos en una iteración N.\n\n\n\n\n\n\nFigure 3.11: Población en iteración N\n\n\n\nFunción Fitness (Evaluación)\nCuando estamos trabajando con el Entrenamiento de una Red Neuronal, el objetivo es conseguir la red neuronal que mejor valor de la función de pérdida de la red neuronal tenga.\nNuestra función fitness deberá seguir estos pasos:\n- Recibe una variable que tiene el tamaño del numero de pesos en la red neuronal (el tamaño del cromosoma), en nuestro caso 500.\n\nLos valores serán un número real\n\n- Se cogen estos pesos y se asignan a la Red Neuronal.\n\nUna vez tenemos la red neuronal, pasamos nuestros datos de entrenamiento y evaluamos el modelo obteniendo el valor de la función de pérdida.\n\n- Devolvemos el valor de la función de pérdida.\nFigure 3.7\n\n\n\n\n\n\nFigure 3.12: Población Final\n\n\n\nUna vez que nuestro algoritmo pare, deberíamos tener la población que mejor se ha adaptado según el fitness que habíamos definido.\nSeleccionamos aquella que menor valor de función de pérdida nos haya dado.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos Genéticos</span>"
    ]
  },
  {
    "objectID": "capitulo3.html#algoritmos-genéticos-con-r",
    "href": "capitulo3.html#algoritmos-genéticos-con-r",
    "title": "3  Algoritmos Genéticos",
    "section": "3.4 Algoritmos genéticos con R",
    "text": "3.4 Algoritmos genéticos con R\n\n3.4.1 Paquetes para usar en R\n\n\n3.4.2 Selección de variables\n\n\n3.4.3 Entrenamiento de Red Neuronal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos Genéticos</span>"
    ]
  },
  {
    "objectID": "capitulo3.html#algoritmos-genéticos-en-python",
    "href": "capitulo3.html#algoritmos-genéticos-en-python",
    "title": "3  Algoritmos Genéticos",
    "section": "3.5 Algoritmos genéticos en Python",
    "text": "3.5 Algoritmos genéticos en Python\n\n3.5.1 Paquetes para usar en Python\n\n\n3.5.2 Optimiación de funciones\n\n\n3.5.3 Entrenamiento de Red Neuronal\nimport os import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns\n\nfrom genetic_selection import GeneticSelectionCV\n\nfrom sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix\nCargar datos de trabajo\n\nos.chdir('C:/Users/p_san/Desktop/Máster_2020/Módulo_5') #directorio datos=pd.read_csv('german_credit.csv',encoding = 'ISO-8859-1', index_col=None)\nTodas las variables son categóricas salvo:\nduration\ncredit_amount\nresidence_since\nage\nexisting_credits\nnum_dependents\nConversión a variables categóricas\ndatos\\['checking_status'\\]=datos\\['checking_status'\\].astype('category') datos\\['credit_history'\\]=datos\\['credit_history'\\].astype('category') datos\\['purpose'\\]=datos\\['purpose'\\].astype('category') datos\\['savings_status'\\]=datos\\['savings_status'\\].astype('category') datos\\['employment'\\]=datos\\['employment'\\].astype('category') datos\\['personal_status'\\]=datos\\['personal_status'\\].astype('category') datos\\['other_parties'\\]=datos\\['other_parties'\\].astype('category') datos\\['property_magnitude'\\]=datos\\['property_magnitude'\\].astype('category') datos\\['other_payment_plans'\\]=datos\\['other_payment_plans'\\].astype('category') datos\\['housing'\\]=datos\\['housing'\\].astype('category') datos\\['job'\\]=datos\\['job'\\].astype('category') datos\\['property_magnitude'\\]=datos\\['property_magnitude'\\].astype('category') datos\\['own_telephone'\\]=datos\\['own_telephone'\\].astype('category') datos\\['foreign_worker'\\]=datos\\['foreign_worker'\\].astype('category') datos\\['class'\\]=datos\\['class'\\].astype('category')\nLa variable class es una variable reservada en diferentes módulos de Python -&gt; reemplazar por por target\ndatos.rename(columns={'class': 'target'}, inplace=True) datos\\['target'\\]=np.where(datos\\['target'\\]=='good', 0, 1) \\# cambio en la codificación por sencillez en el preprocesado\nDefinición de la muestra de trabajo\ndatos_entrada=datos.drop('target', axis=1) \\# Datos de entrada datos_entrada= pd.get_dummies(datos_entrada, drop_first=True) #conversión a variables dummy\ndatos de salida\nrespuesta=datos.loc\\[:, 'target'\\]\nEscalado de las variables, partición de la muestra y Cross Validation\nseed=123 \\# Escalado de los datos de entrada x_esc=StandardScaler().fit_transform(datos_entrada) x_esc=pd.DataFrame(x_esc, columns=datos_entrada.columns)\nPartición de la muestra\ntest_size=0.3 #muestra para el test x_train, x_test, y_train, y_test = train_test_split(x_esc,respuesta, test_size=test_size, random_state=seed, stratify=respuesta) Usando un modelo Cart from sklearn.tree import DecisionTreeClassifier cart=DecisionTreeClassifier(max_depth=5, random_state=seed) cart_algoritmo_gen=GeneticSelectionCV(cart, cv=5, # 5 particiones verbose=0, # no se muestran los resultados en la pantalla scoring=“roc_auc”, # ejemplo métrica para evaluar max_features=15, # número de variables máximas en la selección de\n# características n_population=50, # tamaño de la población crossover_proba=0.5, # probabilidad de cruce entre parejas de genes mutation_proba=0.2, #probabilidad de mutación n_generations=40, #número de generaciones crossover_independent_proba=0.5, # prob. cruce para genes # independientes mutation_independent_proba=0.05, # prob. mutación de genes # independientes tournament_size=3, #tamaño de los grupos n_gen_no_change=10, # genes que se mantienen -&gt; no pasan a # la segunda generación caching=True, n_jobs=1)\ncart_algoritmo_gen=cart_algoritmo_gen.fit(x_train, y_train)\najuste del modelo usando algoritmo genéticos para la selección de variables # Variables Seleccionadas print(‘Num. Var:’, cart_algoritmo_gen.n_features_) # print(cart_algoritmo_gen.support_)\n# Resultados matriz numpy -&gt; mala visualización. Los resultados se # convierten a df de pandas df=pd.DataFrame(cart_algoritmo_gen.support_, columns=[‘Variables’], index=x_train.columns) df=df.loc[~df[‘Variables’].isin([False])] #se elimina del df las variables no seleccionadas por el algoritmo list(df.index) # variables seleccionadas por el algoritmo\nNum. Var: 9 [‘checking_status_0&lt;=X&lt;200’, ‘checking_status_&lt;0’, “credit_history_‘critical/other existing credit’”, “purpose_‘used car’”, ‘savings_status_&gt;=1000’, “personal_status_‘male single’”, ‘other_parties_guarantor’, ‘other_payment_plans_stores’, ‘job_skilled’]\nResultados test - predicción & Matriz de confusión (modelo CART con selección de variables a través de Algoritmos Genéticos)\npred=cart_algoritmo_gen.predict(x_test)\nconfusion_matrix(y_test, pred) # Matriz de confusión\narray([[173, 37], [ 46, 44]], dtype=int64)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos Genéticos</span>"
    ]
  },
  {
    "objectID": "capitulo4.html",
    "href": "capitulo4.html",
    "title": "4  Lógica Difusa",
    "section": "",
    "text": "4.1 Contexto y conceptos clave\nLa lógica difusa tiene un origen que se remonta a la antigua Grecia con Aristóteles, quien ya proponía la idea de grados de verdad o falsedad. Sin embargo, su desarrollo significativo ocurrió en el siglo XVIII, con filósofos como David Hume y Charles Sander Pierce, quienes exploraron conceptos como el razonamiento basado en la experiencia y la idea de vaguedad en lugar de la dicotomía cierto-falso.\nEl verdadero punto de inflexión llegó en 1962 con Lotfi Zadeh, quien cuestionó la rigidez de las matemáticas tradicionales frente a la imprecisión y las verdades parciales. Su trabajo en la Universidad de California en Berkeley condujo al concepto de conjuntos difusos, que generalizan los conjuntos tradicionales para trabajar con expresiones imprecisas, una noción inspirada en una discusión sobre la belleza de sus respectivas esposas.\nDesde entonces, la lógica difusa ha encontrado aplicaciones prácticas en campos como el control de sistemas, la ingeniería y la inteligencia artificial. Ebrahim Mandani y Lauritz Peter Holmbland desarrollaron sistemas de control difuso prácticos en los años 70, mientras que en Japón se produjo un rápido avance en el uso de la lógica difusa en una amplia gama de aplicaciones, desde sistemas de transporte hasta electrodomésticos inteligentes.\nLa importancia de la lógica difusa radica en su capacidad para modelar la incertidumbre y la imprecisión en la toma de decisiones, permitiendo sistemas más adaptables y eficientes en numerosas aplicaciones. Se fundamenta en los conjuntos difusos y un sistema de inferencia basado en reglas, lo que ofrece una manera elegante de abordar problemas con información vaga o incompleta. En contraste con la lógica tradicional, que emplea conceptos absolutos, la lógica difusa permite grados variables de pertenencia a los conjuntos, imitando así el razonamiento humano.\nSe definen a continuación los principales conceptos:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lógica Difusa</span>"
    ]
  },
  {
    "objectID": "capitulo4.html#contexto-y-conceptos-clave",
    "href": "capitulo4.html#contexto-y-conceptos-clave",
    "title": "4  Lógica Difusa",
    "section": "",
    "text": "Lógica Difusa: es un sistema matemático que modela funciones no lineales, que convierte unas entradas en salidas acorde con los planteamientos lógicos que usan el razonamiento aproximado.\nLógica Difusa en Inteligencia Artificial: método de razonamiento de máquina similar al pensamiento humano, que puede procesar información incompleta o incierta, característico de muchos sistemas expertos. Con la lógica difusa o borrosa se puede gobernar un sistema por medio de reglas de “sentido común” las cuales se refieren a cantidades indefinidas. En general, la lógica difusa se puede aplicar tanto a sistemas de control como para modelar cualquier sistema continuo de ingeniería, física, biología o economía.\nConjuntos difusos: son conjuntos que permiten la representación de la imprecisión y la incertidumbre. A diferencia de los conjuntos tradicionales, donde un elemento pertenece o no pertenece al conjunto de manera precisa, en los conjuntos difusos un elemento puede pertenecer al conjunto en diferentes grados. Por ejemplo, en lugar de definir un conjunto “alto” como una altura mayor a 180 cm, se podría definir de manera difusa, permitiendo grados de “altitud” para alturas que no son completamente altas o completamente bajas.\nSistema de Inferencia Difuso: es el componente de la lógica difusa que utiliza reglas lingüísticas para mapear las entradas difusas a salidas difusas. Estas reglas se expresan en la forma “SI…ENTONCES…”, donde se especifica cómo se relacionan las variables de entrada con las de salida. Por ejemplo, una regla en un sistema de control de climatización podría ser: “SI la temperatura es FRÍA y la humedad es ALTA, ENTONCES aumentar la calefacción”.\nFuzzificación: es el proceso de convertir entradas nítidas o precisas en valores difusos. Por ejemplo, en un sistema de control de velocidad de un automóvil, la entrada “velocidad del vehículo” puede ser fuzzificada para representar grados de “lento”, “medio” o “rápido”, en lugar de valores exactos de velocidad.\nDefuzzificación: es el proceso inverso de fuzzificación, donde se convierten las salidas difusas en valores nítidos o precisos. Después de que el sistema de inferencia difuso ha producido una salida difusa basada en las reglas y las entradas, la defuzzificación asigna un valor preciso a esa salida difusa.\nFunciones de Pertenencia: son funciones matemáticas que describen la membresía de un elemento a un conjunto difuso. Estas funciones determinan cómo se distribuyen los grados de pertenencia dentro del conjunto difuso. Algunas funciones comunes son las funciones triangulares, trapezoidales y gaussianas, que modelan diferentes formas de incertidumbre.\nVariable lingüística: es una variable cuyos valores se expresan en términos lingüísticos o en palabras en lugar de valores numéricos precisos. Estos términos lingüísticos se utilizan para describir la incertidumbre o la imprecisión asociada con la variable y pueden incluir etiquetas como “bajo”, “medio” y “alto” en lugar de valores numéricos específicos. Al utilizar términos lingüísticos en lugar de valores numéricos precisos, los sistemas difusos pueden manejar de manera más efectiva la información subjetiva y no lineal existente en el mundo real.\n\n\n4.1.1 Operaciones sobre conjuntos difusos\nLas operaciones sobre conjuntos difusos son operaciones matemáticas que se realizan sobre conjuntos difusos para obtener nuevos conjuntos difusos. Las operaciones sobre conjuntos difusos se pueden clasificar en dos categorías principales:\n\nOperaciones Unarias: se aplican a un solo conjunto difuso y generan un nuevo conjunto difuso como resultado. Algunas de las operaciones unarias más comunes son:\n\nNegación: la negación de un conjunto difuso A, denotada como \\(¬A\\), se obtiene complementando la función de pertenencia de A. En otras palabras, para todo elemento x en el universo de discurso, \\(\\mu ¬A(x) = 1 - \\mu A(x)\\).\nInversión: la inversión de un conjunto difuso A se obtiene invirtiendo la función de pertenencia de A. En otras palabras, para todo elemento x en el universo de discurso, \\(\\mu A^{x} = [1 - \\mu A(x)]^n\\), donde n es un parámetro que controla la forma de la inversión.\nConcentración: la concentración de un conjunto difuso A, denotada como C(A), se obtiene estrechando la función de pertenencia de A alrededor de sus valores máximos. En otras palabras, para todo elemento x en el universo de discurso, \\(\\mu C(A)(x) = f(\\mu A(x))\\), donde f es una función creciente que acerca los valores de \\(\\mu A(x)\\) a 1.\nDilatación: La dilatación de un conjunto difuso A, denotada como D(A), se obtiene ampliando la función de pertenencia de A. En otras palabras, para todo elemento x en el universo de discurso, \\(\\mu D(A)(x) = g(\\mu A(x))\\), donde g es una función decreciente que aleja los valores de \\(\\mu A(x)\\) de 1.\n\nOperaciones Binarias: se aplican a dos conjuntos difusos y generan un nuevo conjunto difuso como resultado. Algunas de las operaciones binarias más comunes son:\n\nIntersección: la intersección de dos conjuntos difusos A y B, denotada como \\(A ∩ B\\), se obtiene como el conjunto difuso que contiene solo los elementos que pertenecen tanto a A como a B. En otras palabras, para todo elemento x en el universo de discurso, \\(\\mu A∩B(x) = min(\\mu A(x), µB(x))\\).\nUnión: La unión de dos conjuntos difusos A y B, denotada como \\(A ∪ B\\), se obtiene como el conjunto difuso que contiene todos los elementos que pertenecen a A, a B o a ambos. En otras palabras, para todo elemento x en el universo de discurso, \\(\\mu A∪B(x) = max(\\mu A(x), \\mu B(x))\\).\nComplemento: ll complemento de un conjunto difuso A, denotado como \\(A^C\\), se obtiene como el conjunto difuso que contiene todos los elementos que no pertenecen a A. En otras palabras, para todo elemento x en el universo de discurso, \\(\\mu A^C(x) = 1 - \\mu A(x)\\).\nComposición: la composición de dos conjuntos difusos A y B, denotada como A o B, se obtiene como el conjunto difuso que representa la “relación” entre A y B. La definición precisa de la composición depende del tipo de conjuntos difusos y la aplicación específica.\n\n\nPropiedades de las Operaciones sobre Conjuntos Difusos\nLas operaciones sobre conjuntos difusos generalmente satisfacen ciertas propiedades matemáticas que garantizan su consistencia y aplicabilidad. Algunas de las propiedades comunes son:\n\nAsociatividad: la asociación de las operaciones define el orden en que se realizan las operaciones. Por ejemplo, para la intersección: \\((A ∩ B) ∩ C = A ∩ (B ∩ C)\\)\nConmutatividad: el orden de los operandos no afecta el resultado de la operación. Por ejemplo, para la unión: \\(A ∪ B = B ∪ A\\)\nDistributividad: la distribución de una operación sobre otra define cómo se combinan las operaciones. Por ejemplo, para la distribución de la unión sobre la intersección: \\(A ∪ (B ∩ C) = (A ∪ B) ∩ (A ∪ C)\\)\nAbsorción: la absorción de una operación por otra define cómo un elemento neutro afecta el resultado\n\n\n4.1.1.1 T-normas y T-conormas\nLas T-normas y T-conormas son operadores fundamentales que permiten combinar información difusa y obtener un resultado difuso. Estas operaciones juegan un papel esencial en la intersección y unión de conjuntos difusos.\n\nT-normas: es una función binaria que combina dos valores difusos en un nuevo valor difuso. Se representa como T(a, b), donde a y b son valores difusos entre 0 y 1. Las T-normas se caracterizan por las siguientes propiedades:\n\nMonotonicidad: T(a1, b1) ≤ T(a2, b2) si a1 ≤ a2 y b1 ≤ b2.\nConmutatividad: T(a, b) = T(b, a).\nAsociatividad: T(T(a1, a2), b) = T(a1, T(a2, b)).\nNeutralidad: T(a, 1) = a para todo a entre 0 y 1.\n\n\nLas T-normas más destacadas son: - Producto T-norma: \\(T(a, b) = a * b\\). Es la T-norma más común y representa la “intersección” difusa - Mínima T-norma: \\(T(a, b) = min(a, b)\\). Representa la “mínima” entre dos valores difusos - Diferencia limitada (o de Lukasiewick): \\(T(a, b) = max(0, a + b - 1)\\).\n\nT-conormas: es una función binaria que combina dos valores difusos en un nuevo valor difuso. Se representa como C(a, b), donde a y b son valores difusos entre 0 y 1. Las T-conormas se caracterizan por las siguientes propiedades:\n\nMonotonicidad: C(a1, b1) ≥ C(a2, b2) si a1 ≤ a2 y b1 ≤ b2.\nConmutatividad: C(a, b) = C(b, a).\nAsociatividad: C(C(a1, a2), b) = C(a1, C(a2, b)).\nNeutralidad: C(a, 0) = a para todo a entre 0 y 1.\n\n\nLas T-conormas más destacadas son: - Producto T-conorma: \\(C(a, b) = a + b - a * b\\). Es la T-conorma más común y representa la “unión” difusa - Máxima T-norma: \\(T(a, b) = max(a, b)\\). Representa la “máxima” entre dos valores difusos - Suma limitada (o de Lukasiewick): \\(T(a, b) = min(a + b, 1)\\).\nA continuación diferentes ejemplos para facilitar la interpretación de estos conceptos:\nProducto T-norma (a = 0.7, b = 0.5)\n\nConsidera un sistema que evalúa la idoneidad de un candidato para un trabajo basado en dos criterios: habilidades técnicas y habilidades de comunicación. Cada criterio se evalúa en una escala de 0 a 1, donde 0 representa ninguna habilidad y 1 representa habilidades excepcionales. Se tienen así lo siguientes valores para cada criterio:\n\n\n\na = 0.7 representa la calificación de habilidades técnicas del candidato (70% sobre 100%)\nb = 0.5 representa la calificación de habilidades de comunicación del candidato (50% sobre 100%)\n\n\n\nInterpretación: aplicando la Producto T-norma (T(0.7, 0.5) = 0.35) indica que la idoneidad general del candidato, considerando tanto habilidades técnicas como de comunicación, es del 35%. Esto sugiere que la idoneidad general del candidato es menor que sus calificaciones individuales de habilidades, reflejando la naturaleza “intersectada” de la T-norma.\n\nMínimo T-norma (a = 0.8, b = 0.3)\n\nImagina un sistema que evalúa el riesgo de una inversión financiera basado en dos factores: volatilidad del mercado y estabilidad económica. Cada factor se evalúa en una escala de 0 a 1, donde 0 representa ningún riesgo y 1 representa alto riesgo. Se tienen así lo siguientes valores para cada criterio:\n\n\n\na = 0.8 representa la calificación de volatilidad del mercado (80% sobre 100%)\nb = 0.3 representa la calificación de estabilidad económica (30% sobre 100%)\n\n\n\nInterpretación: aplicando la Mínimo T-norma (T(0.8, 0.3) = 0.3) indica que el riesgo general de la inversión, considerando tanto factores de mercado como económicos, es del 30%. Esto sugiere que el riesgo general está determinado por el factor de calificación más baja (estabilidad económica), reflejando la naturaleza “mínima” de la T-norma.\n\nProducto T-Conorma (a = 0.4, b = 0.7)\n\nConsidera un sistema que evalúa el éxito potencial de una campaña de marketing basado en dos factores: conocimiento de marca y atractivo del producto. Cada factor se evalúa en una escala de 0 a 1, donde 0 representa ningún potencial y 1 representa alto potencial. Se tienen así lo siguientes valores para cada criterio:\n\n\n\na = 0.4 representa la calificación de conocimiento de marca (40% sobre 100%)\nb = 0.7 representa la calificación de atractivo del producto (70% sobre 100%)\n\n\n\nInterpretación: aplicando el Producto T-conorma (C(0.4, 0.7) = 0.82) indica que el éxito potencial general de la campaña, considerando tanto factores de marca como de producto, es del 82%. Esto sugiere que el potencial general es mayor que las calificaciones individuales, reflejando la naturaleza de “unión” de la T-conorma\n\nMáximo T-conorma (a = 0.6, b = 0.8)\n\nImagina un sistema que evalúa la efectividad de un tratamiento médico experimental basado en dos criterios: reducción del dolor y mejora de la movilidad. Cada criterio se evalúa en una escala de 0 a 1, donde 0 representa ninguna mejora y 1 representa una mejora significativa. Se tienen así lo siguientes valores para cada criterio:\n\n\n\na = 0.6 representa la reducción del dolor obtenida con el tratamiento (60% de mejora)\nb = 0.8 representa la mejora de la movilidad lograda con el tratamiento (80% de mejora)\n\n\n\nInterpretación: aplicando el máximo T-conorma (C(0.6, 0.8) = 0.8) indica que la efectividad general del tratamiento, considerando tanto la reducción del dolor como la mejora de la movilidad, es del 80%. Esto sugiere que la efectividad general está determinada por el factor de calificación más alto (mejora de la movilidad), reflejando la naturaleza de “máximo” de la T-conorma.\n\n\n\n\n4.1.2 Funciones de membresía y modelamiento difuso\n\n4.1.2.1 Funciones de membresía\nLa función de membresía representa la asociación entre un elemento y un conjunto difuso, asignando un grado de pertenencia a dicho elemento en relación con el conjunto difuso. En otras palabras, una función de membresía describe cómo un elemento pertenece a un conjunto difuso en un continuo que va desde 0 (no pertenencia) hasta 1 (pertenencia total).\nImaginemos que medimos la temperatura de una vivienda y queremos analizar el nivel de frío. De forma difusa se podría definir una función de membresía como la presentada en la imagen:\n\n\n\n\n\n\nFigure 4.1: Ejemplo Funciones Membresia\n\n\n\nExisten diferentes funciones de membresía, las más destacadas son:\n\nTriangular: esta función de membresía se caracteriza por tener una forma triangular. Tiene tres parámetros principales: a, b y c, que definen los límites del triángulo en el eje x donde a es el punto de inicio del triángulo, b es el punto medio y c es el punto final. La función aumenta linealmente desde 0 hasta 1, alcanzando su máximo en 𝑏 y luego disminuye linealmente hasta 0. Es ampliamente utilizada debido a su simplicidad y facilidad de interpretación.\nTrapezoidal: la función de membresía trapezoidal tiene cuatro parámetros principales: a, b, c y d, que definen los límites del trapecio en el eje x. La función aumenta linealmente desde 0 hasta 1 entre a y b, permanece constante en 1 entre b y c, y luego disminuye linealmente hasta 0 entre c y d. Esta función es útil cuando se necesita modelar conjuntos con rangos más amplios o cuando se requiere un grado de pertenencia constante en un intervalo.\nGaussiana: la función de membresía gaussiana tiene una forma de campana y se utiliza comúnmente para representar variables que tienen una distribución normal. Esta función tiene dos parámetros principales: \\(\\mu\\), que representa la media de la distribución y \\(\\sigma\\) que controla su anchura de la distribución. A medida que x se aleja de \\(\\mu\\) el grado de pertenencia disminuye gradualmente. La función gaussiana es útil para modelar conjuntos con distribuciones simétricas y para suavizar transiciones entre conjuntos difusos.\nSigmoidal: se utiliza para representar relaciones no lineales entre variables. Esta función tiene tres parámetros principales: a, b y c, que controlan la pendiente y la posición de la curva sigmoidal. A medida que x aumenta desde a hasta b el grado de pertenencia aumenta gradualmente. Luego, a medida que x aumenta desde b hasta c el grado de pertenencia disminuye gradualmente. La función sigmoidal es útil para modelar relaciones complejas entre conjuntos difusos y es especialmente importante en problemas de inferencia difusa y control difuso.\n\n\n\n\n\n\n\nFigure 4.2: Funciones Membresia\n\n\n\n\n\n4.1.2.2 Modelado difuso\nEl modelado difuso implica la creación de un sistema que puede interpretar y procesar información imprecisa o incierta mediante reglas lingüísticas. En este contexto, las reglas se componen de antecedentes y consecuentes.\n\nAntecedentes: son las condiciones o variables de entrada que se evalúan para determinar qué acciones o decisiones tomar. Estos antecedentes se expresan en términos lingüísticos, como “temperatura fría” o “velocidad rápida”.\n\nEl antecedente se expresa mediante una función de pertenencia difusa \\(\\mu_A(x)\\), donde A es el nombre del antecedente y x es el valor de la variable de entrada. Así, si A es la temperatura y x es el valor de la temperatura en grados Celsius entonces \\(\\mu_A(x)\\) podría representar el grado de pertenencia de x a la categoría “frío”, “templado” o “caliente”.\n\nConsecuentes: son las acciones o decisiones que se toman en respuesta a las condiciones evaluadas en los antecedentes. Estos consecuentes también se expresan en términos lingüísticos y representan las salidas del sistema difuso. En el ejemplo del sistema de control de clima, los consecuentes podrían ser ajustes en el sistema de calefacción, ventilación o aire acondicionado.\n\nExisten diferentes modelamientos de consecuentes, los más destacados son:\n\nMamdani: los consecuentes de las reglas difusas se expresan como funciones de pertenencia difusas. Estas funciones representan la contribución de cada regla al resultado final del sistema difuso. Por lo general, estas funciones son conjuntos difusos que se solapan entre sí, lo que significa que una regla puede contribuir parcialmente al resultado final del sistema. Por ejemplo, si tenemos un conjunto de reglas difusas del tipo SI temperatura es fría Y humedad es alta ENTONCES activar calefacción, el consecuente para esta regla podría expresarse como una función de pertenencia difusa \\(\\mu_C(y)\\), donde y es el valor de la salida. Así, esta función de pertenencia podría representar la intensidad de la calefación.\nSugeno: los consecuentes de las reglas difusas se expresan como funciones lineales de la forma \\(y = ax + b\\), donde x es el valor de la entrada e y es el valor de la salida. En lugar de producir conjuntos difusos como en el enfoque de Mamdani, las reglas Sugeno generan un valor numérico preciso como resultado; lo que hace que el enfoque de Sugeno sea más adecuado para aplicaciones donde se requiere una salida numérica clara y no una respuesta lingüística. Así, continuando con el ejemplo anterior, SI temperatura es fría Y humedad es alta ENTONCES activar calefacción con intensidad 0.7, el consecuente para esta regla podría expresarse como \\(y=0.7x\\).\nTsukamoto: los consecuentes se expresan como funciones de pertenencia escalonadas. Estas funciones asignan un grado de pertenencia a diferentes conjuntos difusos de salida en función de la evaluación de los antecedentes. El enfoque de Tsukamoto es útil cuando se necesita un sistema de control que pueda adaptarse a cambios en las condiciones de entrada de manera suave y gradual.\n\n\n\\[ y = \\frac{{\\sum_{i=1}^{n} \\mu_{A_i}(x_i) \\cdot \\text{V}_i}}{{\\sum_{i=1}^{n} \\mu_{A_i}(x_i)}} \\]\n\ndonde \\(y\\) es el valor de salida, \\(\\mu_A(x)\\) es la función de pertenencia difusa del antecedente \\(A_i\\) evaluada en el valor de entrada, \\(V_i\\) es el valor asociado al consecuente para el antecedente \\(A_i\\) y n es el número de antecedentes en la regla\nEsta fórmula calcula el valor de salida 𝑦 como una combinación ponderada de los valores asociados \\(V_i\\) para cada antecedente, donde los pesos están determinados por las funciones de pertenencia de los antecedentes. La suma de las funciones de pertenencia de los antecedentes normaliza el resultado, asegurando que esté en el rango adecuado.\n\nEstas fórmulas proporcionan una manera matemática de expresar los antecedentes y consecuentes en un sistema de lógica difusa, lo que permite modelar y controlar sistemas basados en reglas lingüísticas y condiciones difusas.\n\nLa salida de un sistema difuso es, por tanto, un conjunto difuso. En cambio, los sistemas de control se relacionan con el mundo externo a través de valores exactos lo que implica, a la hora de hacer inferencia, es necesario realizar un proceso de defuzzificación. Así, los principales métodos de defuzzificación son:\n\nCentroide: calcula el centro de gravedad del conjunto difuso ponderado. Consiste en encontrar el punto en el eje de salida donde el área bajo la curva del conjunto difuso es dividida en dos áreas iguales. Matemáticamente, el valor de salida y se calcula como\n\n\n\\[ y = \\frac{{\\sum_{i=1}^{n} u_i \\cdot x_i}}{{\\sum_{i=1}^{n} u_i}} \\]\n\n\nMáximo Valor de Membresía: selecciona el valor de salida con el grado de membresía más alto. Es decir, el valor de salida es el punto en el conjunto difuso donde la membresía es máxima. Así, \\(y = max(u(x)\\)\nMedia de los Máximos: se calcula la media de todos los puntos en el conjunto difuso donde el grado de membresía es máximo. Es útil cuando hay múltiples picos en el conjunto difuso\nPrimera Máxima y Última Máxima: se selecciona el primer y último punto de máximo grado de membresía, respectivamente, como valores de salida\nBisectriz: este método divide el conjunto difuso en dos áreas de igual tamaño y toma el punto medio entre los dos puntos de intersección de la bisectriz con el conjunto difuso",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lógica Difusa</span>"
    ]
  },
  {
    "objectID": "capitulo4.html#ejemplos-prácticos",
    "href": "capitulo4.html#ejemplos-prácticos",
    "title": "4  Lógica Difusa",
    "section": "4.2 Ejemplos prácticos",
    "text": "4.2 Ejemplos prácticos\nPara ilustrar un ejemplo de control de sistemas difuso, consideremos un sistema difuso simple para controlar la potencia de un calefactor basado en la temperatura ambiente. Supongamos que queremos mantener la temperatura en una habitación alrededor de un valor de referencia.\nDefinición de las Variables Difusas - Temperatura: esta variable representa la temperatura medida en grados Celsius. Creamos una variable de entrada temperatura que abarca valores desde 0 hasta 100 grados Celsius. - Potencia: esta variable representa el nivel de potencia de la calefacción. Creamos una variable de salida potencia que también abarca valores desde 0 hasta 100.\nFunciones de Membresía - Temperatura: se definen tres funciones de membresía para la temperatura: “fría”, “templada” y “caliente”. - Potencia: de igual forma, se definen tres funciones de membresía para la potencia de la calefacción: “baja”, “media” y “alta”.\nReglas difusas Se define el sistema de reglas difusas: - Si la temperatura es fría, entonces la potencia debe ser alta - Si la temperatura es alta, entonces la potencia debe ser baja - Si la temperatura es templada, entonces la potencia debe ser media\nSistema de Control Se crea un conjunto de reglas difusas a partir de cada una de las reglas definidas previsamente. Este conjunto de reglas representa el conocimiento difuso que guiará el comportamiento del sistema de control.\nControlador Difuso Se crea un controlador difuso que toma como argumento el sistema de control difuso creado anteriormente. Este controlador simula el comportamiento del sistema de control difuso y nos permite introducir valores de entrada (temperatura) para obtener el valor de salida (potencia) correspondiente.\nSimulación Se simula un cambio en la temperatura estableciendo un valor para la variable de entrada temperatura en el controlador difuso. Luego, se calcula el valor de salida (nivel de potencia) utilizando el método compute() del controlador.\nFinalmente, el código python:\nimport numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\n# Definición de las variables de entrada y salida\ntemperatura = ctrl.Antecedent(np.arange(0, 101, 1), 'temperatura')\npotencia = ctrl.Consequent(np.arange(0, 101, 1), 'potencia')\n\n# Definición de las funciones de membresía para la temperatura\ntemperatura['fría'] = fuzz.trimf(temperatura.universe, [0, 0, 50])\ntemperatura['templada'] = fuzz.trimf(temperatura.universe, [0, 50, 100])\ntemperatura['caliente'] = fuzz.trimf(temperatura.universe, [50, 100, 100])\n\n# Definición de las funciones de membresía para la potencia\npotencia['baja'] = fuzz.trimf(potencia.universe, [0, 0, 50])\npotencia['media'] = fuzz.trimf(potencia.universe, [0, 50, 100])\npotencia['alta'] = fuzz.trimf(potencia.universe, [50, 100, 100])\n\n# Reglas difusas\nregla1 = ctrl.Rule(temperatura['fría'], potencia['alta'])\nregla2 = ctrl.Rule(temperatura['templada'], potencia['media'])\nregla3 = ctrl.Rule(temperatura['caliente'], potencia['baja'])\n\n# Sistema de control difuso\nsistema_control = ctrl.ControlSystem([regla1, regla2, regla3])\ncontrolador = ctrl.ControlSystemSimulation(sistema_control)\n\n# Simulación de un cambio en la temperatura\ncontrolador.input['temperatura'] = 25  # Temperatura medida en grados Celsius\n\n# Computar el resultado\ncontrolador.compute()\n\n# Visualización del resultado\nprint(\"Nivel de potencia de la calefacción:\", controlador.output['potencia'])\nA continuación, se representan e interpretan la funciones de membresía antecedente y consecuente del sistema de control:\n# Detalle de las funciones de membresía (antecedente y consecuente)\n\ntemperatura.view(sim=controlador) \npotencia.view(sim=controlador)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lógica Difusa</span>"
    ]
  },
  {
    "objectID": "capitulo4.html#fuzzy-c-means",
    "href": "capitulo4.html#fuzzy-c-means",
    "title": "4  Lógica Difusa",
    "section": "4.3 Fuzzy C-Means",
    "text": "4.3 Fuzzy C-Means\nLos algoritmos de clustering difuso, como el Fuzzy C-Means (FCM), son esenciales en la agrupación de datos donde la pertenencia de un punto a un clúster no es binaria, sino que se modela como un grado de pertenencia difuso. FCM asigna a cada punto un grado de pertenencia a todos los clústeres, lo que permite una representación más flexible de la estructura de los datos. En lugar de asignar cada punto a un único clúster, FCM asigna grados de pertenencia que indican la probabilidad de que un punto pertenezca a cada clúster. Esto es especialmente útil cuando los datos pueden pertenecer a múltiples clústeres simultáneamente, o cuando la frontera entre clústeres no es clara. La importancia de los algoritmos de clustering difuso radica en su capacidad para manejar la incertidumbre en los datos y proporcionar una agrupación más completa y flexible que los métodos tradicionales de clustering.\n\nA diferencia del algoritmo C-Means clásico, que trabaja con una partición dura, FCM realiza una partición suave del conjunto de datos. En tal partición los datos pertenecen en algún grado a todos los clusters.\n\nEl siguiente código muestra cómo hacer un C-Means difuso:\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport skfuzzy as fuzz\nfrom sklearn import datasets\n\n# Carga de datos de Iris desde scikit-learn\niris = datasets.load_iris()\nX = iris.data  # Características\ny = iris.target  # Etiquetas\n\n# Gráfico de dispersión para visualizar los clusters originales según las dos primeras características (sépalo)\ng = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, palette=\"Set2\")\nplt.title('Sépalo: longitud vs tamaño'), plt.xlabel('Longitud'), plt.ylabel('Ancho')\nplt.show()\n\n# Algoritmo Fuzzy C-Means\n# Es necesario que los datos estén en un array 2D, por lo que se utiliza la función \"reshape\"\nX2 = np.reshape(X.T, (X.shape[1], X.shape[0]))  # Cambio de forma de los datos\n\n# Aplicación del modelo Fuzzy C-Means\ncntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(X2, c=3, m=2, error=0.005, maxiter=1000, init=None, seed=111)\nEntre los parámetros que devuelve la función fuzz.cluster.cmeans se pueden citar: - cntr es el centro de los clusters - u es el grado de membresía - u0 es la matriz inicial de membresía - d es la matriz de distancias euclidiana\n# Centro de los clusters\ncluster_centers_df = pd.DataFrame(cntr, columns=['Longitud Sépalo', 'Ancho Sépalo', 'Longitud Pétalo', 'Ancho Pétalo'],\n                                  index=['Cluster 1', 'Cluster 2', 'Cluster 3'])\ncluster_centers_df\n# Grados de membresía (probabilidad de pertenencia a cada cluster)\nmembership_df = pd.DataFrame(u.T, columns=['Cluster 1', 'Cluster 2', 'Cluster 3'])\nmembership_df.head()\n# Gráfico de dispersión para visualizar los clusters obtenidos\ng = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=np.argmax(u.T, axis=1), palette=\"Set2\")  # Asignación de colores según cluster\nplt.title('Sépalo: longitud vs tamaño'), plt.xlabel('Longitud'), plt.ylabel('Ancho')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lógica Difusa</span>"
    ]
  },
  {
    "objectID": "capitulo4.html#fuzzy-matching",
    "href": "capitulo4.html#fuzzy-matching",
    "title": "4  Lógica Difusa",
    "section": "4.4 Fuzzy Matching",
    "text": "4.4 Fuzzy Matching\nEl Fuzzy Matching es una técnica utilizada para comparar cadenas de texto y determinar su grado de similitud, incluso cuando hay diferencias ortográficas, tipográficas u otras variaciones entre ellas. Esta técnica identifica la probabilidad de que dos registros sean realmente coincidentes en función de si coinciden o no en diferentes identificadores. Los identificadores elegidos y el peso asignado constituyen la base de la Concordancia Difusa. Así, cuando los parámetros son demasiado amplios, se encontrarán más coincidencias, pero también aumentarán las posibilidades de “falsos positivos”.\nExisten diversas métricas a utilizar en Fuzzy Matching siendo la más destacada la distancia de Levenshtein pero existen otras como se indican a continuación:\n\nDistancia de Levenshtein: mide el número mínimo de operaciones necesarias para convertir una cadena en otra. Las operaciones incluyen inserción, eliminación o sustitución de caracteres\nDistancia Damerau-Levenshtein: similar a la distancia de Levenshtein, pero también considera la transposición de caracteres adyacentes como una operación válida\nDistancia Jaro-Winkler: diseñada para comparar cadenas de texto cortas, tiene en cuenta la frecuencia de los caracteres y la posición de las coincidencias comunes\nDistancia del teclado: evalúa la similitud entre dos cadenas basándose en la proximidad de las teclas en un teclado estándar\nDistancia Kullback-Leibler: utilizada en comparaciones de cadenas de texto que representan distribuciones de probabilidad\n\n\nLa concordancia difusa es crucial para identificar similitudes entre registros, permitiendo la gestión eficiente de datos duplicados. La distancia de Levenshtein es fundamental en este proceso, al calcular la diferencia entre dos cadenas de caracteres, siendo especialmente útil para detectar errores ortográficos y variaciones en la escritura. Más sobre Distancia de Levenshtein en el siguiente enlace: https://es.wikipedia.org/wiki/Distancia_de_Levenshtein\n\n\n4.4.1 Ejemplo 1\nfrom fuzzywuzzy import fuzz\n\n# Función para comparar nombres completos\ndef comparar_nombres(nombre1, nombre2):\n    # Usamos el ratio completo para comparar nombres\n    ratio = fuzz.ratio(nombre1.lower(), nombre2.lower())\n    return ratio\n\n# Ejemplo de comparación de nombres\nnombre1 = \"Juan Pérez\"\nnombre2 = \"Juan Pérez Gómez\"\nnombre3 = \"Pedro López\"\nnombre4 = \"María Pérez Gómez\"\n\nprint(\"Comparación de nombres completos:\")\nprint(f\"{nombre1} vs {nombre2}: {comparar_nombres(nombre1, nombre2)}\")\nprint(f\"{nombre1} vs {nombre3}: {comparar_nombres(nombre1, nombre3)}\")\nprint(f\"{nombre2} vs {nombre4}: {comparar_nombres(nombre2, nombre4)}\")\n\n\n4.4.2 Ejemplo 2\n# Función para comparar direcciones de vivienda\ndef comparar_direcciones(direccion1, direccion2):\n    # Usamos el ratio parcial para comparar direcciones\n    ratio = fuzz.partialratio(direccion1.lower(), direccion2.lower())\n    return ratio\n\n# Ejemplo de comparación de direcciones\ndireccion1 = \"123 Calle Principal, Madrid\"\ndireccion2 = \"245 Calle Principal, Madrid\"\ndireccion3 = \"456 Calle Secundaria, Ávila\"\ndireccion4 = \"123 Calle Principal, Ávila\"\n\nprint(\"\\nComparación de direcciones de vivienda:\")\nprint(f\"{direccion1} vs {direccion2}: {comparar_direcciones(direccion1, direccion2)}\")\nprint(f\"{direccion1} vs {direccion3}: {comparar_direcciones(direccion1, direccion3)}\")\nprint(f\"{direccion2} vs {direccion4}: {comparar_direcciones(direccion2, direccion4)}\")\nprint(f\"{direccion3} vs {direccion4}: {comparar_direcciones(direccion3, direccion4)}\")\nEn la biblioteca fuzzywuzzy, las funciones ratio() y partial_ratio() se utilizan para calcular la similitud entre dos cadenas de texto. La diferencia principal radica en cómo se realiza la comparación y qué tan flexible es cada método en términos de coincidencia.\n\nratio(): esta función calcula la similitud entre dos cadenas comparándolas en su totalidad. Compara las cadenas carácter por carácter y devuelve un valor que representa la similitud entre las dos cadenas en términos de coincidencia exacta. Es útil cuando se busca una coincidencia exacta entre las cadenas y se prefiere una comparación estricta.\npartial_ratio(): esta función calcula la similitud entre dos cadenas tomando en cuenta solo una parte de las mismas. Se utiliza para buscar subcadenas que coincidan parcialmente entre las cadenas de texto. Esto significa que puede ser más útil cuando se enfrenta a casos donde las cadenas pueden tener variaciones menores o cuando se desea encontrar coincidencias incluso si las cadenas difieren en longitud o tienen diferencias menores.\n\nEntonces, para decidir cuál usar, considera la naturaleza de los datos y el nivel de flexibilidad para realizar la comparación. Suele usarse ratio() cuando se necesita una comparación estricta y quieras asegurarte de que las cadenas coincidan exactamente; mientras que partial_ratio() se emplea cuando se buscan coincidencias parciales o flexibles entre las cadenas (p.e: cuando solo importan importa una parte específica de las mismas).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lógica Difusa</span>"
    ]
  }
]